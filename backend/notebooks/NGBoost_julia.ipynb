{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66790ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cbd1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.scores import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8909939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.read_pickle('../data/pkl/X_1000.pkl')\n",
    "# y = pd.read_pickle('../data/pkl/y_1000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6c4d4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6558"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_pickle('../data/pkl/X_basic_rolling_df_5yrs_preprocessed.pkl')\n",
    "y = pd.read_pickle('../data/pkl/y_basic_rolling_df_5yrs_preprocessed.pkl')\n",
    "X.shape\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5425fdcf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TEAM_ABBREVIATION_ATL_h', 'TEAM_ABBREVIATION_BKN_h',\n",
       "       'TEAM_ABBREVIATION_BOS_h', 'TEAM_ABBREVIATION_CHA_h',\n",
       "       'TEAM_ABBREVIATION_CHI_h', 'TEAM_ABBREVIATION_CLE_h',\n",
       "       'TEAM_ABBREVIATION_DAL_h', 'TEAM_ABBREVIATION_DEN_h',\n",
       "       'TEAM_ABBREVIATION_DET_h', 'TEAM_ABBREVIATION_GSW_h',\n",
       "       'TEAM_ABBREVIATION_HOU_h', 'TEAM_ABBREVIATION_IND_h',\n",
       "       'TEAM_ABBREVIATION_LAC_h', 'TEAM_ABBREVIATION_LAL_h',\n",
       "       'TEAM_ABBREVIATION_MEM_h', 'TEAM_ABBREVIATION_MIA_h',\n",
       "       'TEAM_ABBREVIATION_MIL_h', 'TEAM_ABBREVIATION_MIN_h',\n",
       "       'TEAM_ABBREVIATION_NOP_h', 'TEAM_ABBREVIATION_NYK_h',\n",
       "       'TEAM_ABBREVIATION_OKC_h', 'TEAM_ABBREVIATION_ORL_h',\n",
       "       'TEAM_ABBREVIATION_PHI_h', 'TEAM_ABBREVIATION_PHX_h',\n",
       "       'TEAM_ABBREVIATION_POR_h', 'TEAM_ABBREVIATION_SAC_h',\n",
       "       'TEAM_ABBREVIATION_SAS_h', 'TEAM_ABBREVIATION_TOR_h',\n",
       "       'TEAM_ABBREVIATION_UTA_h', 'TEAM_ABBREVIATION_WAS_h',\n",
       "       'FG_PCT_rolling_h', 'FG3_PCT_rolling_h', 'FT_PCT_rolling_h',\n",
       "       'OREB_rolling_h', 'DREB_rolling_h', 'REB_rolling_h', 'AST_rolling_h',\n",
       "       'STL_rolling_h', 'BLK_rolling_h', 'TOV_rolling_h', 'PF_rolling_h',\n",
       "       'TEAM_ABBREVIATION_ATL_a', 'TEAM_ABBREVIATION_BKN_a',\n",
       "       'TEAM_ABBREVIATION_BOS_a', 'TEAM_ABBREVIATION_CHA_a',\n",
       "       'TEAM_ABBREVIATION_CHI_a', 'TEAM_ABBREVIATION_CLE_a',\n",
       "       'TEAM_ABBREVIATION_DAL_a', 'TEAM_ABBREVIATION_DEN_a',\n",
       "       'TEAM_ABBREVIATION_DET_a', 'TEAM_ABBREVIATION_GSW_a',\n",
       "       'TEAM_ABBREVIATION_HOU_a', 'TEAM_ABBREVIATION_IND_a',\n",
       "       'TEAM_ABBREVIATION_LAC_a', 'TEAM_ABBREVIATION_LAL_a',\n",
       "       'TEAM_ABBREVIATION_MEM_a', 'TEAM_ABBREVIATION_MIA_a',\n",
       "       'TEAM_ABBREVIATION_MIL_a', 'TEAM_ABBREVIATION_MIN_a',\n",
       "       'TEAM_ABBREVIATION_NOP_a', 'TEAM_ABBREVIATION_NYK_a',\n",
       "       'TEAM_ABBREVIATION_OKC_a', 'TEAM_ABBREVIATION_ORL_a',\n",
       "       'TEAM_ABBREVIATION_PHI_a', 'TEAM_ABBREVIATION_PHX_a',\n",
       "       'TEAM_ABBREVIATION_POR_a', 'TEAM_ABBREVIATION_SAC_a',\n",
       "       'TEAM_ABBREVIATION_SAS_a', 'TEAM_ABBREVIATION_TOR_a',\n",
       "       'TEAM_ABBREVIATION_UTA_a', 'TEAM_ABBREVIATION_WAS_a',\n",
       "       'FG_PCT_rolling_a', 'FG3_PCT_rolling_a', 'FT_PCT_rolling_a',\n",
       "       'OREB_rolling_a', 'DREB_rolling_a', 'REB_rolling_a', 'AST_rolling_a',\n",
       "       'STL_rolling_a', 'BLK_rolling_a', 'TOV_rolling_a', 'PF_rolling_a'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fe75380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac5d1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an NGBoost model with a Gaussian process as the probabilistic model\n",
    "ngb = NGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5c561a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92b66593",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=1.0000 norm=11.6736\n",
      "[iter 100] loss=4.0260 val_loss=0.0000 scale=1.0000 norm=10.8337\n",
      "[iter 200] loss=3.9825 val_loss=0.0000 scale=1.0000 norm=10.4368\n",
      "[iter 300] loss=3.9510 val_loss=0.0000 scale=1.0000 norm=10.1723\n",
      "[iter 400] loss=3.9262 val_loss=0.0000 scale=1.0000 norm=9.9744\n",
      "[iter 0] loss=4.0913 val_loss=0.0000 scale=1.0000 norm=11.5517\n",
      "[iter 100] loss=4.0134 val_loss=0.0000 scale=1.0000 norm=10.7334\n",
      "[iter 200] loss=3.9715 val_loss=0.0000 scale=1.0000 norm=10.3421\n",
      "[iter 300] loss=3.9399 val_loss=0.0000 scale=1.0000 norm=10.0763\n",
      "[iter 400] loss=3.9169 val_loss=0.0000 scale=1.0000 norm=9.8803\n",
      "[iter 0] loss=4.0883 val_loss=0.0000 scale=1.0000 norm=11.5216\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ngb_score \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      5\u001b[0m ngb_score\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.pyenv/versions/nba_betting_analysis/lib/python3.10/site-packages/ngboost/ngboost.py:303\u001b[0m, in \u001b[0;36mNGBoost.fit\u001b[0;34m(self, X, Y, X_val, Y_val, sample_weight, val_sample_weight, train_loss_monitor, val_loss_monitor, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    298\u001b[0m     val_loss_monitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m D, Y: D\u001b[38;5;241m.\u001b[39mtotal_score(  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m    299\u001b[0m         Y, sample_weight\u001b[38;5;241m=\u001b[39mval_sample_weight\n\u001b[1;32m    300\u001b[0m     )  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators):\n\u001b[0;32m--> 303\u001b[0m     _, col_idx, X_batch, Y_batch, weight_batch, P_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_idxs\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[1;32m    308\u001b[0m     D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mManifold(P_batch\u001b[38;5;241m.\u001b[39mT)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ngb, X, y, cv=10)\n",
    "\n",
    "ngb_score = scores.mean()\n",
    "\n",
    "ngb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6bff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=4.0927 val_loss=0.0000 scale=1.0000 norm=11.5720\n",
      "[iter 100] loss=4.0153 val_loss=0.0000 scale=1.0000 norm=10.7708\n",
      "[iter 200] loss=3.9736 val_loss=0.0000 scale=1.0000 norm=10.3654\n",
      "[iter 300] loss=3.9402 val_loss=0.0000 scale=1.0000 norm=10.0928\n",
      "[iter 400] loss=3.9152 val_loss=0.0000 scale=1.0000 norm=9.8925\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NGBRegressor(random_state=RandomState(MT19937) at 0x108C2BE40)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NGBRegressor</label><div class=\"sk-toggleable__content\"><pre>NGBRegressor(random_state=RandomState(MT19937) at 0x108C2BE40)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Base: DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(criterion=&#x27;friedman_mse&#x27;, max_depth=3)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(criterion=&#x27;friedman_mse&#x27;, max_depth=3)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "NGBRegressor(random_state=RandomState(MT19937) at 0x108C2BE40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "ngb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1323f7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 169.38\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing data\n",
    "y_pred = ngb.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse:.2f}\")\n",
    "\n",
    "# # Get uncertainty estimates for the predictions\n",
    "y_dist = ngb.pred_dist(X_test)\n",
    "\n",
    "\n",
    "# # Print the mean and standard deviation of the uncertainty estimates\n",
    "# print(f\"Mean standard deviation: {std_dev.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "597a26a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.528048780487805"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6546ce8d-36ec-4412-822e-a41a2b27888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_loc = ngb.feature_importances_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b9ffc09-11ac-49b2-9371-feaa235d55b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_scale = ngb.feature_importances_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6627c2ed-426a-4c6b-84d0-cffb2fe6e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_feature_dict = dict(zip(X.columns, feature_importance_loc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1c9a952-83af-410a-bb6c-4bd52633bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_feature_dict = dict(zip(X.columns, feature_importance_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6eed9aa8-33b5-4a1e-9f31-90d06f0b1b9a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FG_PCT_rolling_h': 0.11391106122160698,\n",
       " 'FG_PCT_rolling_a': 0.11227961821455913,\n",
       " 'REB_rolling_h': 0.09552685976844132,\n",
       " 'DREB_rolling_a': 0.093689020761687,\n",
       " 'FG3_PCT_rolling_h': 0.07707354791276157,\n",
       " 'TOV_rolling_a': 0.0702726930678369,\n",
       " 'TOV_rolling_h': 0.05757631174732492,\n",
       " 'FG3_PCT_rolling_a': 0.056892493197112835,\n",
       " 'DREB_rolling_h': 0.055723321861222384,\n",
       " 'STL_rolling_h': 0.048453064142738374,\n",
       " 'STL_rolling_a': 0.036083952904733506,\n",
       " 'REB_rolling_a': 0.028109986818006517,\n",
       " 'FT_PCT_rolling_a': 0.024178777025904524,\n",
       " 'AST_rolling_a': 0.02076015383527944,\n",
       " 'FT_PCT_rolling_h': 0.01960327188596753,\n",
       " 'PF_rolling_h': 0.016739511982482844,\n",
       " 'AST_rolling_h': 0.014721904063117673,\n",
       " 'BLK_rolling_h': 0.009965247963072646,\n",
       " 'BLK_rolling_a': 0.009144313408309053,\n",
       " 'TEAM_ABBREVIATION_HOU_h': 0.008059631428782727,\n",
       " 'OREB_rolling_h': 0.005821519402569732,\n",
       " 'PF_rolling_a': 0.004154856107156594,\n",
       " 'TEAM_ABBREVIATION_UTA_h': 0.003006047969326126,\n",
       " 'TEAM_ABBREVIATION_CHI_h': 0.002984092955758459,\n",
       " 'TEAM_ABBREVIATION_SAS_a': 0.002088759968349801,\n",
       " 'OREB_rolling_a': 0.0018988466441352302,\n",
       " 'TEAM_ABBREVIATION_PHI_h': 0.0012290117248588757,\n",
       " 'TEAM_ABBREVIATION_POR_h': 0.0010527039647626197,\n",
       " 'TEAM_ABBREVIATION_POR_a': 0.0010038317544136465,\n",
       " 'TEAM_ABBREVIATION_NYK_h': 0.0009976991282456451,\n",
       " 'TEAM_ABBREVIATION_DEN_h': 0.0008601697827942584,\n",
       " 'TEAM_ABBREVIATION_ATL_a': 0.0007437333760296772,\n",
       " 'TEAM_ABBREVIATION_PHI_a': 0.0006470553772616509,\n",
       " 'TEAM_ABBREVIATION_CLE_a': 0.0005183097008131654,\n",
       " 'TEAM_ABBREVIATION_LAC_h': 0.0004400003778140037,\n",
       " 'TEAM_ABBREVIATION_BOS_a': 0.000419794921570749,\n",
       " 'TEAM_ABBREVIATION_UTA_a': 0.0003968608746601095,\n",
       " 'TEAM_ABBREVIATION_PHX_a': 0.00036592368084021395,\n",
       " 'TEAM_ABBREVIATION_BKN_a': 0.0003629271883750447,\n",
       " 'TEAM_ABBREVIATION_ORL_h': 0.00032391823830483347,\n",
       " 'TEAM_ABBREVIATION_DAL_a': 0.00030651916884504656,\n",
       " 'TEAM_ABBREVIATION_MIL_h': 0.0002632486211296325,\n",
       " 'TEAM_ABBREVIATION_DAL_h': 0.00026231399231066487,\n",
       " 'TEAM_ABBREVIATION_MIA_h': 0.00023426991474946162,\n",
       " 'TEAM_ABBREVIATION_CHA_h': 0.00022947366191952972,\n",
       " 'TEAM_ABBREVIATION_NYK_a': 0.00018593528422214517,\n",
       " 'TEAM_ABBREVIATION_MEM_h': 0.0001831456612786026,\n",
       " 'TEAM_ABBREVIATION_NOP_a': 0.0001674079217992072,\n",
       " 'TEAM_ABBREVIATION_SAC_h': 8.68794247574056e-05,\n",
       " 'TEAM_ABBREVIATION_ATL_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_BKN_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_BOS_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_CLE_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_DET_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_GSW_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_IND_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_LAL_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_MIN_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_NOP_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_OKC_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_PHX_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_SAS_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_TOR_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_WAS_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_CHA_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_CHI_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_DEN_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_DET_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_GSW_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_HOU_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_IND_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_LAC_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_LAL_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_MEM_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_MIA_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_MIL_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_MIN_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_OKC_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_ORL_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_SAC_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_TOR_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_WAS_a': 0.0}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(sorted(loc_feature_dict.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31a87b87-2190-4fbb-b15a-4c77a02e02fc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLK_rolling_a': 0.0958964789214812,\n",
       " 'FT_PCT_rolling_h': 0.09252252928654726,\n",
       " 'OREB_rolling_a': 0.060570963554170694,\n",
       " 'REB_rolling_h': 0.05867814750836528,\n",
       " 'FG3_PCT_rolling_h': 0.05223094782123217,\n",
       " 'STL_rolling_a': 0.05220586047121887,\n",
       " 'FG3_PCT_rolling_a': 0.05142302157766366,\n",
       " 'PF_rolling_h': 0.04188754059284979,\n",
       " 'OREB_rolling_h': 0.04036467485630542,\n",
       " 'STL_rolling_h': 0.040051626691655726,\n",
       " 'DREB_rolling_h': 0.039590712044353674,\n",
       " 'FT_PCT_rolling_a': 0.034428069998425036,\n",
       " 'DREB_rolling_a': 0.0301134359179628,\n",
       " 'AST_rolling_h': 0.02962066414667911,\n",
       " 'TOV_rolling_a': 0.028118902275954927,\n",
       " 'TOV_rolling_h': 0.02776615704817869,\n",
       " 'FG_PCT_rolling_a': 0.027352593024270212,\n",
       " 'BLK_rolling_h': 0.026282587844761486,\n",
       " 'PF_rolling_a': 0.01792218860506461,\n",
       " 'TEAM_ABBREVIATION_MIA_a': 0.01743151132462761,\n",
       " 'REB_rolling_a': 0.01692746435353206,\n",
       " 'FG_PCT_rolling_h': 0.012834078782726696,\n",
       " 'TEAM_ABBREVIATION_LAC_h': 0.010071044525096952,\n",
       " 'AST_rolling_a': 0.009522132871632118,\n",
       " 'TEAM_ABBREVIATION_CHI_h': 0.009168668413110015,\n",
       " 'TEAM_ABBREVIATION_GSW_a': 0.006772058276642199,\n",
       " 'TEAM_ABBREVIATION_BKN_a': 0.0066926364236148195,\n",
       " 'TEAM_ABBREVIATION_DAL_a': 0.005776299466569066,\n",
       " 'TEAM_ABBREVIATION_CLE_a': 0.004408060520538985,\n",
       " 'TEAM_ABBREVIATION_LAL_a': 0.004208940387645228,\n",
       " 'TEAM_ABBREVIATION_SAS_a': 0.003829995777465282,\n",
       " 'TEAM_ABBREVIATION_HOU_h': 0.0037098525908271862,\n",
       " 'TEAM_ABBREVIATION_UTA_a': 0.0033335201899511793,\n",
       " 'TEAM_ABBREVIATION_BOS_h': 0.003051377714800461,\n",
       " 'TEAM_ABBREVIATION_HOU_a': 0.00303587899432016,\n",
       " 'TEAM_ABBREVIATION_NOP_a': 0.002984706662028405,\n",
       " 'TEAM_ABBREVIATION_ORL_h': 0.0025601503920816126,\n",
       " 'TEAM_ABBREVIATION_PHX_h': 0.002532693144349527,\n",
       " 'TEAM_ABBREVIATION_OKC_h': 0.002238785116682275,\n",
       " 'TEAM_ABBREVIATION_SAC_a': 0.0018983309816139272,\n",
       " 'TEAM_ABBREVIATION_POR_a': 0.0018542242171359297,\n",
       " 'TEAM_ABBREVIATION_POR_h': 0.0018237360535660657,\n",
       " 'TEAM_ABBREVIATION_SAC_h': 0.001758393401643413,\n",
       " 'TEAM_ABBREVIATION_WAS_a': 0.0014939216497833413,\n",
       " 'TEAM_ABBREVIATION_PHI_a': 0.0013994912428963295,\n",
       " 'TEAM_ABBREVIATION_OKC_a': 0.0013733768767604606,\n",
       " 'TEAM_ABBREVIATION_LAC_a': 0.0013235024316782774,\n",
       " 'TEAM_ABBREVIATION_ATL_a': 0.0013168975588612292,\n",
       " 'TEAM_ABBREVIATION_IND_a': 0.001154396389653814,\n",
       " 'TEAM_ABBREVIATION_BOS_a': 0.0011494701125290211,\n",
       " 'TEAM_ABBREVIATION_PHX_a': 0.0011018275475208426,\n",
       " 'TEAM_ABBREVIATION_CHA_a': 0.0008307427342885881,\n",
       " 'TEAM_ABBREVIATION_GSW_h': 0.0007946379458541522,\n",
       " 'TEAM_ABBREVIATION_DEN_h': 0.0005619656180884281,\n",
       " 'TEAM_ABBREVIATION_DAL_h': 0.0005481921954916699,\n",
       " 'TEAM_ABBREVIATION_NYK_h': 0.000391486382337563,\n",
       " 'TEAM_ABBREVIATION_TOR_a': 0.00033807407654907947,\n",
       " 'TEAM_ABBREVIATION_MEM_a': 0.00032617442233070837,\n",
       " 'TEAM_ABBREVIATION_CHA_h': 0.0002253665807278014,\n",
       " 'TEAM_ABBREVIATION_SAS_h': 0.00011567298323810807,\n",
       " 'TEAM_ABBREVIATION_MEM_h': 8.645128269474302e-05,\n",
       " 'TEAM_ABBREVIATION_BKN_h': 1.6709199374255622e-05,\n",
       " 'TEAM_ABBREVIATION_ATL_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_CLE_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_DET_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_IND_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_LAL_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_MIA_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_MIL_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_MIN_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_NOP_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_PHI_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_TOR_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_UTA_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_WAS_h': 0.0,\n",
       " 'TEAM_ABBREVIATION_CHI_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_DEN_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_DET_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_MIL_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_MIN_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_NYK_a': 0.0,\n",
       " 'TEAM_ABBREVIATION_ORL_a': 0.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(sorted(scale_feature_dict.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f660b52-087f-4402-95db-7a2b456159d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc = pd.DataFrame({'feature':X.columns,\n",
    "                       'importance':feature_importance_loc})\\\n",
    "    .sort_values('importance',ascending=False)\n",
    "df_scale = pd.DataFrame({'feature':X.columns,\n",
    "                       'importance':feature_importance_scale})\\\n",
    "    .sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a888bff-f141-456f-b1ad-60d86445bbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'scale param')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,6))\n",
    "fig.suptitle(\"Feature importance plot for distribution parameters\", fontsize=17)\n",
    "sns.barplot(x='importance',y='feature',ax=ax1,data=df_loc.head(10), color=\"skyblue\").set_title('loc param')\n",
    "sns.barplot(x='importance',y='feature',ax=ax2,data=df_scale.head(10), color=\"skyblue\").set_title('scale param')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b0288f8-8c51-471e-b108-ac8f928d25b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Using cached shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from shap) (1.2.1)\n",
      "Collecting slicer==0.0.7\n",
      "  Using cached slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from shap) (4.65.0)\n",
      "Requirement already satisfied: pandas in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from shap) (1.5.3)\n",
      "Requirement already satisfied: numpy in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from shap) (1.24.2)\n",
      "Requirement already satisfied: packaging>20.9 in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from shap) (23.0)\n",
      "Collecting numba\n",
      "  Using cached numba-0.56.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "Requirement already satisfied: scipy in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from shap) (1.10.1)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "Requirement already satisfied: setuptools in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from numba->shap) (63.2.0)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Using cached llvmlite-0.39.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from pandas->shap) (2022.7.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from scikit-learn->shap) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from scikit-learn->shap) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
      "Installing collected packages: slicer, numpy, llvmlite, cloudpickle, numba, shap\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.2\n",
      "    Uninstalling numpy-1.24.2:\n",
      "      Successfully uninstalled numpy-1.24.2\n",
      "Successfully installed cloudpickle-2.2.1 llvmlite-0.39.1 numba-0.56.4 numpy-1.23.5 shap-0.41.0 slicer-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fdede812-1ab9-4eb2-a413-17fd084e1dfd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from scipy) (1.23.5)\n",
      "Requirement already satisfied: numba in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (0.56.4)\n",
      "Requirement already satisfied: numpy<1.24,>=1.18 in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from numba) (1.23.5)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from numba) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /home/itcwork66/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages (from numba) (63.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca4815-b3f7-4860-92de-f115d0995022",
   "metadata": {},
   "source": [
    "## Tuning the NGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "50e1eb6c-5883-4880-b68a-93b9653ac2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7b7c7c8e-b561-4193-9d84-99df0ed4a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7c54ea0b-86fc-493b-9779-847cba78368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb_cv = NGBRegressor(n_estimators=500, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d6207ce3-6d9b-4b44-9ad6-9eb1f3142633",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=2)\n",
    "b2 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=4)\n",
    "b3 = DecisionTreeRegressor(criterion='poisson', max_depth=2)\n",
    "b4 = DecisionTreeRegressor(criterion='poisson', max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96208351-d9fc-4ca0-846a-e0d09bcd57a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'minibatch_frac': [0.3, 0.5, 1.0],\n",
    "    'Base': [b1, b2, b3, b4],\n",
    "    'natural_gradient': [True, False],\n",
    "    'learning_rate': [0.1, 0.01, 0.005, 0.001],\n",
    "    'tol': [0.0001, 0.0005, 0.001]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ba990b0a-b9cf-498a-8bd1-ad3385f40938",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(ngb_cv, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "539ef128-4c8b-4574-85d8-0e3526b7b660",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=1.0000 norm=11.6644\n",
      "[iter 100] loss=3.8995 val_loss=0.0000 scale=1.0000 norm=9.8845\n",
      "[iter 200] loss=3.8214 val_loss=0.0000 scale=1.0000 norm=9.1906\n",
      "[iter 300] loss=3.7692 val_loss=0.0000 scale=1.0000 norm=8.9262\n",
      "[iter 400] loss=3.7713 val_loss=0.0000 scale=1.0000 norm=9.1247\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=1.0000 norm=11.7933\n",
      "[iter 100] loss=3.8965 val_loss=0.0000 scale=1.0000 norm=9.8263\n",
      "[iter 200] loss=3.8426 val_loss=0.0000 scale=1.0000 norm=9.5245\n",
      "[iter 300] loss=3.7729 val_loss=0.0000 scale=1.0000 norm=9.0711\n",
      "[iter 400] loss=3.7406 val_loss=0.0000 scale=1.0000 norm=8.9223\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=1.0000 norm=12.0439\n",
      "[iter 100] loss=3.8686 val_loss=0.0000 scale=1.0000 norm=9.5778\n",
      "[iter 200] loss=3.8223 val_loss=0.0000 scale=1.0000 norm=9.3010\n",
      "[iter 300] loss=3.7595 val_loss=0.0000 scale=1.0000 norm=8.9005\n",
      "[iter 400] loss=3.7426 val_loss=0.0000 scale=1.0000 norm=8.9368\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=1.0000 norm=11.9046\n",
      "[iter 100] loss=3.8973 val_loss=0.0000 scale=1.0000 norm=9.8344\n",
      "[iter 200] loss=3.8345 val_loss=0.0000 scale=1.0000 norm=9.4835\n",
      "[iter 300] loss=3.7913 val_loss=0.0000 scale=1.0000 norm=9.0887\n",
      "[iter 400] loss=3.7482 val_loss=0.0000 scale=1.0000 norm=8.8256\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=1.0000 norm=11.7968\n",
      "[iter 100] loss=3.8885 val_loss=0.0000 scale=1.0000 norm=9.7134\n",
      "[iter 200] loss=3.8120 val_loss=0.0000 scale=1.0000 norm=9.2497\n",
      "[iter 300] loss=3.7994 val_loss=0.0000 scale=1.0000 norm=9.3859\n",
      "[iter 400] loss=3.7463 val_loss=0.0000 scale=1.0000 norm=8.9892\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=1.0000 norm=11.6644\n",
      "[iter 100] loss=3.8992 val_loss=0.0000 scale=1.0000 norm=9.8970\n",
      "[iter 200] loss=3.8177 val_loss=0.0000 scale=1.0000 norm=9.2111\n",
      "[iter 300] loss=3.7643 val_loss=0.0000 scale=1.0000 norm=8.9336\n",
      "[iter 400] loss=3.7728 val_loss=0.0000 scale=1.0000 norm=9.1742\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=1.0000 norm=11.7933\n",
      "[iter 100] loss=3.8927 val_loss=0.0000 scale=1.0000 norm=9.7975\n",
      "[iter 200] loss=3.8385 val_loss=0.0000 scale=1.0000 norm=9.4874\n",
      "[iter 300] loss=3.7792 val_loss=0.0000 scale=1.0000 norm=8.9955\n",
      "[iter 400] loss=3.7409 val_loss=0.0000 scale=1.0000 norm=8.9380\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=1.0000 norm=12.0439\n",
      "[iter 100] loss=3.8723 val_loss=0.0000 scale=1.0000 norm=9.6311\n",
      "[iter 200] loss=3.8235 val_loss=0.0000 scale=1.0000 norm=9.3591\n",
      "[iter 300] loss=3.7644 val_loss=0.0000 scale=1.0000 norm=8.8903\n",
      "[iter 400] loss=3.7447 val_loss=0.0000 scale=1.0000 norm=8.9522\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=1.0000 norm=11.9046\n",
      "[iter 100] loss=3.8945 val_loss=0.0000 scale=1.0000 norm=9.8037\n",
      "[iter 200] loss=3.8242 val_loss=0.0000 scale=1.0000 norm=9.4463\n",
      "[iter 300] loss=3.7787 val_loss=0.0000 scale=1.0000 norm=9.0750\n",
      "[iter 400] loss=3.7379 val_loss=0.0000 scale=1.0000 norm=8.9104\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=1.0000 norm=11.7968\n",
      "[iter 100] loss=3.8903 val_loss=0.0000 scale=1.0000 norm=9.6932\n",
      "[iter 200] loss=3.8184 val_loss=0.0000 scale=1.0000 norm=9.2857\n",
      "[iter 300] loss=3.8022 val_loss=0.0000 scale=1.0000 norm=9.4064\n",
      "[iter 400] loss=3.7471 val_loss=0.0000 scale=1.0000 norm=9.0478\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=1.0000 norm=11.6644\n",
      "[iter 100] loss=3.8978 val_loss=0.0000 scale=1.0000 norm=9.8854\n",
      "[iter 200] loss=3.8117 val_loss=0.0000 scale=1.0000 norm=9.1792\n",
      "[iter 300] loss=3.7608 val_loss=0.0000 scale=1.0000 norm=8.9083\n",
      "[iter 400] loss=3.7704 val_loss=0.0000 scale=1.0000 norm=9.2038\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=1.0000 norm=11.7933\n",
      "[iter 100] loss=3.8925 val_loss=0.0000 scale=1.0000 norm=9.7881\n",
      "[iter 200] loss=3.8383 val_loss=0.0000 scale=1.0000 norm=9.4973\n",
      "[iter 300] loss=3.7816 val_loss=0.0000 scale=1.0000 norm=9.0556\n",
      "[iter 400] loss=3.7381 val_loss=0.0000 scale=1.0000 norm=8.9074\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=1.0000 norm=12.0439\n",
      "[iter 100] loss=3.8738 val_loss=0.0000 scale=1.0000 norm=9.5926\n",
      "[iter 200] loss=3.8119 val_loss=0.0000 scale=1.0000 norm=9.2862\n",
      "[iter 300] loss=3.7534 val_loss=0.0000 scale=1.0000 norm=8.8713\n",
      "[iter 400] loss=3.7382 val_loss=0.0000 scale=1.0000 norm=8.9175\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=1.0000 norm=11.9046\n",
      "[iter 100] loss=3.9026 val_loss=0.0000 scale=1.0000 norm=9.8187\n",
      "[iter 200] loss=3.8318 val_loss=0.0000 scale=1.0000 norm=9.4896\n",
      "[iter 300] loss=3.7842 val_loss=0.0000 scale=1.0000 norm=9.0807\n",
      "[iter 400] loss=3.7410 val_loss=0.0000 scale=1.0000 norm=8.8357\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=1.0000 norm=11.7968\n",
      "[iter 100] loss=3.8898 val_loss=0.0000 scale=1.0000 norm=9.7179\n",
      "[iter 200] loss=3.8177 val_loss=0.0000 scale=1.0000 norm=9.3089\n",
      "[iter 300] loss=3.7997 val_loss=0.0000 scale=1.0000 norm=9.4019\n",
      "[iter 400] loss=3.7538 val_loss=0.0000 scale=1.0000 norm=9.0588\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=0.5000 norm=0.4916\n",
      "[iter 100] loss=4.0467 val_loss=0.0000 scale=0.5000 norm=0.4646\n",
      "[iter 200] loss=4.0163 val_loss=0.0000 scale=1.0000 norm=0.9029\n",
      "[iter 300] loss=3.9668 val_loss=0.0000 scale=0.5000 norm=0.4293\n",
      "[iter 400] loss=3.9789 val_loss=0.0000 scale=1.0000 norm=0.8584\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=0.5000 norm=0.4898\n",
      "[iter 100] loss=4.0318 val_loss=0.0000 scale=1.0000 norm=0.8968\n",
      "[iter 200] loss=4.0203 val_loss=0.0000 scale=0.5000 norm=0.4527\n",
      "[iter 300] loss=3.9800 val_loss=0.0000 scale=1.0000 norm=0.8584\n",
      "[iter 400] loss=3.9665 val_loss=0.0000 scale=0.5000 norm=0.4256\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=0.5000 norm=0.4894\n",
      "[iter 100] loss=4.0166 val_loss=0.0000 scale=0.5000 norm=0.4499\n",
      "[iter 200] loss=4.0081 val_loss=0.0000 scale=0.5000 norm=0.4534\n",
      "[iter 300] loss=3.9508 val_loss=0.0000 scale=0.5000 norm=0.4279\n",
      "[iter 400] loss=3.9602 val_loss=0.0000 scale=1.0000 norm=0.8428\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0334 val_loss=0.0000 scale=0.5000 norm=0.4605\n",
      "[iter 200] loss=3.9897 val_loss=0.0000 scale=1.0000 norm=0.8929\n",
      "[iter 300] loss=3.9550 val_loss=0.0000 scale=0.5000 norm=0.4214\n",
      "[iter 400] loss=3.9462 val_loss=0.0000 scale=1.0000 norm=0.8455\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=0.5000 norm=0.4908\n",
      "[iter 100] loss=4.0441 val_loss=0.0000 scale=0.5000 norm=0.4572\n",
      "[iter 200] loss=3.9998 val_loss=0.0000 scale=1.0000 norm=0.9020\n",
      "[iter 300] loss=3.9862 val_loss=0.0000 scale=0.5000 norm=0.4384\n",
      "[iter 400] loss=3.9697 val_loss=0.0000 scale=0.5000 norm=0.4332\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=0.5000 norm=0.4916\n",
      "[iter 100] loss=4.0435 val_loss=0.0000 scale=0.5000 norm=0.4648\n",
      "[iter 200] loss=4.0148 val_loss=0.0000 scale=0.5000 norm=0.4480\n",
      "[iter 300] loss=3.9704 val_loss=0.0000 scale=0.5000 norm=0.4310\n",
      "[iter 400] loss=3.9862 val_loss=0.0000 scale=0.5000 norm=0.4331\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=0.5000 norm=0.4898\n",
      "[iter 100] loss=4.0393 val_loss=0.0000 scale=0.5000 norm=0.4612\n",
      "[iter 200] loss=4.0176 val_loss=0.0000 scale=1.0000 norm=0.8985\n",
      "[iter 300] loss=3.9811 val_loss=0.0000 scale=1.0000 norm=0.8566\n",
      "[iter 400] loss=3.9622 val_loss=0.0000 scale=0.5000 norm=0.4249\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=0.5000 norm=0.4894\n",
      "[iter 100] loss=4.0143 val_loss=0.0000 scale=0.5000 norm=0.4475\n",
      "[iter 200] loss=4.0056 val_loss=0.0000 scale=0.5000 norm=0.4476\n",
      "[iter 300] loss=3.9518 val_loss=0.0000 scale=1.0000 norm=0.8455\n",
      "[iter 400] loss=3.9624 val_loss=0.0000 scale=1.0000 norm=0.8435\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0351 val_loss=0.0000 scale=0.5000 norm=0.4587\n",
      "[iter 200] loss=4.0005 val_loss=0.0000 scale=0.5000 norm=0.4473\n",
      "[iter 300] loss=3.9671 val_loss=0.0000 scale=0.5000 norm=0.4314\n",
      "[iter 400] loss=3.9516 val_loss=0.0000 scale=1.0000 norm=0.8505\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=0.5000 norm=0.4908\n",
      "[iter 100] loss=4.0498 val_loss=0.0000 scale=0.5000 norm=0.4657\n",
      "[iter 200] loss=4.0038 val_loss=0.0000 scale=0.5000 norm=0.4510\n",
      "[iter 300] loss=3.9902 val_loss=0.0000 scale=1.0000 norm=0.8796\n",
      "[iter 400] loss=3.9750 val_loss=0.0000 scale=0.5000 norm=0.4393\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=0.5000 norm=0.4916\n",
      "[iter 100] loss=4.0524 val_loss=0.0000 scale=0.5000 norm=0.4705\n",
      "[iter 200] loss=4.0183 val_loss=0.0000 scale=0.5000 norm=0.4435\n",
      "[iter 300] loss=3.9668 val_loss=0.0000 scale=1.0000 norm=0.8669\n",
      "[iter 400] loss=3.9818 val_loss=0.0000 scale=1.0000 norm=0.8593\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=0.5000 norm=0.4898\n",
      "[iter 100] loss=4.0353 val_loss=0.0000 scale=0.5000 norm=0.4588\n",
      "[iter 200] loss=4.0231 val_loss=0.0000 scale=0.5000 norm=0.4539\n",
      "[iter 300] loss=3.9772 val_loss=0.0000 scale=1.0000 norm=0.8551\n",
      "[iter 400] loss=3.9693 val_loss=0.0000 scale=1.0000 norm=0.8695\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=0.5000 norm=0.4894\n",
      "[iter 100] loss=4.0113 val_loss=0.0000 scale=0.5000 norm=0.4440\n",
      "[iter 200] loss=4.0014 val_loss=0.0000 scale=0.5000 norm=0.4479\n",
      "[iter 300] loss=3.9554 val_loss=0.0000 scale=1.0000 norm=0.8537\n",
      "[iter 400] loss=3.9583 val_loss=0.0000 scale=1.0000 norm=0.8338\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0383 val_loss=0.0000 scale=0.5000 norm=0.4633\n",
      "[iter 200] loss=3.9866 val_loss=0.0000 scale=1.0000 norm=0.8730\n",
      "[iter 300] loss=3.9597 val_loss=0.0000 scale=0.5000 norm=0.4279\n",
      "[iter 400] loss=3.9549 val_loss=0.0000 scale=1.0000 norm=0.8538\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=0.5000 norm=0.4908\n",
      "[iter 100] loss=4.0459 val_loss=0.0000 scale=0.5000 norm=0.4587\n",
      "[iter 200] loss=3.9934 val_loss=0.0000 scale=0.5000 norm=0.4419\n",
      "[iter 300] loss=3.9946 val_loss=0.0000 scale=1.0000 norm=0.8836\n",
      "[iter 400] loss=3.9773 val_loss=0.0000 scale=0.5000 norm=0.4361\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=1.0000 norm=11.5815\n",
      "[iter 100] loss=3.8849 val_loss=0.0000 scale=1.0000 norm=9.7163\n",
      "[iter 200] loss=3.8171 val_loss=0.0000 scale=1.0000 norm=9.2653\n",
      "[iter 300] loss=3.7535 val_loss=0.0000 scale=1.0000 norm=8.9370\n",
      "[iter 400] loss=3.7253 val_loss=0.0000 scale=1.0000 norm=8.8418\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=1.0000 norm=11.7758\n",
      "[iter 100] loss=3.8835 val_loss=0.0000 scale=1.0000 norm=9.6757\n",
      "[iter 200] loss=3.8111 val_loss=0.0000 scale=1.0000 norm=9.2674\n",
      "[iter 300] loss=3.7675 val_loss=0.0000 scale=1.0000 norm=9.0348\n",
      "[iter 400] loss=3.7226 val_loss=0.0000 scale=1.0000 norm=8.8575\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=1.0000 norm=11.8357\n",
      "[iter 100] loss=3.8723 val_loss=0.0000 scale=1.0000 norm=9.6188\n",
      "[iter 200] loss=3.7913 val_loss=0.0000 scale=1.0000 norm=9.0645\n",
      "[iter 300] loss=3.7480 val_loss=0.0000 scale=1.0000 norm=8.8826\n",
      "[iter 400] loss=3.7123 val_loss=0.0000 scale=1.0000 norm=8.7096\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=1.0000 norm=11.7229\n",
      "[iter 100] loss=3.8763 val_loss=0.0000 scale=1.0000 norm=9.6642\n",
      "[iter 200] loss=3.8017 val_loss=0.0000 scale=1.0000 norm=9.1855\n",
      "[iter 300] loss=3.7545 val_loss=0.0000 scale=1.0000 norm=8.9306\n",
      "[iter 400] loss=3.7071 val_loss=0.0000 scale=1.0000 norm=8.7180\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=1.0000 norm=11.7478\n",
      "[iter 100] loss=3.8809 val_loss=0.0000 scale=1.0000 norm=9.6914\n",
      "[iter 200] loss=3.7925 val_loss=0.0000 scale=1.0000 norm=9.0837\n",
      "[iter 300] loss=3.7653 val_loss=0.0000 scale=1.0000 norm=9.0948\n",
      "[iter 400] loss=3.7272 val_loss=0.0000 scale=1.0000 norm=8.8719\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=1.0000 norm=11.5815\n",
      "[iter 100] loss=3.8861 val_loss=0.0000 scale=1.0000 norm=9.7163\n",
      "[iter 200] loss=3.8181 val_loss=0.0000 scale=1.0000 norm=9.2412\n",
      "[iter 300] loss=3.7530 val_loss=0.0000 scale=1.0000 norm=8.8841\n",
      "[iter 400] loss=3.7274 val_loss=0.0000 scale=1.0000 norm=8.8466\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=1.0000 norm=11.7758\n",
      "[iter 100] loss=3.8809 val_loss=0.0000 scale=1.0000 norm=9.6516\n",
      "[iter 200] loss=3.8095 val_loss=0.0000 scale=1.0000 norm=9.2593\n",
      "[iter 300] loss=3.7642 val_loss=0.0000 scale=1.0000 norm=9.0152\n",
      "[iter 400] loss=3.7240 val_loss=0.0000 scale=1.0000 norm=8.8509\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=1.0000 norm=11.8357\n",
      "[iter 100] loss=3.8724 val_loss=0.0000 scale=1.0000 norm=9.6041\n",
      "[iter 200] loss=3.7921 val_loss=0.0000 scale=1.0000 norm=9.0536\n",
      "[iter 300] loss=3.7516 val_loss=0.0000 scale=1.0000 norm=8.8831\n",
      "[iter 400] loss=3.7116 val_loss=0.0000 scale=2.0000 norm=17.4401\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=1.0000 norm=11.7229\n",
      "[iter 100] loss=3.8758 val_loss=0.0000 scale=1.0000 norm=9.6488\n",
      "[iter 200] loss=3.8019 val_loss=0.0000 scale=1.0000 norm=9.1669\n",
      "[iter 300] loss=3.7597 val_loss=0.0000 scale=1.0000 norm=8.9415\n",
      "[iter 400] loss=3.7196 val_loss=0.0000 scale=1.0000 norm=8.7245\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=1.0000 norm=11.7478\n",
      "[iter 100] loss=3.8805 val_loss=0.0000 scale=1.0000 norm=9.6906\n",
      "[iter 200] loss=3.7946 val_loss=0.0000 scale=1.0000 norm=9.0820\n",
      "[iter 300] loss=3.7677 val_loss=0.0000 scale=2.0000 norm=18.1593\n",
      "[iter 400] loss=3.7304 val_loss=0.0000 scale=1.0000 norm=8.9075\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=1.0000 norm=11.5815\n",
      "[iter 100] loss=3.8846 val_loss=0.0000 scale=1.0000 norm=9.7168\n",
      "[iter 200] loss=3.8141 val_loss=0.0000 scale=1.0000 norm=9.2530\n",
      "[iter 300] loss=3.7549 val_loss=0.0000 scale=1.0000 norm=8.9195\n",
      "[iter 400] loss=3.7286 val_loss=0.0000 scale=1.0000 norm=8.8444\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=1.0000 norm=11.7758\n",
      "[iter 100] loss=3.8870 val_loss=0.0000 scale=1.0000 norm=9.7127\n",
      "[iter 200] loss=3.8120 val_loss=0.0000 scale=1.0000 norm=9.2901\n",
      "[iter 300] loss=3.7683 val_loss=0.0000 scale=1.0000 norm=9.0706\n",
      "[iter 400] loss=3.7263 val_loss=0.0000 scale=1.0000 norm=8.8874\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=1.0000 norm=11.8357\n",
      "[iter 100] loss=3.8721 val_loss=0.0000 scale=1.0000 norm=9.6082\n",
      "[iter 200] loss=3.7904 val_loss=0.0000 scale=1.0000 norm=9.0730\n",
      "[iter 300] loss=3.7501 val_loss=0.0000 scale=1.0000 norm=8.8487\n",
      "[iter 400] loss=3.7133 val_loss=0.0000 scale=1.0000 norm=8.7340\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=1.0000 norm=11.7229\n",
      "[iter 100] loss=3.8724 val_loss=0.0000 scale=1.0000 norm=9.6473\n",
      "[iter 200] loss=3.7961 val_loss=0.0000 scale=1.0000 norm=9.1470\n",
      "[iter 300] loss=3.7523 val_loss=0.0000 scale=1.0000 norm=8.9029\n",
      "[iter 400] loss=3.7064 val_loss=0.0000 scale=1.0000 norm=8.6876\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=1.0000 norm=11.7478\n",
      "[iter 100] loss=3.8827 val_loss=0.0000 scale=1.0000 norm=9.6725\n",
      "[iter 200] loss=3.7954 val_loss=0.0000 scale=1.0000 norm=9.0708\n",
      "[iter 300] loss=3.7694 val_loss=0.0000 scale=2.0000 norm=18.1805\n",
      "[iter 400] loss=3.7295 val_loss=0.0000 scale=1.0000 norm=8.8825\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=0.5000 norm=0.4880\n",
      "[iter 100] loss=4.0323 val_loss=0.0000 scale=0.5000 norm=0.4671\n",
      "[iter 200] loss=4.0027 val_loss=0.0000 scale=0.5000 norm=0.4487\n",
      "[iter 300] loss=3.9552 val_loss=0.0000 scale=1.0000 norm=0.8515\n",
      "[iter 400] loss=3.9585 val_loss=0.0000 scale=1.0000 norm=0.8405\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=0.5000 norm=0.5021\n",
      "[iter 100] loss=4.0336 val_loss=0.0000 scale=0.5000 norm=0.4623\n",
      "[iter 200] loss=4.0022 val_loss=0.0000 scale=0.5000 norm=0.4450\n",
      "[iter 300] loss=3.9692 val_loss=0.0000 scale=0.5000 norm=0.4322\n",
      "[iter 400] loss=3.9523 val_loss=0.0000 scale=1.0000 norm=0.8463\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=0.5000 norm=0.4960\n",
      "[iter 100] loss=4.0182 val_loss=0.0000 scale=0.5000 norm=0.4555\n",
      "[iter 200] loss=3.9824 val_loss=0.0000 scale=0.5000 norm=0.4375\n",
      "[iter 300] loss=3.9424 val_loss=0.0000 scale=1.0000 norm=0.8477\n",
      "[iter 400] loss=3.9505 val_loss=0.0000 scale=1.0000 norm=0.8349\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=0.5000 norm=0.4996\n",
      "[iter 100] loss=4.0256 val_loss=0.0000 scale=0.5000 norm=0.4572\n",
      "[iter 200] loss=3.9862 val_loss=0.0000 scale=0.5000 norm=0.4476\n",
      "[iter 300] loss=3.9503 val_loss=0.0000 scale=1.0000 norm=0.8493\n",
      "[iter 400] loss=3.9470 val_loss=0.0000 scale=1.0000 norm=0.8480\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0331 val_loss=0.0000 scale=0.5000 norm=0.4553\n",
      "[iter 200] loss=3.9850 val_loss=0.0000 scale=0.5000 norm=0.4430\n",
      "[iter 300] loss=3.9638 val_loss=0.0000 scale=1.0000 norm=0.8653\n",
      "[iter 400] loss=3.9571 val_loss=0.0000 scale=1.0000 norm=0.8451\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=0.5000 norm=0.4880\n",
      "[iter 100] loss=4.0321 val_loss=0.0000 scale=0.5000 norm=0.4674\n",
      "[iter 200] loss=4.0047 val_loss=0.0000 scale=1.0000 norm=0.9020\n",
      "[iter 300] loss=3.9573 val_loss=0.0000 scale=1.0000 norm=0.8563\n",
      "[iter 400] loss=3.9553 val_loss=0.0000 scale=1.0000 norm=0.8404\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=0.5000 norm=0.5021\n",
      "[iter 100] loss=4.0316 val_loss=0.0000 scale=1.0000 norm=0.9182\n",
      "[iter 200] loss=3.9999 val_loss=0.0000 scale=0.5000 norm=0.4424\n",
      "[iter 300] loss=3.9669 val_loss=0.0000 scale=1.0000 norm=0.8625\n",
      "[iter 400] loss=3.9534 val_loss=0.0000 scale=1.0000 norm=0.8513\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=0.5000 norm=0.4960\n",
      "[iter 100] loss=4.0188 val_loss=0.0000 scale=1.0000 norm=0.9070\n",
      "[iter 200] loss=3.9826 val_loss=0.0000 scale=0.5000 norm=0.4339\n",
      "[iter 300] loss=3.9433 val_loss=0.0000 scale=1.0000 norm=0.8515\n",
      "[iter 400] loss=3.9489 val_loss=0.0000 scale=1.0000 norm=0.8348\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=0.5000 norm=0.4996\n",
      "[iter 100] loss=4.0245 val_loss=0.0000 scale=1.0000 norm=0.9125\n",
      "[iter 200] loss=3.9840 val_loss=0.0000 scale=0.5000 norm=0.4476\n",
      "[iter 300] loss=3.9533 val_loss=0.0000 scale=0.5000 norm=0.4306\n",
      "[iter 400] loss=3.9509 val_loss=0.0000 scale=0.5000 norm=0.4254\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0313 val_loss=0.0000 scale=0.5000 norm=0.4536\n",
      "[iter 200] loss=3.9822 val_loss=0.0000 scale=1.0000 norm=0.8839\n",
      "[iter 300] loss=3.9596 val_loss=0.0000 scale=1.0000 norm=0.8555\n",
      "[iter 400] loss=3.9543 val_loss=0.0000 scale=1.0000 norm=0.8443\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=0.5000 norm=0.4880\n",
      "[iter 100] loss=4.0361 val_loss=0.0000 scale=0.5000 norm=0.4707\n",
      "[iter 200] loss=4.0085 val_loss=0.0000 scale=1.0000 norm=0.9110\n",
      "[iter 300] loss=3.9570 val_loss=0.0000 scale=1.0000 norm=0.8551\n",
      "[iter 400] loss=3.9603 val_loss=0.0000 scale=1.0000 norm=0.8446\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=0.5000 norm=0.5021\n",
      "[iter 100] loss=4.0312 val_loss=0.0000 scale=0.5000 norm=0.4609\n",
      "[iter 200] loss=3.9999 val_loss=0.0000 scale=0.5000 norm=0.4446\n",
      "[iter 300] loss=3.9657 val_loss=0.0000 scale=1.0000 norm=0.8643\n",
      "[iter 400] loss=3.9510 val_loss=0.0000 scale=1.0000 norm=0.8468\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=0.5000 norm=0.4960\n",
      "[iter 100] loss=4.0188 val_loss=0.0000 scale=0.5000 norm=0.4543\n",
      "[iter 200] loss=3.9850 val_loss=0.0000 scale=0.5000 norm=0.4382\n",
      "[iter 300] loss=3.9431 val_loss=0.0000 scale=0.5000 norm=0.4248\n",
      "[iter 400] loss=3.9473 val_loss=0.0000 scale=0.5000 norm=0.4136\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=0.5000 norm=0.4996\n",
      "[iter 100] loss=4.0238 val_loss=0.0000 scale=0.5000 norm=0.4566\n",
      "[iter 200] loss=3.9857 val_loss=0.0000 scale=0.5000 norm=0.4499\n",
      "[iter 300] loss=3.9512 val_loss=0.0000 scale=1.0000 norm=0.8566\n",
      "[iter 400] loss=3.9459 val_loss=0.0000 scale=0.5000 norm=0.4240\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0331 val_loss=0.0000 scale=0.5000 norm=0.4546\n",
      "[iter 200] loss=3.9849 val_loss=0.0000 scale=1.0000 norm=0.8892\n",
      "[iter 300] loss=3.9653 val_loss=0.0000 scale=1.0000 norm=0.8657\n",
      "[iter 400] loss=3.9563 val_loss=0.0000 scale=0.5000 norm=0.4209\n",
      "[iter 0] loss=4.0971 val_loss=0.0000 scale=1.0000 norm=11.6012\n",
      "[iter 100] loss=3.8869 val_loss=0.0000 scale=1.0000 norm=9.6535\n",
      "[iter 200] loss=3.8217 val_loss=0.0000 scale=2.0000 norm=18.4859\n",
      "[iter 300] loss=3.7740 val_loss=0.0000 scale=1.0000 norm=8.9661\n",
      "[iter 400] loss=3.7297 val_loss=0.0000 scale=1.0000 norm=8.7235\n",
      "[iter 0] loss=4.0972 val_loss=0.0000 scale=1.0000 norm=11.6172\n",
      "[iter 100] loss=3.8911 val_loss=0.0000 scale=2.0000 norm=19.4722\n",
      "[iter 200] loss=3.8261 val_loss=0.0000 scale=1.0000 norm=9.3247\n",
      "[iter 300] loss=3.7777 val_loss=0.0000 scale=1.0000 norm=9.0543\n",
      "[iter 400] loss=3.7373 val_loss=0.0000 scale=1.0000 norm=8.8288\n",
      "[iter 0] loss=4.0836 val_loss=0.0000 scale=1.0000 norm=11.5145\n",
      "[iter 100] loss=3.8821 val_loss=0.0000 scale=1.0000 norm=9.6578\n",
      "[iter 200] loss=3.8171 val_loss=0.0000 scale=1.0000 norm=9.2221\n",
      "[iter 300] loss=3.7682 val_loss=0.0000 scale=2.0000 norm=17.8567\n",
      "[iter 400] loss=3.7268 val_loss=0.0000 scale=1.0000 norm=8.7004\n",
      "[iter 0] loss=4.0930 val_loss=0.0000 scale=1.0000 norm=11.5486\n",
      "[iter 100] loss=3.8919 val_loss=0.0000 scale=1.0000 norm=9.7240\n",
      "[iter 200] loss=3.8281 val_loss=0.0000 scale=1.0000 norm=9.2957\n",
      "[iter 300] loss=3.7773 val_loss=0.0000 scale=1.0000 norm=8.9848\n",
      "[iter 400] loss=3.7350 val_loss=0.0000 scale=1.0000 norm=8.7420\n",
      "[iter 0] loss=4.0924 val_loss=0.0000 scale=1.0000 norm=11.5740\n",
      "[iter 100] loss=3.8872 val_loss=0.0000 scale=1.0000 norm=9.7127\n",
      "[iter 200] loss=3.8177 val_loss=0.0000 scale=1.0000 norm=9.2514\n",
      "[iter 300] loss=3.7711 val_loss=0.0000 scale=1.0000 norm=8.9693\n",
      "[iter 400] loss=3.7299 val_loss=0.0000 scale=1.0000 norm=8.7337\n",
      "[iter 0] loss=4.0971 val_loss=0.0000 scale=1.0000 norm=11.6012\n",
      "[iter 100] loss=3.8869 val_loss=0.0000 scale=1.0000 norm=9.6535\n",
      "[iter 200] loss=3.8217 val_loss=0.0000 scale=2.0000 norm=18.4859\n",
      "[iter 300] loss=3.7740 val_loss=0.0000 scale=1.0000 norm=8.9661\n",
      "[iter 400] loss=3.7297 val_loss=0.0000 scale=1.0000 norm=8.7235\n",
      "[iter 0] loss=4.0972 val_loss=0.0000 scale=1.0000 norm=11.6172\n",
      "[iter 100] loss=3.8911 val_loss=0.0000 scale=2.0000 norm=19.4722\n",
      "[iter 200] loss=3.8261 val_loss=0.0000 scale=1.0000 norm=9.3247\n",
      "[iter 300] loss=3.7777 val_loss=0.0000 scale=1.0000 norm=9.0543\n",
      "[iter 400] loss=3.7373 val_loss=0.0000 scale=1.0000 norm=8.8288\n",
      "[iter 0] loss=4.0836 val_loss=0.0000 scale=1.0000 norm=11.5145\n",
      "[iter 100] loss=3.8821 val_loss=0.0000 scale=1.0000 norm=9.6578\n",
      "[iter 200] loss=3.8171 val_loss=0.0000 scale=1.0000 norm=9.2221\n",
      "[iter 300] loss=3.7682 val_loss=0.0000 scale=2.0000 norm=17.8567\n",
      "[iter 400] loss=3.7268 val_loss=0.0000 scale=1.0000 norm=8.7004\n",
      "[iter 0] loss=4.0930 val_loss=0.0000 scale=1.0000 norm=11.5486\n",
      "[iter 100] loss=3.8919 val_loss=0.0000 scale=1.0000 norm=9.7240\n",
      "[iter 200] loss=3.8281 val_loss=0.0000 scale=1.0000 norm=9.2957\n",
      "[iter 300] loss=3.7773 val_loss=0.0000 scale=1.0000 norm=8.9848\n",
      "[iter 400] loss=3.7350 val_loss=0.0000 scale=1.0000 norm=8.7420\n",
      "[iter 0] loss=4.0924 val_loss=0.0000 scale=1.0000 norm=11.5740\n",
      "[iter 100] loss=3.8872 val_loss=0.0000 scale=1.0000 norm=9.7127\n",
      "[iter 200] loss=3.8177 val_loss=0.0000 scale=1.0000 norm=9.2514\n",
      "[iter 300] loss=3.7711 val_loss=0.0000 scale=1.0000 norm=8.9693\n",
      "[iter 400] loss=3.7299 val_loss=0.0000 scale=1.0000 norm=8.7337\n",
      "[iter 0] loss=4.0971 val_loss=0.0000 scale=1.0000 norm=11.6012\n",
      "[iter 100] loss=3.8869 val_loss=0.0000 scale=1.0000 norm=9.6535\n",
      "[iter 200] loss=3.8217 val_loss=0.0000 scale=2.0000 norm=18.4859\n",
      "[iter 300] loss=3.7740 val_loss=0.0000 scale=1.0000 norm=8.9661\n",
      "[iter 400] loss=3.7297 val_loss=0.0000 scale=1.0000 norm=8.7235\n",
      "[iter 0] loss=4.0972 val_loss=0.0000 scale=1.0000 norm=11.6172\n",
      "[iter 100] loss=3.8911 val_loss=0.0000 scale=2.0000 norm=19.4722\n",
      "[iter 200] loss=3.8261 val_loss=0.0000 scale=1.0000 norm=9.3247\n",
      "[iter 300] loss=3.7777 val_loss=0.0000 scale=1.0000 norm=9.0543\n",
      "[iter 400] loss=3.7373 val_loss=0.0000 scale=1.0000 norm=8.8288\n",
      "[iter 0] loss=4.0836 val_loss=0.0000 scale=1.0000 norm=11.5145\n",
      "[iter 100] loss=3.8821 val_loss=0.0000 scale=1.0000 norm=9.6578\n",
      "[iter 200] loss=3.8171 val_loss=0.0000 scale=1.0000 norm=9.2221\n",
      "[iter 300] loss=3.7682 val_loss=0.0000 scale=2.0000 norm=17.8567\n",
      "[iter 400] loss=3.7268 val_loss=0.0000 scale=1.0000 norm=8.7004\n",
      "[iter 0] loss=4.0930 val_loss=0.0000 scale=1.0000 norm=11.5486\n",
      "[iter 100] loss=3.8919 val_loss=0.0000 scale=1.0000 norm=9.7240\n",
      "[iter 200] loss=3.8281 val_loss=0.0000 scale=1.0000 norm=9.2957\n",
      "[iter 300] loss=3.7773 val_loss=0.0000 scale=1.0000 norm=8.9848\n",
      "[iter 400] loss=3.7350 val_loss=0.0000 scale=1.0000 norm=8.7420\n",
      "[iter 0] loss=4.0924 val_loss=0.0000 scale=1.0000 norm=11.5740\n",
      "[iter 100] loss=3.8872 val_loss=0.0000 scale=1.0000 norm=9.7127\n",
      "[iter 200] loss=3.8177 val_loss=0.0000 scale=1.0000 norm=9.2514\n",
      "[iter 300] loss=3.7711 val_loss=0.0000 scale=1.0000 norm=8.9693\n",
      "[iter 400] loss=3.7299 val_loss=0.0000 scale=1.0000 norm=8.7337\n",
      "[iter 0] loss=4.0971 val_loss=0.0000 scale=0.5000 norm=0.4963\n",
      "[iter 100] loss=4.0360 val_loss=0.0000 scale=0.5000 norm=0.4644\n",
      "[iter 200] loss=3.9989 val_loss=0.0000 scale=1.0000 norm=0.8921\n",
      "[iter 300] loss=3.9679 val_loss=0.0000 scale=1.0000 norm=0.8627\n",
      "[iter 400] loss=3.9381 val_loss=0.0000 scale=1.0000 norm=0.8325\n",
      "[iter 0] loss=4.0972 val_loss=0.0000 scale=0.5000 norm=0.4980\n",
      "[iter 100] loss=4.0390 val_loss=0.0000 scale=1.0000 norm=0.9339\n",
      "[iter 200] loss=4.0031 val_loss=0.0000 scale=1.0000 norm=0.8992\n",
      "[iter 300] loss=3.9731 val_loss=0.0000 scale=1.0000 norm=0.8710\n",
      "[iter 400] loss=3.9454 val_loss=0.0000 scale=1.0000 norm=0.8439\n",
      "[iter 0] loss=4.0836 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0273 val_loss=0.0000 scale=1.0000 norm=0.9246\n",
      "[iter 200] loss=3.9947 val_loss=0.0000 scale=1.0000 norm=0.8925\n",
      "[iter 300] loss=3.9665 val_loss=0.0000 scale=1.0000 norm=0.8639\n",
      "[iter 400] loss=3.9403 val_loss=0.0000 scale=1.0000 norm=0.8385\n",
      "[iter 0] loss=4.0930 val_loss=0.0000 scale=0.5000 norm=0.4987\n",
      "[iter 100] loss=4.0320 val_loss=0.0000 scale=0.5000 norm=0.4672\n",
      "[iter 200] loss=3.9935 val_loss=0.0000 scale=1.0000 norm=0.8929\n",
      "[iter 300] loss=3.9641 val_loss=0.0000 scale=1.0000 norm=0.8629\n",
      "[iter 400] loss=3.9389 val_loss=0.0000 scale=1.0000 norm=0.8375\n",
      "[iter 0] loss=4.0924 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0337 val_loss=0.0000 scale=0.5000 norm=0.4632\n",
      "[iter 200] loss=3.9965 val_loss=0.0000 scale=1.0000 norm=0.8882\n",
      "[iter 300] loss=3.9672 val_loss=0.0000 scale=1.0000 norm=0.8596\n",
      "[iter 400] loss=3.9407 val_loss=0.0000 scale=1.0000 norm=0.8336\n",
      "[iter 0] loss=4.0971 val_loss=0.0000 scale=0.5000 norm=0.4963\n",
      "[iter 100] loss=4.0360 val_loss=0.0000 scale=0.5000 norm=0.4644\n",
      "[iter 200] loss=3.9989 val_loss=0.0000 scale=1.0000 norm=0.8921\n",
      "[iter 300] loss=3.9679 val_loss=0.0000 scale=1.0000 norm=0.8627\n",
      "[iter 400] loss=3.9381 val_loss=0.0000 scale=1.0000 norm=0.8325\n",
      "[iter 0] loss=4.0972 val_loss=0.0000 scale=0.5000 norm=0.4980\n",
      "[iter 100] loss=4.0390 val_loss=0.0000 scale=1.0000 norm=0.9339\n",
      "[iter 200] loss=4.0031 val_loss=0.0000 scale=1.0000 norm=0.8992\n",
      "[iter 300] loss=3.9731 val_loss=0.0000 scale=1.0000 norm=0.8710\n",
      "[iter 400] loss=3.9454 val_loss=0.0000 scale=1.0000 norm=0.8439\n",
      "[iter 0] loss=4.0836 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0273 val_loss=0.0000 scale=1.0000 norm=0.9246\n",
      "[iter 200] loss=3.9947 val_loss=0.0000 scale=1.0000 norm=0.8925\n",
      "[iter 300] loss=3.9665 val_loss=0.0000 scale=1.0000 norm=0.8639\n",
      "[iter 400] loss=3.9403 val_loss=0.0000 scale=1.0000 norm=0.8385\n",
      "[iter 0] loss=4.0930 val_loss=0.0000 scale=0.5000 norm=0.4987\n",
      "[iter 100] loss=4.0320 val_loss=0.0000 scale=0.5000 norm=0.4672\n",
      "[iter 200] loss=3.9935 val_loss=0.0000 scale=1.0000 norm=0.8929\n",
      "[iter 300] loss=3.9641 val_loss=0.0000 scale=1.0000 norm=0.8629\n",
      "[iter 400] loss=3.9389 val_loss=0.0000 scale=1.0000 norm=0.8375\n",
      "[iter 0] loss=4.0924 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0337 val_loss=0.0000 scale=0.5000 norm=0.4632\n",
      "[iter 200] loss=3.9965 val_loss=0.0000 scale=1.0000 norm=0.8882\n",
      "[iter 300] loss=3.9672 val_loss=0.0000 scale=1.0000 norm=0.8596\n",
      "[iter 400] loss=3.9407 val_loss=0.0000 scale=1.0000 norm=0.8336\n",
      "[iter 0] loss=4.0971 val_loss=0.0000 scale=0.5000 norm=0.4963\n",
      "[iter 100] loss=4.0360 val_loss=0.0000 scale=0.5000 norm=0.4644\n",
      "[iter 200] loss=3.9989 val_loss=0.0000 scale=1.0000 norm=0.8921\n",
      "[iter 300] loss=3.9679 val_loss=0.0000 scale=1.0000 norm=0.8627\n",
      "[iter 400] loss=3.9381 val_loss=0.0000 scale=1.0000 norm=0.8325\n",
      "[iter 0] loss=4.0972 val_loss=0.0000 scale=0.5000 norm=0.4980\n",
      "[iter 100] loss=4.0390 val_loss=0.0000 scale=1.0000 norm=0.9339\n",
      "[iter 200] loss=4.0031 val_loss=0.0000 scale=1.0000 norm=0.8992\n",
      "[iter 300] loss=3.9731 val_loss=0.0000 scale=1.0000 norm=0.8710\n",
      "[iter 400] loss=3.9454 val_loss=0.0000 scale=1.0000 norm=0.8439\n",
      "[iter 0] loss=4.0836 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0273 val_loss=0.0000 scale=1.0000 norm=0.9246\n",
      "[iter 200] loss=3.9947 val_loss=0.0000 scale=1.0000 norm=0.8925\n",
      "[iter 300] loss=3.9665 val_loss=0.0000 scale=1.0000 norm=0.8639\n",
      "[iter 400] loss=3.9403 val_loss=0.0000 scale=1.0000 norm=0.8385\n",
      "[iter 0] loss=4.0930 val_loss=0.0000 scale=0.5000 norm=0.4987\n",
      "[iter 100] loss=4.0320 val_loss=0.0000 scale=0.5000 norm=0.4672\n",
      "[iter 200] loss=3.9935 val_loss=0.0000 scale=1.0000 norm=0.8929\n",
      "[iter 300] loss=3.9641 val_loss=0.0000 scale=1.0000 norm=0.8629\n",
      "[iter 400] loss=3.9389 val_loss=0.0000 scale=1.0000 norm=0.8375\n",
      "[iter 0] loss=4.0924 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0337 val_loss=0.0000 scale=0.5000 norm=0.4632\n",
      "[iter 200] loss=3.9965 val_loss=0.0000 scale=1.0000 norm=0.8882\n",
      "[iter 300] loss=3.9672 val_loss=0.0000 scale=1.0000 norm=0.8596\n",
      "[iter 400] loss=3.9407 val_loss=0.0000 scale=1.0000 norm=0.8336\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=1.0000 norm=11.6644\n",
      "[iter 100] loss=4.0508 val_loss=0.0000 scale=1.0000 norm=11.1425\n",
      "[iter 200] loss=3.9966 val_loss=0.0000 scale=1.0000 norm=10.6151\n",
      "[iter 300] loss=3.9294 val_loss=0.0000 scale=1.0000 norm=9.8811\n",
      "[iter 400] loss=3.9567 val_loss=0.0000 scale=1.0000 norm=10.2116\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=1.0000 norm=11.7933\n",
      "[iter 100] loss=4.0373 val_loss=0.0000 scale=1.0000 norm=11.0106\n",
      "[iter 200] loss=4.0052 val_loss=0.0000 scale=1.0000 norm=10.7307\n",
      "[iter 300] loss=3.9663 val_loss=0.0000 scale=2.0000 norm=20.5943\n",
      "[iter 400] loss=3.9506 val_loss=0.0000 scale=1.0000 norm=10.2110\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=1.0000 norm=12.0439\n",
      "[iter 100] loss=4.0158 val_loss=0.0000 scale=1.0000 norm=10.8435\n",
      "[iter 200] loss=3.9926 val_loss=0.0000 scale=1.0000 norm=10.5818\n",
      "[iter 300] loss=3.9350 val_loss=0.0000 scale=1.0000 norm=9.9531\n",
      "[iter 400] loss=3.9504 val_loss=0.0000 scale=1.0000 norm=10.2527\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=1.0000 norm=11.9046\n",
      "[iter 100] loss=4.0378 val_loss=0.0000 scale=1.0000 norm=11.0700\n",
      "[iter 200] loss=3.9947 val_loss=0.0000 scale=1.0000 norm=10.5534\n",
      "[iter 300] loss=3.9510 val_loss=0.0000 scale=1.0000 norm=10.1464\n",
      "[iter 400] loss=3.9442 val_loss=0.0000 scale=1.0000 norm=10.0797\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=1.0000 norm=11.7968\n",
      "[iter 100] loss=4.0410 val_loss=0.0000 scale=1.0000 norm=11.0959\n",
      "[iter 200] loss=3.9873 val_loss=0.0000 scale=1.0000 norm=10.4504\n",
      "[iter 300] loss=3.9791 val_loss=0.0000 scale=1.0000 norm=10.4710\n",
      "[iter 400] loss=3.9521 val_loss=0.0000 scale=1.0000 norm=10.2297\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=1.0000 norm=11.6644\n",
      "[iter 100] loss=4.0506 val_loss=0.0000 scale=1.0000 norm=11.1425\n",
      "[iter 200] loss=3.9964 val_loss=0.0000 scale=1.0000 norm=10.6166\n",
      "[iter 300] loss=3.9296 val_loss=0.0000 scale=1.0000 norm=9.8822\n",
      "[iter 400] loss=3.9569 val_loss=0.0000 scale=1.0000 norm=10.2149\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=1.0000 norm=11.7933\n",
      "[iter 100] loss=4.0375 val_loss=0.0000 scale=1.0000 norm=11.0106\n",
      "[iter 200] loss=4.0054 val_loss=0.0000 scale=1.0000 norm=10.7305\n",
      "[iter 300] loss=3.9666 val_loss=0.0000 scale=2.0000 norm=20.5995\n",
      "[iter 400] loss=3.9509 val_loss=0.0000 scale=1.0000 norm=10.2137\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=1.0000 norm=12.0439\n",
      "[iter 100] loss=4.0158 val_loss=0.0000 scale=1.0000 norm=10.8435\n",
      "[iter 200] loss=3.9923 val_loss=0.0000 scale=1.0000 norm=10.5818\n",
      "[iter 300] loss=3.9352 val_loss=0.0000 scale=1.0000 norm=9.9512\n",
      "[iter 400] loss=3.9499 val_loss=0.0000 scale=1.0000 norm=10.2456\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=1.0000 norm=11.9046\n",
      "[iter 100] loss=4.0378 val_loss=0.0000 scale=1.0000 norm=11.0700\n",
      "[iter 200] loss=3.9949 val_loss=0.0000 scale=1.0000 norm=10.5535\n",
      "[iter 300] loss=3.9508 val_loss=0.0000 scale=1.0000 norm=10.1455\n",
      "[iter 400] loss=3.9443 val_loss=0.0000 scale=1.0000 norm=10.0810\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=1.0000 norm=11.7968\n",
      "[iter 100] loss=4.0415 val_loss=0.0000 scale=1.0000 norm=11.1011\n",
      "[iter 200] loss=3.9875 val_loss=0.0000 scale=1.0000 norm=10.4526\n",
      "[iter 300] loss=3.9794 val_loss=0.0000 scale=1.0000 norm=10.4739\n",
      "[iter 400] loss=3.9516 val_loss=0.0000 scale=1.0000 norm=10.2250\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=1.0000 norm=11.6644\n",
      "[iter 100] loss=4.0506 val_loss=0.0000 scale=1.0000 norm=11.1425\n",
      "[iter 200] loss=3.9962 val_loss=0.0000 scale=1.0000 norm=10.6166\n",
      "[iter 300] loss=3.9295 val_loss=0.0000 scale=1.0000 norm=9.8824\n",
      "[iter 400] loss=3.9569 val_loss=0.0000 scale=1.0000 norm=10.2144\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=1.0000 norm=11.7933\n",
      "[iter 100] loss=4.0375 val_loss=0.0000 scale=1.0000 norm=11.0106\n",
      "[iter 200] loss=4.0055 val_loss=0.0000 scale=1.0000 norm=10.7317\n",
      "[iter 300] loss=3.9667 val_loss=0.0000 scale=2.0000 norm=20.6036\n",
      "[iter 400] loss=3.9517 val_loss=0.0000 scale=1.0000 norm=10.2131\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=1.0000 norm=12.0439\n",
      "[iter 100] loss=4.0158 val_loss=0.0000 scale=1.0000 norm=10.8435\n",
      "[iter 200] loss=3.9926 val_loss=0.0000 scale=1.0000 norm=10.5846\n",
      "[iter 300] loss=3.9355 val_loss=0.0000 scale=1.0000 norm=9.9539\n",
      "[iter 400] loss=3.9508 val_loss=0.0000 scale=1.0000 norm=10.2549\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=1.0000 norm=11.9046\n",
      "[iter 100] loss=4.0377 val_loss=0.0000 scale=1.0000 norm=11.0700\n",
      "[iter 200] loss=3.9943 val_loss=0.0000 scale=1.0000 norm=10.5515\n",
      "[iter 300] loss=3.9502 val_loss=0.0000 scale=1.0000 norm=10.1466\n",
      "[iter 400] loss=3.9437 val_loss=0.0000 scale=1.0000 norm=10.0781\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=1.0000 norm=11.7968\n",
      "[iter 100] loss=4.0411 val_loss=0.0000 scale=1.0000 norm=11.1011\n",
      "[iter 200] loss=3.9873 val_loss=0.0000 scale=1.0000 norm=10.4529\n",
      "[iter 300] loss=3.9791 val_loss=0.0000 scale=1.0000 norm=10.4706\n",
      "[iter 400] loss=3.9516 val_loss=0.0000 scale=1.0000 norm=10.2239\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=0.5000 norm=0.4916\n",
      "[iter 100] loss=4.1034 val_loss=0.0000 scale=0.5000 norm=0.5015\n",
      "[iter 200] loss=4.0827 val_loss=0.0000 scale=0.5000 norm=0.4814\n",
      "[iter 300] loss=4.0384 val_loss=0.0000 scale=1.0000 norm=0.9117\n",
      "[iter 400] loss=4.0739 val_loss=0.0000 scale=0.5000 norm=0.4781\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=0.5000 norm=0.4898\n",
      "[iter 100] loss=4.0900 val_loss=0.0000 scale=0.5000 norm=0.4928\n",
      "[iter 200] loss=4.0801 val_loss=0.0000 scale=0.5000 norm=0.4804\n",
      "[iter 300] loss=4.0674 val_loss=0.0000 scale=1.0000 norm=0.9531\n",
      "[iter 400] loss=4.0697 val_loss=0.0000 scale=0.5000 norm=0.4831\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=0.5000 norm=0.4894\n",
      "[iter 100] loss=4.0629 val_loss=0.0000 scale=0.5000 norm=0.4753\n",
      "[iter 200] loss=4.0620 val_loss=0.0000 scale=0.5000 norm=0.4708\n",
      "[iter 300] loss=4.0296 val_loss=0.0000 scale=0.5000 norm=0.4573\n",
      "[iter 400] loss=4.0683 val_loss=0.0000 scale=0.5000 norm=0.4807\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0824 val_loss=0.0000 scale=0.5000 norm=0.4851\n",
      "[iter 200] loss=4.0655 val_loss=0.0000 scale=0.5000 norm=0.4807\n",
      "[iter 300] loss=4.0411 val_loss=0.0000 scale=0.5000 norm=0.4589\n",
      "[iter 400] loss=4.0564 val_loss=0.0000 scale=0.5000 norm=0.4816\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=0.5000 norm=0.4908\n",
      "[iter 100] loss=4.0947 val_loss=0.0000 scale=0.5000 norm=0.4883\n",
      "[iter 200] loss=4.0662 val_loss=0.0000 scale=0.5000 norm=0.4812\n",
      "[iter 300] loss=4.0739 val_loss=0.0000 scale=0.5000 norm=0.4788\n",
      "[iter 400] loss=4.0681 val_loss=0.0000 scale=0.5000 norm=0.4782\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=0.5000 norm=0.4916\n",
      "[iter 100] loss=4.1035 val_loss=0.0000 scale=0.5000 norm=0.5019\n",
      "[iter 200] loss=4.0830 val_loss=0.0000 scale=0.5000 norm=0.4818\n",
      "[iter 300] loss=4.0387 val_loss=0.0000 scale=1.0000 norm=0.9117\n",
      "[iter 400] loss=4.0739 val_loss=0.0000 scale=0.5000 norm=0.4776\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=0.5000 norm=0.4898\n",
      "[iter 100] loss=4.0901 val_loss=0.0000 scale=0.5000 norm=0.4928\n",
      "[iter 200] loss=4.0798 val_loss=0.0000 scale=0.5000 norm=0.4798\n",
      "[iter 300] loss=4.0670 val_loss=0.0000 scale=1.0000 norm=0.9527\n",
      "[iter 400] loss=4.0692 val_loss=0.0000 scale=0.5000 norm=0.4831\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=0.5000 norm=0.4894\n",
      "[iter 100] loss=4.0630 val_loss=0.0000 scale=0.5000 norm=0.4754\n",
      "[iter 200] loss=4.0619 val_loss=0.0000 scale=0.5000 norm=0.4706\n",
      "[iter 300] loss=4.0293 val_loss=0.0000 scale=0.5000 norm=0.4569\n",
      "[iter 400] loss=4.0682 val_loss=0.0000 scale=0.5000 norm=0.4809\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0829 val_loss=0.0000 scale=0.5000 norm=0.4856\n",
      "[iter 200] loss=4.0655 val_loss=0.0000 scale=0.5000 norm=0.4807\n",
      "[iter 300] loss=4.0408 val_loss=0.0000 scale=0.5000 norm=0.4590\n",
      "[iter 400] loss=4.0569 val_loss=0.0000 scale=1.0000 norm=0.9638\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=0.5000 norm=0.4908\n",
      "[iter 100] loss=4.0947 val_loss=0.0000 scale=0.5000 norm=0.4883\n",
      "[iter 200] loss=4.0662 val_loss=0.0000 scale=0.5000 norm=0.4812\n",
      "[iter 300] loss=4.0736 val_loss=0.0000 scale=0.5000 norm=0.4783\n",
      "[iter 400] loss=4.0683 val_loss=0.0000 scale=0.5000 norm=0.4786\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=0.5000 norm=0.4916\n",
      "[iter 100] loss=4.1035 val_loss=0.0000 scale=0.5000 norm=0.5019\n",
      "[iter 200] loss=4.0826 val_loss=0.0000 scale=0.5000 norm=0.4815\n",
      "[iter 300] loss=4.0389 val_loss=0.0000 scale=1.0000 norm=0.9125\n",
      "[iter 400] loss=4.0746 val_loss=0.0000 scale=0.5000 norm=0.4784\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=0.5000 norm=0.4898\n",
      "[iter 100] loss=4.0899 val_loss=0.0000 scale=0.5000 norm=0.4925\n",
      "[iter 200] loss=4.0799 val_loss=0.0000 scale=0.5000 norm=0.4799\n",
      "[iter 300] loss=4.0666 val_loss=0.0000 scale=1.0000 norm=0.9516\n",
      "[iter 400] loss=4.0694 val_loss=0.0000 scale=0.5000 norm=0.4829\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=0.5000 norm=0.4894\n",
      "[iter 100] loss=4.0631 val_loss=0.0000 scale=0.5000 norm=0.4754\n",
      "[iter 200] loss=4.0619 val_loss=0.0000 scale=0.5000 norm=0.4705\n",
      "[iter 300] loss=4.0298 val_loss=0.0000 scale=0.5000 norm=0.4570\n",
      "[iter 400] loss=4.0680 val_loss=0.0000 scale=0.5000 norm=0.4802\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0826 val_loss=0.0000 scale=0.5000 norm=0.4854\n",
      "[iter 200] loss=4.0655 val_loss=0.0000 scale=0.5000 norm=0.4809\n",
      "[iter 300] loss=4.0410 val_loss=0.0000 scale=0.5000 norm=0.4590\n",
      "[iter 400] loss=4.0568 val_loss=0.0000 scale=0.5000 norm=0.4820\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=0.5000 norm=0.4908\n",
      "[iter 100] loss=4.0950 val_loss=0.0000 scale=0.5000 norm=0.4885\n",
      "[iter 200] loss=4.0661 val_loss=0.0000 scale=0.5000 norm=0.4808\n",
      "[iter 300] loss=4.0739 val_loss=0.0000 scale=0.5000 norm=0.4781\n",
      "[iter 400] loss=4.0688 val_loss=0.0000 scale=0.5000 norm=0.4790\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=1.0000 norm=11.5815\n",
      "[iter 100] loss=4.0370 val_loss=0.0000 scale=1.0000 norm=10.9016\n",
      "[iter 200] loss=4.0055 val_loss=0.0000 scale=1.0000 norm=10.6516\n",
      "[iter 300] loss=3.9608 val_loss=0.0000 scale=1.0000 norm=10.1961\n",
      "[iter 400] loss=3.9603 val_loss=0.0000 scale=1.0000 norm=10.2922\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=1.0000 norm=11.7758\n",
      "[iter 100] loss=4.0352 val_loss=0.0000 scale=1.0000 norm=10.9594\n",
      "[iter 200] loss=4.0000 val_loss=0.0000 scale=1.0000 norm=10.6139\n",
      "[iter 300] loss=3.9722 val_loss=0.0000 scale=1.0000 norm=10.3824\n",
      "[iter 400] loss=3.9597 val_loss=0.0000 scale=1.0000 norm=10.2779\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=1.0000 norm=11.8357\n",
      "[iter 100] loss=4.0225 val_loss=0.0000 scale=1.0000 norm=10.8848\n",
      "[iter 200] loss=3.9891 val_loss=0.0000 scale=1.0000 norm=10.5391\n",
      "[iter 300] loss=3.9451 val_loss=0.0000 scale=1.0000 norm=10.1469\n",
      "[iter 400] loss=3.9528 val_loss=0.0000 scale=1.0000 norm=10.2879\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=1.0000 norm=11.7229\n",
      "[iter 100] loss=4.0333 val_loss=0.0000 scale=1.0000 norm=10.9932\n",
      "[iter 200] loss=3.9939 val_loss=0.0000 scale=1.0000 norm=10.5067\n",
      "[iter 300] loss=3.9576 val_loss=0.0000 scale=1.0000 norm=10.2424\n",
      "[iter 400] loss=3.9577 val_loss=0.0000 scale=1.0000 norm=10.2564\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=1.0000 norm=11.7478\n",
      "[iter 100] loss=4.0346 val_loss=0.0000 scale=1.0000 norm=11.0233\n",
      "[iter 200] loss=3.9817 val_loss=0.0000 scale=1.0000 norm=10.3557\n",
      "[iter 300] loss=3.9672 val_loss=0.0000 scale=1.0000 norm=10.3373\n",
      "[iter 400] loss=3.9619 val_loss=0.0000 scale=1.0000 norm=10.3380\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=1.0000 norm=11.5815\n",
      "[iter 100] loss=4.0368 val_loss=0.0000 scale=1.0000 norm=10.9015\n",
      "[iter 200] loss=4.0058 val_loss=0.0000 scale=1.0000 norm=10.6541\n",
      "[iter 300] loss=3.9614 val_loss=0.0000 scale=1.0000 norm=10.2030\n",
      "[iter 400] loss=3.9605 val_loss=0.0000 scale=1.0000 norm=10.2962\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=1.0000 norm=11.7758\n",
      "[iter 100] loss=4.0352 val_loss=0.0000 scale=1.0000 norm=10.9594\n",
      "[iter 200] loss=3.9998 val_loss=0.0000 scale=1.0000 norm=10.6139\n",
      "[iter 300] loss=3.9724 val_loss=0.0000 scale=1.0000 norm=10.3824\n",
      "[iter 400] loss=3.9599 val_loss=0.0000 scale=1.0000 norm=10.2779\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=1.0000 norm=11.8357\n",
      "[iter 100] loss=4.0224 val_loss=0.0000 scale=1.0000 norm=10.8848\n",
      "[iter 200] loss=3.9893 val_loss=0.0000 scale=1.0000 norm=10.5391\n",
      "[iter 300] loss=3.9450 val_loss=0.0000 scale=1.0000 norm=10.1469\n",
      "[iter 400] loss=3.9521 val_loss=0.0000 scale=1.0000 norm=10.2814\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=1.0000 norm=11.7229\n",
      "[iter 100] loss=4.0330 val_loss=0.0000 scale=1.0000 norm=10.9931\n",
      "[iter 200] loss=3.9935 val_loss=0.0000 scale=1.0000 norm=10.5006\n",
      "[iter 300] loss=3.9573 val_loss=0.0000 scale=1.0000 norm=10.2363\n",
      "[iter 400] loss=3.9575 val_loss=0.0000 scale=1.0000 norm=10.2530\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=1.0000 norm=11.7478\n",
      "[iter 100] loss=4.0345 val_loss=0.0000 scale=1.0000 norm=11.0233\n",
      "[iter 200] loss=3.9817 val_loss=0.0000 scale=1.0000 norm=10.3559\n",
      "[iter 300] loss=3.9672 val_loss=0.0000 scale=1.0000 norm=10.3377\n",
      "[iter 400] loss=3.9618 val_loss=0.0000 scale=1.0000 norm=10.3381\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=1.0000 norm=11.5815\n",
      "[iter 100] loss=4.0371 val_loss=0.0000 scale=1.0000 norm=10.9016\n",
      "[iter 200] loss=4.0056 val_loss=0.0000 scale=1.0000 norm=10.6516\n",
      "[iter 300] loss=3.9608 val_loss=0.0000 scale=1.0000 norm=10.1974\n",
      "[iter 400] loss=3.9600 val_loss=0.0000 scale=1.0000 norm=10.2946\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=1.0000 norm=11.7758\n",
      "[iter 100] loss=4.0351 val_loss=0.0000 scale=1.0000 norm=10.9594\n",
      "[iter 200] loss=3.9999 val_loss=0.0000 scale=1.0000 norm=10.6139\n",
      "[iter 300] loss=3.9723 val_loss=0.0000 scale=1.0000 norm=10.3844\n",
      "[iter 400] loss=3.9600 val_loss=0.0000 scale=1.0000 norm=10.2800\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=1.0000 norm=11.8357\n",
      "[iter 100] loss=4.0224 val_loss=0.0000 scale=1.0000 norm=10.8848\n",
      "[iter 200] loss=3.9893 val_loss=0.0000 scale=1.0000 norm=10.5391\n",
      "[iter 300] loss=3.9450 val_loss=0.0000 scale=1.0000 norm=10.1469\n",
      "[iter 400] loss=3.9529 val_loss=0.0000 scale=1.0000 norm=10.2899\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=1.0000 norm=11.7229\n",
      "[iter 100] loss=4.0331 val_loss=0.0000 scale=1.0000 norm=10.9931\n",
      "[iter 200] loss=3.9935 val_loss=0.0000 scale=1.0000 norm=10.5043\n",
      "[iter 300] loss=3.9573 val_loss=0.0000 scale=1.0000 norm=10.2404\n",
      "[iter 400] loss=3.9573 val_loss=0.0000 scale=1.0000 norm=10.2548\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=1.0000 norm=11.7478\n",
      "[iter 100] loss=4.0345 val_loss=0.0000 scale=1.0000 norm=11.0233\n",
      "[iter 200] loss=3.9815 val_loss=0.0000 scale=1.0000 norm=10.3557\n",
      "[iter 300] loss=3.9670 val_loss=0.0000 scale=1.0000 norm=10.3375\n",
      "[iter 400] loss=3.9621 val_loss=0.0000 scale=1.0000 norm=10.3383\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=0.5000 norm=0.4880\n",
      "[iter 100] loss=4.0905 val_loss=0.0000 scale=0.5000 norm=0.4975\n",
      "[iter 200] loss=4.0879 val_loss=0.0000 scale=0.5000 norm=0.4922\n",
      "[iter 300] loss=4.0566 val_loss=0.0000 scale=1.0000 norm=0.9449\n",
      "[iter 400] loss=4.0742 val_loss=0.0000 scale=1.0000 norm=0.9598\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=0.5000 norm=0.5021\n",
      "[iter 100] loss=4.0872 val_loss=0.0000 scale=0.5000 norm=0.4910\n",
      "[iter 200] loss=4.0762 val_loss=0.0000 scale=0.5000 norm=0.4799\n",
      "[iter 300] loss=4.0696 val_loss=0.0000 scale=1.0000 norm=0.9621\n",
      "[iter 400] loss=4.0718 val_loss=0.0000 scale=0.5000 norm=0.4830\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=0.5000 norm=0.4960\n",
      "[iter 100] loss=4.0690 val_loss=0.0000 scale=0.5000 norm=0.4799\n",
      "[iter 200] loss=4.0611 val_loss=0.0000 scale=0.5000 norm=0.4738\n",
      "[iter 300] loss=4.0367 val_loss=0.0000 scale=0.5000 norm=0.4623\n",
      "[iter 400] loss=4.0634 val_loss=0.0000 scale=0.5000 norm=0.4760\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=0.5000 norm=0.4996\n",
      "[iter 100] loss=4.0783 val_loss=0.0000 scale=0.5000 norm=0.4838\n",
      "[iter 200] loss=4.0669 val_loss=0.0000 scale=1.0000 norm=0.9689\n",
      "[iter 300] loss=4.0486 val_loss=0.0000 scale=0.5000 norm=0.4701\n",
      "[iter 400] loss=4.0632 val_loss=0.0000 scale=0.5000 norm=0.4818\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0850 val_loss=0.0000 scale=0.5000 norm=0.4829\n",
      "[iter 200] loss=4.0612 val_loss=0.0000 scale=0.5000 norm=0.4809\n",
      "[iter 300] loss=4.0601 val_loss=0.0000 scale=1.0000 norm=0.9507\n",
      "[iter 400] loss=4.0689 val_loss=0.0000 scale=0.5000 norm=0.4758\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=0.5000 norm=0.4880\n",
      "[iter 100] loss=4.0905 val_loss=0.0000 scale=0.5000 norm=0.4973\n",
      "[iter 200] loss=4.0879 val_loss=0.0000 scale=0.5000 norm=0.4921\n",
      "[iter 300] loss=4.0563 val_loss=0.0000 scale=1.0000 norm=0.9441\n",
      "[iter 400] loss=4.0742 val_loss=0.0000 scale=1.0000 norm=0.9600\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=0.5000 norm=0.5021\n",
      "[iter 100] loss=4.0873 val_loss=0.0000 scale=0.5000 norm=0.4911\n",
      "[iter 200] loss=4.0759 val_loss=0.0000 scale=0.5000 norm=0.4795\n",
      "[iter 300] loss=4.0693 val_loss=0.0000 scale=1.0000 norm=0.9611\n",
      "[iter 400] loss=4.0713 val_loss=0.0000 scale=0.5000 norm=0.4824\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=0.5000 norm=0.4960\n",
      "[iter 100] loss=4.0692 val_loss=0.0000 scale=0.5000 norm=0.4802\n",
      "[iter 200] loss=4.0611 val_loss=0.0000 scale=0.5000 norm=0.4737\n",
      "[iter 300] loss=4.0368 val_loss=0.0000 scale=0.5000 norm=0.4622\n",
      "[iter 400] loss=4.0639 val_loss=0.0000 scale=0.5000 norm=0.4764\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=0.5000 norm=0.4996\n",
      "[iter 100] loss=4.0780 val_loss=0.0000 scale=0.5000 norm=0.4836\n",
      "[iter 200] loss=4.0669 val_loss=0.0000 scale=0.5000 norm=0.4846\n",
      "[iter 300] loss=4.0488 val_loss=0.0000 scale=0.5000 norm=0.4704\n",
      "[iter 400] loss=4.0634 val_loss=0.0000 scale=0.5000 norm=0.4819\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0851 val_loss=0.0000 scale=0.5000 norm=0.4830\n",
      "[iter 200] loss=4.0611 val_loss=0.0000 scale=0.5000 norm=0.4808\n",
      "[iter 300] loss=4.0601 val_loss=0.0000 scale=1.0000 norm=0.9506\n",
      "[iter 400] loss=4.0691 val_loss=0.0000 scale=0.5000 norm=0.4759\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=0.5000 norm=0.4880\n",
      "[iter 100] loss=4.0901 val_loss=0.0000 scale=0.5000 norm=0.4971\n",
      "[iter 200] loss=4.0878 val_loss=0.0000 scale=0.5000 norm=0.4922\n",
      "[iter 300] loss=4.0562 val_loss=0.0000 scale=1.0000 norm=0.9438\n",
      "[iter 400] loss=4.0741 val_loss=0.0000 scale=1.0000 norm=0.9600\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=0.5000 norm=0.5021\n",
      "[iter 100] loss=4.0873 val_loss=0.0000 scale=0.5000 norm=0.4911\n",
      "[iter 200] loss=4.0760 val_loss=0.0000 scale=0.5000 norm=0.4797\n",
      "[iter 300] loss=4.0696 val_loss=0.0000 scale=1.0000 norm=0.9623\n",
      "[iter 400] loss=4.0715 val_loss=0.0000 scale=0.5000 norm=0.4824\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=0.5000 norm=0.4960\n",
      "[iter 100] loss=4.0689 val_loss=0.0000 scale=0.5000 norm=0.4798\n",
      "[iter 200] loss=4.0610 val_loss=0.0000 scale=0.5000 norm=0.4736\n",
      "[iter 300] loss=4.0367 val_loss=0.0000 scale=0.5000 norm=0.4624\n",
      "[iter 400] loss=4.0637 val_loss=0.0000 scale=0.5000 norm=0.4764\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=0.5000 norm=0.4996\n",
      "[iter 100] loss=4.0781 val_loss=0.0000 scale=0.5000 norm=0.4836\n",
      "[iter 200] loss=4.0669 val_loss=0.0000 scale=1.0000 norm=0.9692\n",
      "[iter 300] loss=4.0489 val_loss=0.0000 scale=0.5000 norm=0.4702\n",
      "[iter 400] loss=4.0633 val_loss=0.0000 scale=0.5000 norm=0.4816\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0851 val_loss=0.0000 scale=0.5000 norm=0.4830\n",
      "[iter 200] loss=4.0614 val_loss=0.0000 scale=0.5000 norm=0.4810\n",
      "[iter 300] loss=4.0605 val_loss=0.0000 scale=1.0000 norm=0.9513\n",
      "[iter 400] loss=4.0690 val_loss=0.0000 scale=0.5000 norm=0.4757\n",
      "[iter 0] loss=4.0971 val_loss=0.0000 scale=1.0000 norm=11.6012\n",
      "[iter 100] loss=4.0331 val_loss=0.0000 scale=1.0000 norm=10.9091\n",
      "[iter 200] loss=4.0003 val_loss=0.0000 scale=1.0000 norm=10.5789\n",
      "[iter 300] loss=3.9770 val_loss=0.0000 scale=1.0000 norm=10.3604\n",
      "[iter 400] loss=3.9580 val_loss=0.0000 scale=1.0000 norm=10.1938\n",
      "[iter 0] loss=4.0972 val_loss=0.0000 scale=1.0000 norm=11.6172\n",
      "[iter 100] loss=4.0388 val_loss=0.0000 scale=1.0000 norm=10.9741\n",
      "[iter 200] loss=4.0072 val_loss=0.0000 scale=1.0000 norm=10.6531\n",
      "[iter 300] loss=3.9850 val_loss=0.0000 scale=1.0000 norm=10.4395\n",
      "[iter 400] loss=3.9665 val_loss=0.0000 scale=1.0000 norm=10.2700\n",
      "[iter 0] loss=4.0836 val_loss=0.0000 scale=1.0000 norm=11.5145\n",
      "[iter 100] loss=4.0281 val_loss=0.0000 scale=1.0000 norm=10.9305\n",
      "[iter 200] loss=3.9964 val_loss=0.0000 scale=1.0000 norm=10.6064\n",
      "[iter 300] loss=3.9738 val_loss=0.0000 scale=1.0000 norm=10.3869\n",
      "[iter 400] loss=3.9550 val_loss=0.0000 scale=1.0000 norm=10.2210\n",
      "[iter 0] loss=4.0930 val_loss=0.0000 scale=1.0000 norm=11.5486\n",
      "[iter 100] loss=4.0365 val_loss=0.0000 scale=1.0000 norm=10.9639\n",
      "[iter 200] loss=4.0049 val_loss=0.0000 scale=1.0000 norm=10.6484\n",
      "[iter 300] loss=3.9825 val_loss=0.0000 scale=1.0000 norm=10.4365\n",
      "[iter 400] loss=3.9644 val_loss=0.0000 scale=1.0000 norm=10.2755\n",
      "[iter 0] loss=4.0924 val_loss=0.0000 scale=1.0000 norm=11.5740\n",
      "[iter 100] loss=4.0341 val_loss=0.0000 scale=1.0000 norm=10.9649\n",
      "[iter 200] loss=4.0016 val_loss=0.0000 scale=1.0000 norm=10.6503\n",
      "[iter 300] loss=3.9797 val_loss=0.0000 scale=1.0000 norm=10.4442\n",
      "[iter 400] loss=3.9619 val_loss=0.0000 scale=1.0000 norm=10.2827\n",
      "[iter 0] loss=4.0971 val_loss=0.0000 scale=1.0000 norm=11.6012\n",
      "[iter 100] loss=4.0331 val_loss=0.0000 scale=1.0000 norm=10.9091\n",
      "[iter 200] loss=4.0003 val_loss=0.0000 scale=1.0000 norm=10.5789\n",
      "[iter 300] loss=3.9770 val_loss=0.0000 scale=1.0000 norm=10.3604\n",
      "[iter 400] loss=3.9580 val_loss=0.0000 scale=1.0000 norm=10.1938\n",
      "[iter 0] loss=4.0972 val_loss=0.0000 scale=1.0000 norm=11.6172\n",
      "[iter 100] loss=4.0388 val_loss=0.0000 scale=1.0000 norm=10.9741\n",
      "[iter 200] loss=4.0072 val_loss=0.0000 scale=1.0000 norm=10.6531\n",
      "[iter 300] loss=3.9850 val_loss=0.0000 scale=1.0000 norm=10.4395\n",
      "[iter 400] loss=3.9665 val_loss=0.0000 scale=1.0000 norm=10.2700\n",
      "[iter 0] loss=4.0836 val_loss=0.0000 scale=1.0000 norm=11.5145\n",
      "[iter 100] loss=4.0281 val_loss=0.0000 scale=1.0000 norm=10.9305\n",
      "[iter 200] loss=3.9964 val_loss=0.0000 scale=1.0000 norm=10.6064\n",
      "[iter 300] loss=3.9738 val_loss=0.0000 scale=1.0000 norm=10.3869\n",
      "[iter 400] loss=3.9550 val_loss=0.0000 scale=1.0000 norm=10.2210\n",
      "[iter 0] loss=4.0930 val_loss=0.0000 scale=1.0000 norm=11.5486\n",
      "[iter 100] loss=4.0365 val_loss=0.0000 scale=1.0000 norm=10.9639\n",
      "[iter 200] loss=4.0049 val_loss=0.0000 scale=1.0000 norm=10.6484\n",
      "[iter 300] loss=3.9825 val_loss=0.0000 scale=1.0000 norm=10.4365\n",
      "[iter 400] loss=3.9644 val_loss=0.0000 scale=1.0000 norm=10.2755\n",
      "[iter 0] loss=4.0924 val_loss=0.0000 scale=1.0000 norm=11.5740\n",
      "[iter 100] loss=4.0341 val_loss=0.0000 scale=1.0000 norm=10.9649\n",
      "[iter 200] loss=4.0016 val_loss=0.0000 scale=1.0000 norm=10.6503\n",
      "[iter 300] loss=3.9797 val_loss=0.0000 scale=1.0000 norm=10.4442\n",
      "[iter 400] loss=3.9619 val_loss=0.0000 scale=1.0000 norm=10.2827\n",
      "[iter 0] loss=4.0971 val_loss=0.0000 scale=1.0000 norm=11.6012\n",
      "[iter 100] loss=4.0331 val_loss=0.0000 scale=1.0000 norm=10.9091\n",
      "[iter 200] loss=4.0003 val_loss=0.0000 scale=1.0000 norm=10.5789\n",
      "[iter 300] loss=3.9770 val_loss=0.0000 scale=1.0000 norm=10.3604\n",
      "[iter 400] loss=3.9580 val_loss=0.0000 scale=1.0000 norm=10.1938\n",
      "[iter 0] loss=4.0972 val_loss=0.0000 scale=1.0000 norm=11.6172\n",
      "[iter 100] loss=4.0388 val_loss=0.0000 scale=1.0000 norm=10.9741\n",
      "[iter 200] loss=4.0072 val_loss=0.0000 scale=1.0000 norm=10.6531\n",
      "[iter 300] loss=3.9850 val_loss=0.0000 scale=1.0000 norm=10.4395\n",
      "[iter 400] loss=3.9665 val_loss=0.0000 scale=1.0000 norm=10.2700\n",
      "[iter 0] loss=4.0836 val_loss=0.0000 scale=1.0000 norm=11.5145\n",
      "[iter 100] loss=4.0281 val_loss=0.0000 scale=1.0000 norm=10.9305\n",
      "[iter 200] loss=3.9964 val_loss=0.0000 scale=1.0000 norm=10.6064\n",
      "[iter 300] loss=3.9738 val_loss=0.0000 scale=1.0000 norm=10.3869\n",
      "[iter 400] loss=3.9550 val_loss=0.0000 scale=1.0000 norm=10.2210\n",
      "[iter 0] loss=4.0930 val_loss=0.0000 scale=1.0000 norm=11.5486\n",
      "[iter 100] loss=4.0365 val_loss=0.0000 scale=1.0000 norm=10.9639\n",
      "[iter 200] loss=4.0049 val_loss=0.0000 scale=1.0000 norm=10.6484\n",
      "[iter 300] loss=3.9825 val_loss=0.0000 scale=1.0000 norm=10.4365\n",
      "[iter 400] loss=3.9644 val_loss=0.0000 scale=1.0000 norm=10.2755\n",
      "[iter 0] loss=4.0924 val_loss=0.0000 scale=1.0000 norm=11.5740\n",
      "[iter 100] loss=4.0341 val_loss=0.0000 scale=1.0000 norm=10.9649\n",
      "[iter 200] loss=4.0016 val_loss=0.0000 scale=1.0000 norm=10.6503\n",
      "[iter 300] loss=3.9797 val_loss=0.0000 scale=1.0000 norm=10.4442\n",
      "[iter 400] loss=3.9619 val_loss=0.0000 scale=1.0000 norm=10.2827\n",
      "[iter 0] loss=4.0971 val_loss=0.0000 scale=0.5000 norm=0.4963\n",
      "[iter 100] loss=4.0861 val_loss=0.0000 scale=0.5000 norm=0.4876\n",
      "[iter 200] loss=4.0782 val_loss=0.0000 scale=0.5000 norm=0.4840\n",
      "[iter 300] loss=4.0714 val_loss=0.0000 scale=1.0000 norm=0.9624\n",
      "[iter 400] loss=4.0657 val_loss=0.0000 scale=1.0000 norm=0.9580\n",
      "[iter 0] loss=4.0972 val_loss=0.0000 scale=0.5000 norm=0.4980\n",
      "[iter 100] loss=4.0872 val_loss=0.0000 scale=0.5000 norm=0.4898\n",
      "[iter 200] loss=4.0805 val_loss=0.0000 scale=0.5000 norm=0.4866\n",
      "[iter 300] loss=4.0734 val_loss=0.0000 scale=1.0000 norm=0.9672\n",
      "[iter 400] loss=4.0677 val_loss=0.0000 scale=1.0000 norm=0.9628\n",
      "[iter 0] loss=4.0836 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0742 val_loss=0.0000 scale=0.5000 norm=0.4841\n",
      "[iter 200] loss=4.0671 val_loss=0.0000 scale=0.5000 norm=0.4808\n",
      "[iter 300] loss=4.0611 val_loss=0.0000 scale=1.0000 norm=0.9565\n",
      "[iter 400] loss=4.0560 val_loss=0.0000 scale=0.5000 norm=0.4760\n",
      "[iter 0] loss=4.0930 val_loss=0.0000 scale=0.5000 norm=0.4987\n",
      "[iter 100] loss=4.0811 val_loss=0.0000 scale=1.0000 norm=0.9786\n",
      "[iter 200] loss=4.0736 val_loss=0.0000 scale=1.0000 norm=0.9719\n",
      "[iter 300] loss=4.0672 val_loss=0.0000 scale=1.0000 norm=0.9669\n",
      "[iter 400] loss=4.0614 val_loss=0.0000 scale=1.0000 norm=0.9619\n",
      "[iter 0] loss=4.0924 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0823 val_loss=0.0000 scale=0.5000 norm=0.4864\n",
      "[iter 200] loss=4.0748 val_loss=0.0000 scale=0.5000 norm=0.4826\n",
      "[iter 300] loss=4.0681 val_loss=0.0000 scale=1.0000 norm=0.9594\n",
      "[iter 400] loss=4.0622 val_loss=0.0000 scale=1.0000 norm=0.9543\n",
      "[iter 0] loss=4.0971 val_loss=0.0000 scale=0.5000 norm=0.4963\n",
      "[iter 100] loss=4.0861 val_loss=0.0000 scale=0.5000 norm=0.4876\n",
      "[iter 200] loss=4.0782 val_loss=0.0000 scale=0.5000 norm=0.4840\n",
      "[iter 300] loss=4.0714 val_loss=0.0000 scale=1.0000 norm=0.9624\n",
      "[iter 400] loss=4.0657 val_loss=0.0000 scale=1.0000 norm=0.9580\n",
      "[iter 0] loss=4.0972 val_loss=0.0000 scale=0.5000 norm=0.4980\n",
      "[iter 100] loss=4.0872 val_loss=0.0000 scale=0.5000 norm=0.4898\n",
      "[iter 200] loss=4.0805 val_loss=0.0000 scale=0.5000 norm=0.4866\n",
      "[iter 300] loss=4.0734 val_loss=0.0000 scale=1.0000 norm=0.9672\n",
      "[iter 400] loss=4.0677 val_loss=0.0000 scale=1.0000 norm=0.9628\n",
      "[iter 0] loss=4.0836 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0742 val_loss=0.0000 scale=0.5000 norm=0.4841\n",
      "[iter 200] loss=4.0671 val_loss=0.0000 scale=0.5000 norm=0.4808\n",
      "[iter 300] loss=4.0611 val_loss=0.0000 scale=1.0000 norm=0.9565\n",
      "[iter 400] loss=4.0560 val_loss=0.0000 scale=0.5000 norm=0.4760\n",
      "[iter 0] loss=4.0930 val_loss=0.0000 scale=0.5000 norm=0.4987\n",
      "[iter 100] loss=4.0811 val_loss=0.0000 scale=1.0000 norm=0.9786\n",
      "[iter 200] loss=4.0736 val_loss=0.0000 scale=1.0000 norm=0.9719\n",
      "[iter 300] loss=4.0672 val_loss=0.0000 scale=1.0000 norm=0.9669\n",
      "[iter 400] loss=4.0614 val_loss=0.0000 scale=1.0000 norm=0.9619\n",
      "[iter 0] loss=4.0924 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0823 val_loss=0.0000 scale=0.5000 norm=0.4864\n",
      "[iter 200] loss=4.0748 val_loss=0.0000 scale=0.5000 norm=0.4826\n",
      "[iter 300] loss=4.0681 val_loss=0.0000 scale=1.0000 norm=0.9594\n",
      "[iter 400] loss=4.0622 val_loss=0.0000 scale=1.0000 norm=0.9543\n",
      "[iter 0] loss=4.0971 val_loss=0.0000 scale=0.5000 norm=0.4963\n",
      "[iter 100] loss=4.0861 val_loss=0.0000 scale=0.5000 norm=0.4876\n",
      "[iter 200] loss=4.0782 val_loss=0.0000 scale=0.5000 norm=0.4840\n",
      "[iter 300] loss=4.0714 val_loss=0.0000 scale=1.0000 norm=0.9624\n",
      "[iter 400] loss=4.0657 val_loss=0.0000 scale=1.0000 norm=0.9580\n",
      "[iter 0] loss=4.0972 val_loss=0.0000 scale=0.5000 norm=0.4980\n",
      "[iter 100] loss=4.0872 val_loss=0.0000 scale=0.5000 norm=0.4898\n",
      "[iter 200] loss=4.0805 val_loss=0.0000 scale=0.5000 norm=0.4866\n",
      "[iter 300] loss=4.0734 val_loss=0.0000 scale=1.0000 norm=0.9672\n",
      "[iter 400] loss=4.0677 val_loss=0.0000 scale=1.0000 norm=0.9628\n",
      "[iter 0] loss=4.0836 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0742 val_loss=0.0000 scale=0.5000 norm=0.4841\n",
      "[iter 200] loss=4.0671 val_loss=0.0000 scale=0.5000 norm=0.4808\n",
      "[iter 300] loss=4.0611 val_loss=0.0000 scale=1.0000 norm=0.9565\n",
      "[iter 400] loss=4.0560 val_loss=0.0000 scale=0.5000 norm=0.4760\n",
      "[iter 0] loss=4.0930 val_loss=0.0000 scale=0.5000 norm=0.4987\n",
      "[iter 100] loss=4.0811 val_loss=0.0000 scale=1.0000 norm=0.9786\n",
      "[iter 200] loss=4.0736 val_loss=0.0000 scale=1.0000 norm=0.9719\n",
      "[iter 300] loss=4.0672 val_loss=0.0000 scale=1.0000 norm=0.9669\n",
      "[iter 400] loss=4.0614 val_loss=0.0000 scale=1.0000 norm=0.9619\n",
      "[iter 0] loss=4.0924 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0823 val_loss=0.0000 scale=0.5000 norm=0.4864\n",
      "[iter 200] loss=4.0748 val_loss=0.0000 scale=0.5000 norm=0.4826\n",
      "[iter 300] loss=4.0681 val_loss=0.0000 scale=1.0000 norm=0.9594\n",
      "[iter 400] loss=4.0622 val_loss=0.0000 scale=1.0000 norm=0.9543\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=1.0000 norm=11.6644\n",
      "[iter 100] loss=4.0781 val_loss=0.0000 scale=1.0000 norm=11.4139\n",
      "[iter 200] loss=4.0365 val_loss=0.0000 scale=1.0000 norm=10.9937\n",
      "[iter 300] loss=3.9748 val_loss=0.0000 scale=1.0000 norm=10.2803\n",
      "[iter 400] loss=4.0013 val_loss=0.0000 scale=1.0000 norm=10.6585\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=1.0000 norm=11.7933\n",
      "[iter 100] loss=4.0656 val_loss=0.0000 scale=1.0000 norm=11.3067\n",
      "[iter 200] loss=4.0405 val_loss=0.0000 scale=1.0000 norm=11.0746\n",
      "[iter 300] loss=4.0111 val_loss=0.0000 scale=2.0000 norm=21.4393\n",
      "[iter 400] loss=4.0006 val_loss=0.0000 scale=1.0000 norm=10.6239\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=1.0000 norm=12.0439\n",
      "[iter 100] loss=4.0413 val_loss=0.0000 scale=1.0000 norm=11.1159\n",
      "[iter 200] loss=4.0259 val_loss=0.0000 scale=1.0000 norm=10.9442\n",
      "[iter 300] loss=3.9769 val_loss=0.0000 scale=1.0000 norm=10.3504\n",
      "[iter 400] loss=4.0008 val_loss=0.0000 scale=1.0000 norm=10.6966\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=1.0000 norm=11.9046\n",
      "[iter 100] loss=4.0632 val_loss=0.0000 scale=1.0000 norm=11.3379\n",
      "[iter 200] loss=4.0302 val_loss=0.0000 scale=1.0000 norm=10.9039\n",
      "[iter 300] loss=3.9935 val_loss=0.0000 scale=1.0000 norm=10.5606\n",
      "[iter 400] loss=3.9931 val_loss=0.0000 scale=1.0000 norm=10.5018\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=1.0000 norm=11.7968\n",
      "[iter 100] loss=4.0691 val_loss=0.0000 scale=1.0000 norm=11.4036\n",
      "[iter 200] loss=4.0235 val_loss=0.0000 scale=1.0000 norm=10.8128\n",
      "[iter 300] loss=4.0195 val_loss=0.0000 scale=1.0000 norm=10.8658\n",
      "[iter 400] loss=3.9979 val_loss=0.0000 scale=1.0000 norm=10.6619\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=1.0000 norm=11.6644\n",
      "[iter 100] loss=4.0781 val_loss=0.0000 scale=1.0000 norm=11.4138\n",
      "[iter 200] loss=4.0363 val_loss=0.0000 scale=1.0000 norm=10.9937\n",
      "[iter 300] loss=3.9746 val_loss=0.0000 scale=1.0000 norm=10.2803\n",
      "[iter 400] loss=4.0013 val_loss=0.0000 scale=1.0000 norm=10.6584\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=1.0000 norm=11.7933\n",
      "[iter 100] loss=4.0656 val_loss=0.0000 scale=1.0000 norm=11.3067\n",
      "[iter 200] loss=4.0406 val_loss=0.0000 scale=1.0000 norm=11.0746\n",
      "[iter 300] loss=4.0110 val_loss=0.0000 scale=2.0000 norm=21.4366\n",
      "[iter 400] loss=4.0009 val_loss=0.0000 scale=1.0000 norm=10.6269\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=1.0000 norm=12.0439\n",
      "[iter 100] loss=4.0413 val_loss=0.0000 scale=1.0000 norm=11.1159\n",
      "[iter 200] loss=4.0261 val_loss=0.0000 scale=1.0000 norm=10.9442\n",
      "[iter 300] loss=3.9766 val_loss=0.0000 scale=1.0000 norm=10.3504\n",
      "[iter 400] loss=4.0004 val_loss=0.0000 scale=1.0000 norm=10.6952\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=1.0000 norm=11.9046\n",
      "[iter 100] loss=4.0633 val_loss=0.0000 scale=1.0000 norm=11.3379\n",
      "[iter 200] loss=4.0303 val_loss=0.0000 scale=1.0000 norm=10.9039\n",
      "[iter 300] loss=3.9932 val_loss=0.0000 scale=1.0000 norm=10.5581\n",
      "[iter 400] loss=3.9931 val_loss=0.0000 scale=1.0000 norm=10.5001\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=1.0000 norm=11.7968\n",
      "[iter 100] loss=4.0690 val_loss=0.0000 scale=1.0000 norm=11.4036\n",
      "[iter 200] loss=4.0233 val_loss=0.0000 scale=1.0000 norm=10.8130\n",
      "[iter 300] loss=4.0195 val_loss=0.0000 scale=1.0000 norm=10.8657\n",
      "[iter 400] loss=3.9976 val_loss=0.0000 scale=1.0000 norm=10.6618\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=1.0000 norm=11.6644\n",
      "[iter 100] loss=4.0781 val_loss=0.0000 scale=1.0000 norm=11.4139\n",
      "[iter 200] loss=4.0364 val_loss=0.0000 scale=1.0000 norm=10.9937\n",
      "[iter 300] loss=3.9748 val_loss=0.0000 scale=1.0000 norm=10.2803\n",
      "[iter 400] loss=4.0012 val_loss=0.0000 scale=1.0000 norm=10.6595\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=1.0000 norm=11.7933\n",
      "[iter 100] loss=4.0657 val_loss=0.0000 scale=1.0000 norm=11.3067\n",
      "[iter 200] loss=4.0405 val_loss=0.0000 scale=1.0000 norm=11.0754\n",
      "[iter 300] loss=4.0112 val_loss=0.0000 scale=2.0000 norm=21.4432\n",
      "[iter 400] loss=4.0005 val_loss=0.0000 scale=1.0000 norm=10.6277\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=1.0000 norm=12.0439\n",
      "[iter 100] loss=4.0413 val_loss=0.0000 scale=1.0000 norm=11.1159\n",
      "[iter 200] loss=4.0259 val_loss=0.0000 scale=1.0000 norm=10.9442\n",
      "[iter 300] loss=3.9767 val_loss=0.0000 scale=1.0000 norm=10.3504\n",
      "[iter 400] loss=4.0008 val_loss=0.0000 scale=1.0000 norm=10.6968\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=1.0000 norm=11.9046\n",
      "[iter 100] loss=4.0634 val_loss=0.0000 scale=1.0000 norm=11.3379\n",
      "[iter 200] loss=4.0301 val_loss=0.0000 scale=1.0000 norm=10.9039\n",
      "[iter 300] loss=3.9933 val_loss=0.0000 scale=1.0000 norm=10.5606\n",
      "[iter 400] loss=3.9935 val_loss=0.0000 scale=1.0000 norm=10.5021\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=1.0000 norm=11.7968\n",
      "[iter 100] loss=4.0689 val_loss=0.0000 scale=1.0000 norm=11.4036\n",
      "[iter 200] loss=4.0232 val_loss=0.0000 scale=1.0000 norm=10.8128\n",
      "[iter 300] loss=4.0194 val_loss=0.0000 scale=1.0000 norm=10.8658\n",
      "[iter 400] loss=3.9978 val_loss=0.0000 scale=1.0000 norm=10.6617\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=0.5000 norm=0.4916\n",
      "[iter 100] loss=4.1100 val_loss=0.0000 scale=0.5000 norm=0.5061\n",
      "[iter 200] loss=4.0923 val_loss=0.0000 scale=0.5000 norm=0.4878\n",
      "[iter 300] loss=4.0490 val_loss=0.0000 scale=1.0000 norm=0.9175\n",
      "[iter 400] loss=4.0864 val_loss=0.0000 scale=0.5000 norm=0.4819\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=0.5000 norm=0.4898\n",
      "[iter 100] loss=4.0975 val_loss=0.0000 scale=0.5000 norm=0.4989\n",
      "[iter 200] loss=4.0893 val_loss=0.0000 scale=0.5000 norm=0.4849\n",
      "[iter 300] loss=4.0803 val_loss=0.0000 scale=0.5000 norm=0.4821\n",
      "[iter 400] loss=4.0837 val_loss=0.0000 scale=0.5000 norm=0.4882\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=0.5000 norm=0.4894\n",
      "[iter 100] loss=4.0693 val_loss=0.0000 scale=0.5000 norm=0.4798\n",
      "[iter 200] loss=4.0708 val_loss=0.0000 scale=0.5000 norm=0.4747\n",
      "[iter 300] loss=4.0405 val_loss=0.0000 scale=0.5000 norm=0.4616\n",
      "[iter 400] loss=4.0841 val_loss=0.0000 scale=0.5000 norm=0.4893\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0906 val_loss=0.0000 scale=0.5000 norm=0.4919\n",
      "[iter 200] loss=4.0746 val_loss=0.0000 scale=0.5000 norm=0.4856\n",
      "[iter 300] loss=4.0540 val_loss=0.0000 scale=0.5000 norm=0.4652\n",
      "[iter 400] loss=4.0722 val_loss=0.0000 scale=0.5000 norm=0.4885\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=0.5000 norm=0.4908\n",
      "[iter 100] loss=4.1020 val_loss=0.0000 scale=0.5000 norm=0.4946\n",
      "[iter 200] loss=4.0753 val_loss=0.0000 scale=0.5000 norm=0.4855\n",
      "[iter 300] loss=4.0855 val_loss=0.0000 scale=0.5000 norm=0.4831\n",
      "[iter 400] loss=4.0813 val_loss=0.0000 scale=0.5000 norm=0.4830\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=0.5000 norm=0.4916\n",
      "[iter 100] loss=4.1101 val_loss=0.0000 scale=0.5000 norm=0.5063\n",
      "[iter 200] loss=4.0922 val_loss=0.0000 scale=0.5000 norm=0.4877\n",
      "[iter 300] loss=4.0495 val_loss=0.0000 scale=1.0000 norm=0.9187\n",
      "[iter 400] loss=4.0867 val_loss=0.0000 scale=0.5000 norm=0.4819\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=0.5000 norm=0.4898\n",
      "[iter 100] loss=4.0976 val_loss=0.0000 scale=0.5000 norm=0.4991\n",
      "[iter 200] loss=4.0897 val_loss=0.0000 scale=0.5000 norm=0.4853\n",
      "[iter 300] loss=4.0801 val_loss=0.0000 scale=0.5000 norm=0.4819\n",
      "[iter 400] loss=4.0838 val_loss=0.0000 scale=0.5000 norm=0.4879\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=0.5000 norm=0.4894\n",
      "[iter 100] loss=4.0693 val_loss=0.0000 scale=0.5000 norm=0.4799\n",
      "[iter 200] loss=4.0707 val_loss=0.0000 scale=0.5000 norm=0.4747\n",
      "[iter 300] loss=4.0406 val_loss=0.0000 scale=0.5000 norm=0.4617\n",
      "[iter 400] loss=4.0842 val_loss=0.0000 scale=0.5000 norm=0.4895\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0906 val_loss=0.0000 scale=0.5000 norm=0.4920\n",
      "[iter 200] loss=4.0746 val_loss=0.0000 scale=0.5000 norm=0.4856\n",
      "[iter 300] loss=4.0538 val_loss=0.0000 scale=0.5000 norm=0.4649\n",
      "[iter 400] loss=4.0722 val_loss=0.0000 scale=0.5000 norm=0.4886\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=0.5000 norm=0.4908\n",
      "[iter 100] loss=4.1020 val_loss=0.0000 scale=0.5000 norm=0.4945\n",
      "[iter 200] loss=4.0754 val_loss=0.0000 scale=0.5000 norm=0.4855\n",
      "[iter 300] loss=4.0856 val_loss=0.0000 scale=0.5000 norm=0.4832\n",
      "[iter 400] loss=4.0814 val_loss=0.0000 scale=0.5000 norm=0.4833\n",
      "[iter 0] loss=4.0953 val_loss=0.0000 scale=0.5000 norm=0.4916\n",
      "[iter 100] loss=4.1102 val_loss=0.0000 scale=0.5000 norm=0.5063\n",
      "[iter 200] loss=4.0923 val_loss=0.0000 scale=0.5000 norm=0.4877\n",
      "[iter 300] loss=4.0493 val_loss=0.0000 scale=1.0000 norm=0.9181\n",
      "[iter 400] loss=4.0870 val_loss=0.0000 scale=0.5000 norm=0.4825\n",
      "[iter 0] loss=4.1019 val_loss=0.0000 scale=0.5000 norm=0.4898\n",
      "[iter 100] loss=4.0976 val_loss=0.0000 scale=0.5000 norm=0.4990\n",
      "[iter 200] loss=4.0894 val_loss=0.0000 scale=0.5000 norm=0.4850\n",
      "[iter 300] loss=4.0800 val_loss=0.0000 scale=0.5000 norm=0.4819\n",
      "[iter 400] loss=4.0838 val_loss=0.0000 scale=0.5000 norm=0.4883\n",
      "[iter 0] loss=4.1117 val_loss=0.0000 scale=0.5000 norm=0.4894\n",
      "[iter 100] loss=4.0692 val_loss=0.0000 scale=0.5000 norm=0.4797\n",
      "[iter 200] loss=4.0707 val_loss=0.0000 scale=0.5000 norm=0.4747\n",
      "[iter 300] loss=4.0406 val_loss=0.0000 scale=0.5000 norm=0.4619\n",
      "[iter 400] loss=4.0840 val_loss=0.0000 scale=0.5000 norm=0.4892\n",
      "[iter 0] loss=4.1104 val_loss=0.0000 scale=0.5000 norm=0.4918\n",
      "[iter 100] loss=4.0907 val_loss=0.0000 scale=0.5000 norm=0.4921\n",
      "[iter 200] loss=4.0744 val_loss=0.0000 scale=0.5000 norm=0.4853\n",
      "[iter 300] loss=4.0536 val_loss=0.0000 scale=0.5000 norm=0.4647\n",
      "[iter 400] loss=4.0725 val_loss=0.0000 scale=0.5000 norm=0.4889\n",
      "[iter 0] loss=4.1027 val_loss=0.0000 scale=0.5000 norm=0.4908\n",
      "[iter 100] loss=4.1019 val_loss=0.0000 scale=0.5000 norm=0.4944\n",
      "[iter 200] loss=4.0755 val_loss=0.0000 scale=0.5000 norm=0.4856\n",
      "[iter 300] loss=4.0857 val_loss=0.0000 scale=0.5000 norm=0.4832\n",
      "[iter 400] loss=4.0816 val_loss=0.0000 scale=0.5000 norm=0.4835\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=1.0000 norm=11.5815\n",
      "[iter 100] loss=4.0641 val_loss=0.0000 scale=1.0000 norm=11.1885\n",
      "[iter 200] loss=4.0445 val_loss=0.0000 scale=1.0000 norm=11.0198\n",
      "[iter 300] loss=4.0013 val_loss=0.0000 scale=1.0000 norm=10.5661\n",
      "[iter 400] loss=4.0088 val_loss=0.0000 scale=1.0000 norm=10.7229\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=1.0000 norm=11.7758\n",
      "[iter 100] loss=4.0623 val_loss=0.0000 scale=1.0000 norm=11.2475\n",
      "[iter 200] loss=4.0354 val_loss=0.0000 scale=1.0000 norm=10.9808\n",
      "[iter 300] loss=4.0139 val_loss=0.0000 scale=1.0000 norm=10.7591\n",
      "[iter 400] loss=4.0059 val_loss=0.0000 scale=1.0000 norm=10.6939\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=1.0000 norm=11.8357\n",
      "[iter 100] loss=4.0464 val_loss=0.0000 scale=1.0000 norm=11.1430\n",
      "[iter 200] loss=4.0223 val_loss=0.0000 scale=1.0000 norm=10.8892\n",
      "[iter 300] loss=3.9860 val_loss=0.0000 scale=1.0000 norm=10.5092\n",
      "[iter 400] loss=4.0009 val_loss=0.0000 scale=1.0000 norm=10.7296\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=1.0000 norm=11.7229\n",
      "[iter 100] loss=4.0581 val_loss=0.0000 scale=1.0000 norm=11.2419\n",
      "[iter 200] loss=4.0290 val_loss=0.0000 scale=1.0000 norm=10.8596\n",
      "[iter 300] loss=3.9999 val_loss=0.0000 scale=1.0000 norm=10.6304\n",
      "[iter 400] loss=4.0035 val_loss=0.0000 scale=1.0000 norm=10.6788\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=1.0000 norm=11.7478\n",
      "[iter 100] loss=4.0608 val_loss=0.0000 scale=1.0000 norm=11.2952\n",
      "[iter 200] loss=4.0175 val_loss=0.0000 scale=1.0000 norm=10.7052\n",
      "[iter 300] loss=4.0073 val_loss=0.0000 scale=1.0000 norm=10.6941\n",
      "[iter 400] loss=4.0049 val_loss=0.0000 scale=1.0000 norm=10.7446\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=1.0000 norm=11.5815\n",
      "[iter 100] loss=4.0641 val_loss=0.0000 scale=1.0000 norm=11.1885\n",
      "[iter 200] loss=4.0445 val_loss=0.0000 scale=1.0000 norm=11.0198\n",
      "[iter 300] loss=4.0013 val_loss=0.0000 scale=1.0000 norm=10.5661\n",
      "[iter 400] loss=4.0087 val_loss=0.0000 scale=1.0000 norm=10.7229\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=1.0000 norm=11.7758\n",
      "[iter 100] loss=4.0623 val_loss=0.0000 scale=1.0000 norm=11.2475\n",
      "[iter 200] loss=4.0354 val_loss=0.0000 scale=1.0000 norm=10.9808\n",
      "[iter 300] loss=4.0139 val_loss=0.0000 scale=1.0000 norm=10.7591\n",
      "[iter 400] loss=4.0059 val_loss=0.0000 scale=1.0000 norm=10.6939\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=1.0000 norm=11.8357\n",
      "[iter 100] loss=4.0466 val_loss=0.0000 scale=1.0000 norm=11.1431\n",
      "[iter 200] loss=4.0224 val_loss=0.0000 scale=1.0000 norm=10.8892\n",
      "[iter 300] loss=3.9860 val_loss=0.0000 scale=1.0000 norm=10.5092\n",
      "[iter 400] loss=4.0010 val_loss=0.0000 scale=1.0000 norm=10.7296\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=1.0000 norm=11.7229\n",
      "[iter 100] loss=4.0581 val_loss=0.0000 scale=1.0000 norm=11.2419\n",
      "[iter 200] loss=4.0289 val_loss=0.0000 scale=1.0000 norm=10.8596\n",
      "[iter 300] loss=3.9999 val_loss=0.0000 scale=1.0000 norm=10.6304\n",
      "[iter 400] loss=4.0035 val_loss=0.0000 scale=1.0000 norm=10.6786\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=1.0000 norm=11.7478\n",
      "[iter 100] loss=4.0608 val_loss=0.0000 scale=1.0000 norm=11.2952\n",
      "[iter 200] loss=4.0175 val_loss=0.0000 scale=1.0000 norm=10.7052\n",
      "[iter 300] loss=4.0074 val_loss=0.0000 scale=1.0000 norm=10.6937\n",
      "[iter 400] loss=4.0048 val_loss=0.0000 scale=1.0000 norm=10.7441\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=1.0000 norm=11.5815\n",
      "[iter 100] loss=4.0641 val_loss=0.0000 scale=1.0000 norm=11.1885\n",
      "[iter 200] loss=4.0445 val_loss=0.0000 scale=1.0000 norm=11.0198\n",
      "[iter 300] loss=4.0012 val_loss=0.0000 scale=1.0000 norm=10.5661\n",
      "[iter 400] loss=4.0084 val_loss=0.0000 scale=1.0000 norm=10.7217\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=1.0000 norm=11.7758\n",
      "[iter 100] loss=4.0623 val_loss=0.0000 scale=1.0000 norm=11.2475\n",
      "[iter 200] loss=4.0354 val_loss=0.0000 scale=1.0000 norm=10.9808\n",
      "[iter 300] loss=4.0141 val_loss=0.0000 scale=1.0000 norm=10.7619\n",
      "[iter 400] loss=4.0063 val_loss=0.0000 scale=1.0000 norm=10.6977\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=1.0000 norm=11.8357\n",
      "[iter 100] loss=4.0466 val_loss=0.0000 scale=1.0000 norm=11.1431\n",
      "[iter 200] loss=4.0224 val_loss=0.0000 scale=1.0000 norm=10.8892\n",
      "[iter 300] loss=3.9860 val_loss=0.0000 scale=1.0000 norm=10.5092\n",
      "[iter 400] loss=4.0009 val_loss=0.0000 scale=1.0000 norm=10.7296\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=1.0000 norm=11.7229\n",
      "[iter 100] loss=4.0582 val_loss=0.0000 scale=1.0000 norm=11.2419\n",
      "[iter 200] loss=4.0290 val_loss=0.0000 scale=1.0000 norm=10.8596\n",
      "[iter 300] loss=4.0000 val_loss=0.0000 scale=1.0000 norm=10.6304\n",
      "[iter 400] loss=4.0035 val_loss=0.0000 scale=1.0000 norm=10.6788\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=1.0000 norm=11.7478\n",
      "[iter 100] loss=4.0608 val_loss=0.0000 scale=1.0000 norm=11.2952\n",
      "[iter 200] loss=4.0175 val_loss=0.0000 scale=1.0000 norm=10.7052\n",
      "[iter 300] loss=4.0073 val_loss=0.0000 scale=1.0000 norm=10.6941\n",
      "[iter 400] loss=4.0049 val_loss=0.0000 scale=1.0000 norm=10.7446\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=0.5000 norm=0.4880\n",
      "[iter 100] loss=4.0964 val_loss=0.0000 scale=0.5000 norm=0.5015\n",
      "[iter 200] loss=4.0983 val_loss=0.0000 scale=0.5000 norm=0.4988\n",
      "[iter 300] loss=4.0687 val_loss=0.0000 scale=1.0000 norm=0.9539\n",
      "[iter 400] loss=4.0891 val_loss=0.0000 scale=0.5000 norm=0.4856\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=0.5000 norm=0.5021\n",
      "[iter 100] loss=4.0932 val_loss=0.0000 scale=0.5000 norm=0.4951\n",
      "[iter 200] loss=4.0848 val_loss=0.0000 scale=0.5000 norm=0.4838\n",
      "[iter 300] loss=4.0822 val_loss=0.0000 scale=0.5000 norm=0.4868\n",
      "[iter 400] loss=4.0857 val_loss=0.0000 scale=0.5000 norm=0.4879\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=0.5000 norm=0.4960\n",
      "[iter 100] loss=4.0747 val_loss=0.0000 scale=0.5000 norm=0.4842\n",
      "[iter 200] loss=4.0696 val_loss=0.0000 scale=0.5000 norm=0.4781\n",
      "[iter 300] loss=4.0478 val_loss=0.0000 scale=0.5000 norm=0.4673\n",
      "[iter 400] loss=4.0779 val_loss=0.0000 scale=0.5000 norm=0.4826\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=0.5000 norm=0.4996\n",
      "[iter 100] loss=4.0849 val_loss=0.0000 scale=0.5000 norm=0.4888\n",
      "[iter 200] loss=4.0759 val_loss=0.0000 scale=0.5000 norm=0.4890\n",
      "[iter 300] loss=4.0608 val_loss=0.0000 scale=0.5000 norm=0.4751\n",
      "[iter 400] loss=4.0774 val_loss=0.0000 scale=0.5000 norm=0.4866\n",
      "[iter 0] loss=4.1023 val_loss=0.0000 scale=0.5000 norm=0.4949\n",
      "[iter 100] loss=4.0914 val_loss=0.0000 scale=0.5000 norm=0.4880\n",
      "[iter 200] loss=4.0702 val_loss=0.0000 scale=0.5000 norm=0.4858\n",
      "[iter 300] loss=4.0716 val_loss=0.0000 scale=1.0000 norm=0.9611\n",
      "[iter 400] loss=4.0813 val_loss=0.0000 scale=0.5000 norm=0.4805\n",
      "[iter 0] loss=4.0887 val_loss=0.0000 scale=0.5000 norm=0.4880\n",
      "[iter 100] loss=4.0965 val_loss=0.0000 scale=0.5000 norm=0.5017\n",
      "[iter 200] loss=4.0982 val_loss=0.0000 scale=0.5000 norm=0.4986\n",
      "[iter 300] loss=4.0689 val_loss=0.0000 scale=1.0000 norm=0.9545\n",
      "[iter 400] loss=4.0892 val_loss=0.0000 scale=0.5000 norm=0.4857\n",
      "[iter 0] loss=4.1079 val_loss=0.0000 scale=0.5000 norm=0.5021\n",
      "[iter 100] loss=4.0932 val_loss=0.0000 scale=0.5000 norm=0.4952\n",
      "[iter 200] loss=4.0848 val_loss=0.0000 scale=0.5000 norm=0.4839\n",
      "[iter 300] loss=4.0821 val_loss=0.0000 scale=0.5000 norm=0.4867\n",
      "[iter 400] loss=4.0856 val_loss=0.0000 scale=0.5000 norm=0.4879\n",
      "[iter 0] loss=4.1036 val_loss=0.0000 scale=0.5000 norm=0.4960\n",
      "[iter 100] loss=4.0747 val_loss=0.0000 scale=0.5000 norm=0.4842\n",
      "[iter 200] loss=4.0696 val_loss=0.0000 scale=0.5000 norm=0.4781\n",
      "[iter 300] loss=4.0478 val_loss=0.0000 scale=0.5000 norm=0.4673\n",
      "[iter 400] loss=4.0779 val_loss=0.0000 scale=0.5000 norm=0.4825\n",
      "[iter 0] loss=4.1035 val_loss=0.0000 scale=0.5000 norm=0.4996\n",
      "[iter 100] loss=4.0848 val_loss=0.0000 scale=0.5000 norm=0.4888\n",
      "[iter 200] loss=4.0759 val_loss=0.0000 scale=0.5000 norm=0.4891\n",
      "[iter 300] loss=4.0608 val_loss=0.0000 scale=0.5000 norm=0.4752\n",
      "[iter 400] loss=4.0773 val_loss=0.0000 scale=0.5000 norm=0.4866\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/ngboost/ngboost.py:321\u001b[0m, in \u001b[0;36mNGBoost.fit\u001b[0;34m(self, X, Y, X_val, Y_val, sample_weight, val_sample_weight, train_loss_monitor, val_loss_monitor, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    315\u001b[0m scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_search(proj_grad, P_batch, Y_batch, weight_batch)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# pdb.set_trace()\u001b[39;00m\n\u001b[1;32m    318\u001b[0m params \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;241m*\u001b[39m scale\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39marray([m\u001b[38;5;241m.\u001b[39mpredict(X[:, col_idx]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_models[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    322\u001b[0m )\n\u001b[1;32m    324\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m Y_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/ngboost/ngboost.py:321\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    315\u001b[0m scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_search(proj_grad, P_batch, Y_batch, weight_batch)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# pdb.set_trace()\u001b[39;00m\n\u001b[1;32m    318\u001b[0m params \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;241m*\u001b[39m scale\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_models[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    322\u001b[0m )\n\u001b[1;32m    324\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m Y_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/sklearn/tree/_classes.py:426\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    425\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m    428\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/sklearn/tree/_classes.py:392\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_input:\n\u001b[0;32m--> 392\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    394\u001b[0m         X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc\n\u001b[1;32m    395\u001b[0m     ):\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/sklearn/base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 546\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/sklearn/utils/validation.py:821\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforce_all_finite should be a bool or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             force_all_finite\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m estimator_name \u001b[38;5;241m=\u001b[39m \u001b[43m_check_estimator_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m estimator_name \u001b[38;5;28;01mif\u001b[39;00m estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;66;03m# When all dataframe columns are sparse, convert to a sparse array\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/nba_betting_analysis/lib/python3.10/site-packages/sklearn/utils/validation.py:581\u001b[0m, in \u001b[0;36m_check_estimator_name\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28mhasattr\u001b[39m(array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array))\n\u001b[0;32m--> 581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_estimator_name\u001b[39m(estimator):\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(estimator, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "44718df4-70e0-4ef7-bc29-a8ecca2330d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params'"
     ]
    }
   ],
   "source": [
    "grid_search.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c98c1-4d34-4849-81e7-8ebf14fcc529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b60b94a88b06cb6055855c21d204c5d44d1f1cdd16a36317c55022353c0fb7c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
