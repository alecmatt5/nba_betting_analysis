{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f731556",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd4c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e86c3d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import ngboost as ngb\n",
    "import xgboost as xgb\n",
    "from hyperopt import Trials, STATUS_OK, tpe, hp, fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f90ee",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e722e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLAYER_ID</th>\n",
       "      <th>TEAM_ID</th>\n",
       "      <th>TEAM_ABBREVIATION</th>\n",
       "      <th>GAME_DATE</th>\n",
       "      <th>PLAYER_NAME</th>\n",
       "      <th>PLUS_MINUS</th>\n",
       "      <th>AST_PCT</th>\n",
       "      <th>AST_RATIO</th>\n",
       "      <th>AST_TOV</th>\n",
       "      <th>DEF_RATING</th>\n",
       "      <th>...</th>\n",
       "      <th>OFF_RATING</th>\n",
       "      <th>OREB_PCT</th>\n",
       "      <th>PACE</th>\n",
       "      <th>PIE</th>\n",
       "      <th>POSS</th>\n",
       "      <th>PTS</th>\n",
       "      <th>REB_PCT</th>\n",
       "      <th>TM_TOV_PCT</th>\n",
       "      <th>TS_PCT</th>\n",
       "      <th>USG_PCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1713</td>\n",
       "      <td>1610612737</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>Vince Carter</td>\n",
       "      <td>-14.9</td>\n",
       "      <td>0.123266</td>\n",
       "      <td>0.169653</td>\n",
       "      <td>0.042814</td>\n",
       "      <td>0.578202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587002</td>\n",
       "      <td>0.050330</td>\n",
       "      <td>0.014105</td>\n",
       "      <td>0.542849</td>\n",
       "      <td>0.347285</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.228959</td>\n",
       "      <td>0.072430</td>\n",
       "      <td>0.431366</td>\n",
       "      <td>0.376921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1713</td>\n",
       "      <td>1610612737</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2018-12-05</td>\n",
       "      <td>Vince Carter</td>\n",
       "      <td>-14.8</td>\n",
       "      <td>0.132462</td>\n",
       "      <td>0.147789</td>\n",
       "      <td>0.042814</td>\n",
       "      <td>0.546110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573205</td>\n",
       "      <td>0.064214</td>\n",
       "      <td>0.013059</td>\n",
       "      <td>0.548627</td>\n",
       "      <td>0.383484</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.231373</td>\n",
       "      <td>0.072430</td>\n",
       "      <td>0.465736</td>\n",
       "      <td>0.384718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1713</td>\n",
       "      <td>1610612737</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2018-12-08</td>\n",
       "      <td>Vince Carter</td>\n",
       "      <td>-13.9</td>\n",
       "      <td>0.147144</td>\n",
       "      <td>0.161434</td>\n",
       "      <td>0.057086</td>\n",
       "      <td>0.548544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577199</td>\n",
       "      <td>0.083652</td>\n",
       "      <td>0.013363</td>\n",
       "      <td>0.554863</td>\n",
       "      <td>0.373303</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.249472</td>\n",
       "      <td>0.090059</td>\n",
       "      <td>0.499260</td>\n",
       "      <td>0.418802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1713</td>\n",
       "      <td>1610612737</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>Vince Carter</td>\n",
       "      <td>-13.9</td>\n",
       "      <td>0.125686</td>\n",
       "      <td>0.163571</td>\n",
       "      <td>0.014271</td>\n",
       "      <td>0.554043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561224</td>\n",
       "      <td>0.065255</td>\n",
       "      <td>0.014036</td>\n",
       "      <td>0.548169</td>\n",
       "      <td>0.368778</td>\n",
       "      <td>0.445312</td>\n",
       "      <td>0.231071</td>\n",
       "      <td>0.070731</td>\n",
       "      <td>0.447864</td>\n",
       "      <td>0.404767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1713</td>\n",
       "      <td>1610612737</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2018-12-14</td>\n",
       "      <td>Vince Carter</td>\n",
       "      <td>-11.5</td>\n",
       "      <td>0.125686</td>\n",
       "      <td>0.163571</td>\n",
       "      <td>0.014271</td>\n",
       "      <td>0.595691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566125</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.013775</td>\n",
       "      <td>0.547654</td>\n",
       "      <td>0.367647</td>\n",
       "      <td>0.474609</td>\n",
       "      <td>0.221418</td>\n",
       "      <td>0.070731</td>\n",
       "      <td>0.422377</td>\n",
       "      <td>0.418133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100079</th>\n",
       "      <td>1631323</td>\n",
       "      <td>1610612762</td>\n",
       "      <td>UTA</td>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>Simone Fontecchio</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.092611</td>\n",
       "      <td>0.167352</td>\n",
       "      <td>0.071357</td>\n",
       "      <td>0.566844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706635</td>\n",
       "      <td>0.133287</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>0.522883</td>\n",
       "      <td>0.263575</td>\n",
       "      <td>0.759766</td>\n",
       "      <td>0.171342</td>\n",
       "      <td>0.267842</td>\n",
       "      <td>0.421003</td>\n",
       "      <td>0.387391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100080</th>\n",
       "      <td>1631323</td>\n",
       "      <td>1610612762</td>\n",
       "      <td>UTA</td>\n",
       "      <td>2023-02-25</td>\n",
       "      <td>Simone Fontecchio</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.190860</td>\n",
       "      <td>0.071357</td>\n",
       "      <td>0.579735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770627</td>\n",
       "      <td>0.152725</td>\n",
       "      <td>0.012945</td>\n",
       "      <td>0.538387</td>\n",
       "      <td>0.277149</td>\n",
       "      <td>0.761719</td>\n",
       "      <td>0.182202</td>\n",
       "      <td>0.214741</td>\n",
       "      <td>0.455690</td>\n",
       "      <td>0.367788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100081</th>\n",
       "      <td>1631323</td>\n",
       "      <td>1610612762</td>\n",
       "      <td>UTA</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>Simone Fontecchio</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.128751</td>\n",
       "      <td>0.218313</td>\n",
       "      <td>0.080919</td>\n",
       "      <td>0.630758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761732</td>\n",
       "      <td>0.152725</td>\n",
       "      <td>0.012803</td>\n",
       "      <td>0.522654</td>\n",
       "      <td>0.319005</td>\n",
       "      <td>0.699219</td>\n",
       "      <td>0.139065</td>\n",
       "      <td>0.267842</td>\n",
       "      <td>0.402813</td>\n",
       "      <td>0.317665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100082</th>\n",
       "      <td>1631323</td>\n",
       "      <td>1610612762</td>\n",
       "      <td>UTA</td>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>Simone Fontecchio</td>\n",
       "      <td>-5.3</td>\n",
       "      <td>0.139561</td>\n",
       "      <td>0.228670</td>\n",
       "      <td>0.095191</td>\n",
       "      <td>0.612729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773260</td>\n",
       "      <td>0.096147</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.523112</td>\n",
       "      <td>0.369910</td>\n",
       "      <td>0.666016</td>\n",
       "      <td>0.080543</td>\n",
       "      <td>0.139550</td>\n",
       "      <td>0.305626</td>\n",
       "      <td>0.312542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100083</th>\n",
       "      <td>1631323</td>\n",
       "      <td>1610612762</td>\n",
       "      <td>UTA</td>\n",
       "      <td>2023-03-05</td>\n",
       "      <td>Simone Fontecchio</td>\n",
       "      <td>-5.3</td>\n",
       "      <td>0.168925</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>0.095191</td>\n",
       "      <td>0.641936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687574</td>\n",
       "      <td>0.096147</td>\n",
       "      <td>0.011432</td>\n",
       "      <td>0.523112</td>\n",
       "      <td>0.408371</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.090498</td>\n",
       "      <td>0.139550</td>\n",
       "      <td>0.343380</td>\n",
       "      <td>0.350635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100084 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PLAYER_ID     TEAM_ID TEAM_ABBREVIATION  GAME_DATE        PLAYER_NAME  \\\n",
       "0            1713  1610612737               ATL 2018-12-03       Vince Carter   \n",
       "1            1713  1610612737               ATL 2018-12-05       Vince Carter   \n",
       "2            1713  1610612737               ATL 2018-12-08       Vince Carter   \n",
       "3            1713  1610612737               ATL 2018-12-12       Vince Carter   \n",
       "4            1713  1610612737               ATL 2018-12-14       Vince Carter   \n",
       "...           ...         ...               ...        ...                ...   \n",
       "100079    1631323  1610612762               UTA 2023-02-23  Simone Fontecchio   \n",
       "100080    1631323  1610612762               UTA 2023-02-25  Simone Fontecchio   \n",
       "100081    1631323  1610612762               UTA 2023-02-28  Simone Fontecchio   \n",
       "100082    1631323  1610612762               UTA 2023-03-03  Simone Fontecchio   \n",
       "100083    1631323  1610612762               UTA 2023-03-05  Simone Fontecchio   \n",
       "\n",
       "        PLUS_MINUS   AST_PCT  AST_RATIO   AST_TOV  DEF_RATING  ...  \\\n",
       "0            -14.9  0.123266   0.169653  0.042814    0.578202  ...   \n",
       "1            -14.8  0.132462   0.147789  0.042814    0.546110  ...   \n",
       "2            -13.9  0.147144   0.161434  0.057086    0.548544  ...   \n",
       "3            -13.9  0.125686   0.163571  0.014271    0.554043  ...   \n",
       "4            -11.5  0.125686   0.163571  0.014271    0.595691  ...   \n",
       "...            ...       ...        ...       ...         ...  ...   \n",
       "100079         0.6  0.092611   0.167352  0.071357    0.566844  ...   \n",
       "100080         2.3  0.107293   0.190860  0.071357    0.579735  ...   \n",
       "100081        -0.8  0.128751   0.218313  0.080919    0.630758  ...   \n",
       "100082        -5.3  0.139561   0.228670  0.095191    0.612729  ...   \n",
       "100083        -5.3  0.168925   0.265165  0.095191    0.641936  ...   \n",
       "\n",
       "        OFF_RATING  OREB_PCT      PACE       PIE      POSS       PTS  \\\n",
       "0         0.587002  0.050330  0.014105  0.542849  0.347285  0.421875   \n",
       "1         0.573205  0.064214  0.013059  0.548627  0.383484  0.437500   \n",
       "2         0.577199  0.083652  0.013363  0.554863  0.373303  0.437500   \n",
       "3         0.561224  0.065255  0.014036  0.548169  0.368778  0.445312   \n",
       "4         0.566125  0.047900  0.013775  0.547654  0.367647  0.474609   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "100079    0.706635  0.133287  0.012827  0.522883  0.263575  0.759766   \n",
       "100080    0.770627  0.152725  0.012945  0.538387  0.277149  0.761719   \n",
       "100081    0.761732  0.152725  0.012803  0.522654  0.319005  0.699219   \n",
       "100082    0.773260  0.096147  0.011583  0.523112  0.369910  0.666016   \n",
       "100083    0.687574  0.096147  0.011432  0.523112  0.408371  0.656250   \n",
       "\n",
       "         REB_PCT  TM_TOV_PCT    TS_PCT   USG_PCT  \n",
       "0       0.228959    0.072430  0.431366  0.376921  \n",
       "1       0.231373    0.072430  0.465736  0.384718  \n",
       "2       0.249472    0.090059  0.499260  0.418802  \n",
       "3       0.231071    0.070731  0.447864  0.404767  \n",
       "4       0.221418    0.070731  0.422377  0.418133  \n",
       "...          ...         ...       ...       ...  \n",
       "100079  0.171342    0.267842  0.421003  0.387391  \n",
       "100080  0.182202    0.214741  0.455690  0.367788  \n",
       "100081  0.139065    0.267842  0.402813  0.317665  \n",
       "100082  0.080543    0.139550  0.305626  0.312542  \n",
       "100083  0.090498    0.139550  0.343380  0.350635  \n",
       "\n",
       "[100084 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('/Users/lj/code/5pacepenguin/nba_betting_analysis/nba_betting_analysis/backend/data/pkl/scaled_boxscore_advanced_rolling_players.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f81ee91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLAYER_ID                     int64\n",
       "TEAM_ID                       int64\n",
       "TEAM_ABBREVIATION            object\n",
       "GAME_DATE            datetime64[ns]\n",
       "PLAYER_NAME                category\n",
       "PLUS_MINUS                  float64\n",
       "AST_PCT                     float64\n",
       "AST_RATIO                   float64\n",
       "AST_TOV                     float64\n",
       "DEF_RATING                  float64\n",
       "DREB_PCT                    float64\n",
       "EFG_PCT                     float64\n",
       "MIN                         float64\n",
       "NET_RATING                  float64\n",
       "OFF_RATING                  float64\n",
       "OREB_PCT                    float64\n",
       "PACE                        float64\n",
       "PIE                         float64\n",
       "POSS                        float64\n",
       "PTS                         float64\n",
       "REB_PCT                     float64\n",
       "TM_TOV_PCT                  float64\n",
       "TS_PCT                      float64\n",
       "USG_PCT                     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af14f19b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84285cba",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d80c3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from neural_net.ipynb\n",
    "train_test_split_date = '2022-04-27 00:00:00'\n",
    "X_train = df.loc[df.GAME_DATE < train_test_split_date, ~df.columns.isin(['PLUS_MINUS'])]\n",
    "X_test = df.loc[df.GAME_DATE > train_test_split_date, ~df.columns.isin(['PLUS_MINUS'])]\n",
    "y_train = df.loc[df.GAME_DATE < train_test_split_date, ['PLUS_MINUS']]\n",
    "y_test = df.loc[df.GAME_DATE > train_test_split_date, ['PLUS_MINUS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7722c922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       PLAYER_ID     TEAM_ID TEAM_ABBREVIATION  GAME_DATE   PLAYER_NAME  \\\n",
       " 0           1713  1610612737               ATL 2018-12-03  Vince Carter   \n",
       " 1           1713  1610612737               ATL 2018-12-05  Vince Carter   \n",
       " 2           1713  1610612737               ATL 2018-12-08  Vince Carter   \n",
       " 3           1713  1610612737               ATL 2018-12-12  Vince Carter   \n",
       " 4           1713  1610612737               ATL 2018-12-14  Vince Carter   \n",
       " ...          ...         ...               ...        ...           ...   \n",
       " 98794    1630846  1610612760               OKC 2022-03-28  Olivier Sarr   \n",
       " 98795    1630846  1610612760               OKC 2022-03-30  Olivier Sarr   \n",
       " 98796    1630846  1610612760               OKC 2022-04-01  Olivier Sarr   \n",
       " 98797    1630846  1610612760               OKC 2022-04-03  Olivier Sarr   \n",
       " 98798    1630846  1610612760               OKC 2022-04-05  Olivier Sarr   \n",
       " \n",
       "         AST_PCT  AST_RATIO   AST_TOV  DEF_RATING  DREB_PCT  ...  OFF_RATING  \\\n",
       " 0      0.123266   0.169653  0.042814    0.578202  0.250375  ...    0.587002   \n",
       " 1      0.132462   0.147789  0.042814    0.546110  0.255056  ...    0.573205   \n",
       " 2      0.147144   0.161434  0.057086    0.548544  0.268539  ...    0.577199   \n",
       " 3      0.125686   0.163571  0.014271    0.554043  0.256742  ...    0.561224   \n",
       " 4      0.125686   0.163571  0.014271    0.595691  0.256742  ...    0.566125   \n",
       " ...         ...        ...       ...         ...       ...  ...         ...   \n",
       " 98794  0.102775   0.245602  0.085629    0.582259  0.182210  ...    0.723881   \n",
       " 98795  0.124234   0.267467  0.099900    0.562877  0.229026  ...    0.724880   \n",
       " 98796  0.121007   0.242643  0.114172    0.567385  0.181461  ...    0.721067   \n",
       " 98797  0.135689   0.253658  0.121307    0.526188  0.210300  ...    0.721249   \n",
       " 98798  0.123104   0.223738  0.092764    0.494726  0.215356  ...    0.702460   \n",
       " \n",
       "        OREB_PCT      PACE       PIE      POSS       PTS   REB_PCT  TM_TOV_PCT  \\\n",
       " 0      0.050330  0.014105  0.542849  0.347285  0.421875  0.228959    0.072430   \n",
       " 1      0.064214  0.013059  0.548627  0.383484  0.437500  0.231373    0.072430   \n",
       " 2      0.083652  0.013363  0.554863  0.373303  0.437500  0.249472    0.090059   \n",
       " 3      0.065255  0.014036  0.548169  0.368778  0.445312  0.231071    0.070731   \n",
       " 4      0.047900  0.013775  0.547654  0.367647  0.474609  0.221418    0.070731   \n",
       " ...         ...       ...       ...       ...       ...       ...         ...   \n",
       " 98794  0.244707  0.008205  0.541991  0.417421  0.607422  0.252187    0.379142   \n",
       " 98795  0.269351  0.007788  0.550057  0.452489  0.613281  0.300754    0.354291   \n",
       " 98796  0.258591  0.008106  0.556350  0.503394  0.611328  0.259729    0.302889   \n",
       " 98797  0.198889  0.008570  0.572311  0.520362  0.609375  0.263952    0.246177   \n",
       " 98798  0.208608  0.008400  0.575343  0.532805  0.574219  0.272700    0.246177   \n",
       " \n",
       "          TS_PCT   USG_PCT  \n",
       " 0      0.431366  0.376921  \n",
       " 1      0.465736  0.384718  \n",
       " 2      0.499260  0.418802  \n",
       " 3      0.447864  0.404767  \n",
       " 4      0.422377  0.418133  \n",
       " ...         ...       ...  \n",
       " 98794  0.598879  0.206728  \n",
       " 98795  0.629547  0.220539  \n",
       " 98796  0.703363  0.230118  \n",
       " 98797  0.770199  0.255959  \n",
       " 98798  0.769564  0.258409  \n",
       " \n",
       " [79708 rows x 23 columns],\n",
       "         PLAYER_ID     TEAM_ID TEAM_ABBREVIATION  GAME_DATE        PLAYER_NAME  \\\n",
       " 456          2544  1610612747               LAL 2022-10-03       LeBron James   \n",
       " 457          2544  1610612747               LAL 2022-10-05       LeBron James   \n",
       " 458          2544  1610612747               LAL 2022-10-12       LeBron James   \n",
       " 459          2544  1610612747               LAL 2022-10-14       LeBron James   \n",
       " 460          2544  1610612747               LAL 2022-10-18       LeBron James   \n",
       " ...           ...         ...               ...        ...                ...   \n",
       " 100079    1631323  1610612762               UTA 2023-02-23  Simone Fontecchio   \n",
       " 100080    1631323  1610612762               UTA 2023-02-25  Simone Fontecchio   \n",
       " 100081    1631323  1610612762               UTA 2023-02-28  Simone Fontecchio   \n",
       " 100082    1631323  1610612762               UTA 2023-03-03  Simone Fontecchio   \n",
       " 100083    1631323  1610612762               UTA 2023-03-05  Simone Fontecchio   \n",
       " \n",
       "          AST_PCT  AST_RATIO   AST_TOV  DEF_RATING  DREB_PCT  ...  OFF_RATING  \\\n",
       " 456     0.432720   0.262206  0.259027    0.650410  0.322285  ...    0.724426   \n",
       " 457     0.448854   0.273385  0.230484    0.665915  0.315730  ...    0.722974   \n",
       " 458     0.429171   0.266974  0.223348    0.614712  0.331086  ...    0.712807   \n",
       " 459     0.400774   0.250041  0.214785    0.620391  0.339888  ...    0.699555   \n",
       " 460     0.440626   0.262371  0.226202    0.608852  0.370974  ...    0.697377   \n",
       " ...          ...        ...       ...         ...       ...  ...         ...   \n",
       " 100079  0.092611   0.167352  0.071357    0.566844  0.141199  ...    0.706635   \n",
       " 100080  0.107293   0.190860  0.071357    0.579735  0.141199  ...    0.770627   \n",
       " 100081  0.128751   0.218313  0.080919    0.630758  0.094382  ...    0.761732   \n",
       " 100082  0.139561   0.228670  0.095191    0.612729  0.045880  ...    0.773260   \n",
       " 100083  0.168925   0.265165  0.095191    0.641936  0.060300  ...    0.687574   \n",
       " \n",
       "         OREB_PCT      PACE       PIE      POSS       PTS   REB_PCT  \\\n",
       " 456     0.093023  0.008474  0.608410  0.822398  0.585938  0.308296   \n",
       " 457     0.093023  0.009722  0.603890  0.787330  0.572266  0.302262   \n",
       " 458     0.106907  0.010254  0.610526  0.776018  0.576172  0.323680   \n",
       " 459     0.115932  0.010094  0.604405  0.720588  0.542969  0.333032   \n",
       " 460     0.154460  0.010898  0.613844  0.726244  0.552734  0.371342   \n",
       " ...          ...       ...       ...       ...       ...       ...   \n",
       " 100079  0.133287  0.012827  0.522883  0.263575  0.759766  0.171342   \n",
       " 100080  0.152725  0.012945  0.538387  0.277149  0.761719  0.182202   \n",
       " 100081  0.152725  0.012803  0.522654  0.319005  0.699219  0.139065   \n",
       " 100082  0.096147  0.011583  0.523112  0.369910  0.666016  0.080543   \n",
       " 100083  0.096147  0.011432  0.523112  0.408371  0.656250  0.090498   \n",
       " \n",
       "         TM_TOV_PCT    TS_PCT   USG_PCT  \n",
       " 456       0.230459  0.639594  0.730675  \n",
       " 457       0.212829  0.638959  0.710403  \n",
       " 458       0.217502  0.658206  0.695032  \n",
       " 459       0.207519  0.623308  0.714413  \n",
       " 460       0.200935  0.639065  0.719537  \n",
       " ...            ...       ...       ...  \n",
       " 100079    0.267842  0.421003  0.387391  \n",
       " 100080    0.214741  0.455690  0.367788  \n",
       " 100081    0.267842  0.402813  0.317665  \n",
       " 100082    0.139550  0.305626  0.312542  \n",
       " 100083    0.139550  0.343380  0.350635  \n",
       " \n",
       " [20349 rows x 23 columns],\n",
       "        PLUS_MINUS\n",
       " 0           -14.9\n",
       " 1           -14.8\n",
       " 2           -13.9\n",
       " 3           -13.9\n",
       " 4           -11.5\n",
       " ...           ...\n",
       " 98794        -9.0\n",
       " 98795        -8.1\n",
       " 98796        -6.0\n",
       " 98797        -3.2\n",
       " 98798        -1.0\n",
       " \n",
       " [79708 rows x 1 columns],\n",
       "         PLUS_MINUS\n",
       " 456           -8.0\n",
       " 457           -9.7\n",
       " 458           -7.3\n",
       " 459          -10.1\n",
       " 460           -9.5\n",
       " ...            ...\n",
       " 100079         0.6\n",
       " 100080         2.3\n",
       " 100081        -0.8\n",
       " 100082        -5.3\n",
       " 100083        -5.3\n",
       " \n",
       " [20349 rows x 1 columns])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1611becf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM_ID</th>\n",
       "      <th>TEAM_ABBREVIATION</th>\n",
       "      <th>AST_PCT</th>\n",
       "      <th>AST_RATIO</th>\n",
       "      <th>AST_TOV</th>\n",
       "      <th>DEF_RATING</th>\n",
       "      <th>DREB_PCT</th>\n",
       "      <th>EFG_PCT</th>\n",
       "      <th>MIN</th>\n",
       "      <th>NET_RATING</th>\n",
       "      <th>OFF_RATING</th>\n",
       "      <th>OREB_PCT</th>\n",
       "      <th>PACE</th>\n",
       "      <th>PIE</th>\n",
       "      <th>POSS</th>\n",
       "      <th>PTS</th>\n",
       "      <th>REB_PCT</th>\n",
       "      <th>TM_TOV_PCT</th>\n",
       "      <th>TS_PCT</th>\n",
       "      <th>USG_PCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>0.432720</td>\n",
       "      <td>0.262206</td>\n",
       "      <td>0.259027</td>\n",
       "      <td>0.650410</td>\n",
       "      <td>0.322285</td>\n",
       "      <td>0.604447</td>\n",
       "      <td>0.841656</td>\n",
       "      <td>0.544617</td>\n",
       "      <td>0.724426</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.008474</td>\n",
       "      <td>0.608410</td>\n",
       "      <td>0.822398</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.308296</td>\n",
       "      <td>0.230459</td>\n",
       "      <td>0.639594</td>\n",
       "      <td>0.730675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>0.448854</td>\n",
       "      <td>0.273385</td>\n",
       "      <td>0.230484</td>\n",
       "      <td>0.665915</td>\n",
       "      <td>0.315730</td>\n",
       "      <td>0.607013</td>\n",
       "      <td>0.797485</td>\n",
       "      <td>0.533373</td>\n",
       "      <td>0.722974</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.009722</td>\n",
       "      <td>0.603890</td>\n",
       "      <td>0.787330</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>0.302262</td>\n",
       "      <td>0.212829</td>\n",
       "      <td>0.638959</td>\n",
       "      <td>0.710403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>0.429171</td>\n",
       "      <td>0.266974</td>\n",
       "      <td>0.223348</td>\n",
       "      <td>0.614712</td>\n",
       "      <td>0.331086</td>\n",
       "      <td>0.629250</td>\n",
       "      <td>0.784186</td>\n",
       "      <td>0.560646</td>\n",
       "      <td>0.712807</td>\n",
       "      <td>0.106907</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>0.776018</td>\n",
       "      <td>0.576172</td>\n",
       "      <td>0.323680</td>\n",
       "      <td>0.217502</td>\n",
       "      <td>0.658206</td>\n",
       "      <td>0.695032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>0.400774</td>\n",
       "      <td>0.250041</td>\n",
       "      <td>0.214785</td>\n",
       "      <td>0.620391</td>\n",
       "      <td>0.339888</td>\n",
       "      <td>0.589053</td>\n",
       "      <td>0.729658</td>\n",
       "      <td>0.548086</td>\n",
       "      <td>0.699555</td>\n",
       "      <td>0.115932</td>\n",
       "      <td>0.010094</td>\n",
       "      <td>0.604405</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.542969</td>\n",
       "      <td>0.333032</td>\n",
       "      <td>0.207519</td>\n",
       "      <td>0.623308</td>\n",
       "      <td>0.714413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>0.440626</td>\n",
       "      <td>0.262371</td>\n",
       "      <td>0.226202</td>\n",
       "      <td>0.608852</td>\n",
       "      <td>0.370974</td>\n",
       "      <td>0.601240</td>\n",
       "      <td>0.725144</td>\n",
       "      <td>0.554306</td>\n",
       "      <td>0.697377</td>\n",
       "      <td>0.154460</td>\n",
       "      <td>0.010898</td>\n",
       "      <td>0.613844</td>\n",
       "      <td>0.726244</td>\n",
       "      <td>0.552734</td>\n",
       "      <td>0.371342</td>\n",
       "      <td>0.200935</td>\n",
       "      <td>0.639065</td>\n",
       "      <td>0.719537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100079</th>\n",
       "      <td>1610612762</td>\n",
       "      <td>UTA</td>\n",
       "      <td>0.092611</td>\n",
       "      <td>0.167352</td>\n",
       "      <td>0.071357</td>\n",
       "      <td>0.566844</td>\n",
       "      <td>0.141199</td>\n",
       "      <td>0.415758</td>\n",
       "      <td>0.251965</td>\n",
       "      <td>0.588337</td>\n",
       "      <td>0.706635</td>\n",
       "      <td>0.133287</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>0.522883</td>\n",
       "      <td>0.263575</td>\n",
       "      <td>0.759766</td>\n",
       "      <td>0.171342</td>\n",
       "      <td>0.267842</td>\n",
       "      <td>0.421003</td>\n",
       "      <td>0.387391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100080</th>\n",
       "      <td>1610612762</td>\n",
       "      <td>UTA</td>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.190860</td>\n",
       "      <td>0.071357</td>\n",
       "      <td>0.579735</td>\n",
       "      <td>0.141199</td>\n",
       "      <td>0.429121</td>\n",
       "      <td>0.264539</td>\n",
       "      <td>0.622010</td>\n",
       "      <td>0.770627</td>\n",
       "      <td>0.152725</td>\n",
       "      <td>0.012945</td>\n",
       "      <td>0.538387</td>\n",
       "      <td>0.277149</td>\n",
       "      <td>0.761719</td>\n",
       "      <td>0.182202</td>\n",
       "      <td>0.214741</td>\n",
       "      <td>0.455690</td>\n",
       "      <td>0.367788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100081</th>\n",
       "      <td>1610612762</td>\n",
       "      <td>UTA</td>\n",
       "      <td>0.128751</td>\n",
       "      <td>0.218313</td>\n",
       "      <td>0.080919</td>\n",
       "      <td>0.630758</td>\n",
       "      <td>0.094382</td>\n",
       "      <td>0.375668</td>\n",
       "      <td>0.305364</td>\n",
       "      <td>0.582356</td>\n",
       "      <td>0.761732</td>\n",
       "      <td>0.152725</td>\n",
       "      <td>0.012803</td>\n",
       "      <td>0.522654</td>\n",
       "      <td>0.319005</td>\n",
       "      <td>0.699219</td>\n",
       "      <td>0.139065</td>\n",
       "      <td>0.267842</td>\n",
       "      <td>0.402813</td>\n",
       "      <td>0.317665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100082</th>\n",
       "      <td>1610612762</td>\n",
       "      <td>UTA</td>\n",
       "      <td>0.139561</td>\n",
       "      <td>0.228670</td>\n",
       "      <td>0.095191</td>\n",
       "      <td>0.612729</td>\n",
       "      <td>0.045880</td>\n",
       "      <td>0.277635</td>\n",
       "      <td>0.358884</td>\n",
       "      <td>0.601974</td>\n",
       "      <td>0.773260</td>\n",
       "      <td>0.096147</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.523112</td>\n",
       "      <td>0.369910</td>\n",
       "      <td>0.666016</td>\n",
       "      <td>0.080543</td>\n",
       "      <td>0.139550</td>\n",
       "      <td>0.305626</td>\n",
       "      <td>0.312542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100083</th>\n",
       "      <td>1610612762</td>\n",
       "      <td>UTA</td>\n",
       "      <td>0.168925</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>0.095191</td>\n",
       "      <td>0.641936</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.315801</td>\n",
       "      <td>0.395518</td>\n",
       "      <td>0.526136</td>\n",
       "      <td>0.687574</td>\n",
       "      <td>0.096147</td>\n",
       "      <td>0.011432</td>\n",
       "      <td>0.523112</td>\n",
       "      <td>0.408371</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.090498</td>\n",
       "      <td>0.139550</td>\n",
       "      <td>0.343380</td>\n",
       "      <td>0.350635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20349 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           TEAM_ID TEAM_ABBREVIATION   AST_PCT  AST_RATIO   AST_TOV  \\\n",
       "456     1610612747               LAL  0.432720   0.262206  0.259027   \n",
       "457     1610612747               LAL  0.448854   0.273385  0.230484   \n",
       "458     1610612747               LAL  0.429171   0.266974  0.223348   \n",
       "459     1610612747               LAL  0.400774   0.250041  0.214785   \n",
       "460     1610612747               LAL  0.440626   0.262371  0.226202   \n",
       "...            ...               ...       ...        ...       ...   \n",
       "100079  1610612762               UTA  0.092611   0.167352  0.071357   \n",
       "100080  1610612762               UTA  0.107293   0.190860  0.071357   \n",
       "100081  1610612762               UTA  0.128751   0.218313  0.080919   \n",
       "100082  1610612762               UTA  0.139561   0.228670  0.095191   \n",
       "100083  1610612762               UTA  0.168925   0.265165  0.095191   \n",
       "\n",
       "        DEF_RATING  DREB_PCT   EFG_PCT       MIN  NET_RATING  OFF_RATING  \\\n",
       "456       0.650410  0.322285  0.604447  0.841656    0.544617    0.724426   \n",
       "457       0.665915  0.315730  0.607013  0.797485    0.533373    0.722974   \n",
       "458       0.614712  0.331086  0.629250  0.784186    0.560646    0.712807   \n",
       "459       0.620391  0.339888  0.589053  0.729658    0.548086    0.699555   \n",
       "460       0.608852  0.370974  0.601240  0.725144    0.554306    0.697377   \n",
       "...            ...       ...       ...       ...         ...         ...   \n",
       "100079    0.566844  0.141199  0.415758  0.251965    0.588337    0.706635   \n",
       "100080    0.579735  0.141199  0.429121  0.264539    0.622010    0.770627   \n",
       "100081    0.630758  0.094382  0.375668  0.305364    0.582356    0.761732   \n",
       "100082    0.612729  0.045880  0.277635  0.358884    0.601974    0.773260   \n",
       "100083    0.641936  0.060300  0.315801  0.395518    0.526136    0.687574   \n",
       "\n",
       "        OREB_PCT      PACE       PIE      POSS       PTS   REB_PCT  \\\n",
       "456     0.093023  0.008474  0.608410  0.822398  0.585938  0.308296   \n",
       "457     0.093023  0.009722  0.603890  0.787330  0.572266  0.302262   \n",
       "458     0.106907  0.010254  0.610526  0.776018  0.576172  0.323680   \n",
       "459     0.115932  0.010094  0.604405  0.720588  0.542969  0.333032   \n",
       "460     0.154460  0.010898  0.613844  0.726244  0.552734  0.371342   \n",
       "...          ...       ...       ...       ...       ...       ...   \n",
       "100079  0.133287  0.012827  0.522883  0.263575  0.759766  0.171342   \n",
       "100080  0.152725  0.012945  0.538387  0.277149  0.761719  0.182202   \n",
       "100081  0.152725  0.012803  0.522654  0.319005  0.699219  0.139065   \n",
       "100082  0.096147  0.011583  0.523112  0.369910  0.666016  0.080543   \n",
       "100083  0.096147  0.011432  0.523112  0.408371  0.656250  0.090498   \n",
       "\n",
       "        TM_TOV_PCT    TS_PCT   USG_PCT  \n",
       "456       0.230459  0.639594  0.730675  \n",
       "457       0.212829  0.638959  0.710403  \n",
       "458       0.217502  0.658206  0.695032  \n",
       "459       0.207519  0.623308  0.714413  \n",
       "460       0.200935  0.639065  0.719537  \n",
       "...            ...       ...       ...  \n",
       "100079    0.267842  0.421003  0.387391  \n",
       "100080    0.214741  0.455690  0.367788  \n",
       "100081    0.267842  0.402813  0.317665  \n",
       "100082    0.139550  0.305626  0.312542  \n",
       "100083    0.139550  0.343380  0.350635  \n",
       "\n",
       "[20349 rows x 20 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_catless = X_train.loc[:, ~X_train.columns.isin(['GAME_DATE', 'PLAYER_ID', 'PLAYER_NAME'])]\n",
    "X_test_catless = X_test.loc[:, ~X_test.columns.isin(['GAME_DATE', 'PLAYER_ID', 'PLAYER_NAME'])]\n",
    "X_test_catless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1b9f0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_reg = DMatrix(X_train_dateless, y_train, enable_categorical=True)\n",
    "dtest_reg = DMatrix(X_test_dateless, y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a3efe",
   "metadata": {},
   "source": [
    "## 3.2 NGBoost with Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82a0cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=20)\n",
    "b2 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=35)\n",
    "b3 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0250e0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ngboost.scores.LogScore"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngb.scores.LogScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d59e5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'Dist' : hp.choice('Dist', [ngb.distns.Normal]),\n",
    "    'learning_rate' : hp.quniform('learning_rate', 0.01, 0.5, 0.01),\n",
    "    'n_estimators' : hp.choice('n_estimators', range(20, 200, 5)),\n",
    "    #'gamma' : hp.quniform('gamma', 0, 0.50, 0.01),\n",
    "    #'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'minibatch_frac' : hp.quniform('minibatch_frac', 0.1, 1, 0.01),\n",
    "    'Score' : hp.choice('Score', [ngb.scores.LogScore]),\n",
    "    'col_sample' : hp.quniform('col_sample', 0.1, 1.0, 0.01),\n",
    "    'Base': hp.choice('Base', [b1, b2, b3])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "427f758e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.53, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=0.35000000000000003, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.06, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=50,\n",
       "             max_leaves=None, min_child_weight=6.0, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "             num_parallel_tree=None, predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.53, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=0.35000000000000003, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.06, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=50,\n",
       "             max_leaves=None, min_child_weight=6.0, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "             num_parallel_tree=None, predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.53, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=0.35000000000000003, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.06, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=50,\n",
       "             max_leaves=None, min_child_weight=6.0, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "             num_parallel_tree=None, predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = xgb.XGBRegressor()\n",
    "base.load_model('/Users/lj/code/5pacepenguin/nba_betting_analysis/nba_betting_analysis/backend/models/hyperopted_gbdt_players_33estimators')\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17466a72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLAYER_ID</th>\n",
       "      <th>TEAM_ID</th>\n",
       "      <th>AST_PCT</th>\n",
       "      <th>AST_RATIO</th>\n",
       "      <th>AST_TOV</th>\n",
       "      <th>DEF_RATING</th>\n",
       "      <th>DREB_PCT</th>\n",
       "      <th>EFG_PCT</th>\n",
       "      <th>MIN</th>\n",
       "      <th>NET_RATING</th>\n",
       "      <th>OFF_RATING</th>\n",
       "      <th>OREB_PCT</th>\n",
       "      <th>PACE</th>\n",
       "      <th>PIE</th>\n",
       "      <th>POSS</th>\n",
       "      <th>PTS</th>\n",
       "      <th>REB_PCT</th>\n",
       "      <th>TM_TOV_PCT</th>\n",
       "      <th>TS_PCT</th>\n",
       "      <th>USG_PCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.970800e+04</td>\n",
       "      <td>7.970800e+04</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "      <td>79708.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.019341e+06</td>\n",
       "      <td>1.610613e+09</td>\n",
       "      <td>0.227147</td>\n",
       "      <td>0.270407</td>\n",
       "      <td>0.151329</td>\n",
       "      <td>0.587824</td>\n",
       "      <td>0.256084</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.522678</td>\n",
       "      <td>0.558884</td>\n",
       "      <td>0.683243</td>\n",
       "      <td>0.140036</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.572093</td>\n",
       "      <td>0.516615</td>\n",
       "      <td>0.580998</td>\n",
       "      <td>0.267454</td>\n",
       "      <td>0.213144</td>\n",
       "      <td>0.558146</td>\n",
       "      <td>0.390102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.107855e+05</td>\n",
       "      <td>8.500417e+00</td>\n",
       "      <td>0.148640</td>\n",
       "      <td>0.135253</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.076285</td>\n",
       "      <td>0.113695</td>\n",
       "      <td>0.120786</td>\n",
       "      <td>0.204998</td>\n",
       "      <td>0.070835</td>\n",
       "      <td>0.088492</td>\n",
       "      <td>0.121796</td>\n",
       "      <td>0.015361</td>\n",
       "      <td>0.033038</td>\n",
       "      <td>0.199153</td>\n",
       "      <td>0.115368</td>\n",
       "      <td>0.129764</td>\n",
       "      <td>0.096079</td>\n",
       "      <td>0.115660</td>\n",
       "      <td>0.127350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.713000e+03</td>\n",
       "      <td>1.610613e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.030980e+05</td>\n",
       "      <td>1.610613e+09</td>\n",
       "      <td>0.116328</td>\n",
       "      <td>0.171626</td>\n",
       "      <td>0.052376</td>\n",
       "      <td>0.546020</td>\n",
       "      <td>0.173408</td>\n",
       "      <td>0.465362</td>\n",
       "      <td>0.364406</td>\n",
       "      <td>0.519617</td>\n",
       "      <td>0.638831</td>\n",
       "      <td>0.053801</td>\n",
       "      <td>0.007786</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.363122</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>0.171342</td>\n",
       "      <td>0.150170</td>\n",
       "      <td>0.497039</td>\n",
       "      <td>0.298953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.627732e+06</td>\n",
       "      <td>1.610613e+09</td>\n",
       "      <td>0.186189</td>\n",
       "      <td>0.249877</td>\n",
       "      <td>0.114172</td>\n",
       "      <td>0.592085</td>\n",
       "      <td>0.234831</td>\n",
       "      <td>0.540411</td>\n",
       "      <td>0.536372</td>\n",
       "      <td>0.563397</td>\n",
       "      <td>0.692748</td>\n",
       "      <td>0.099965</td>\n",
       "      <td>0.009396</td>\n",
       "      <td>0.571739</td>\n",
       "      <td>0.530543</td>\n",
       "      <td>0.583984</td>\n",
       "      <td>0.236501</td>\n",
       "      <td>0.203696</td>\n",
       "      <td>0.570960</td>\n",
       "      <td>0.374471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.629006e+06</td>\n",
       "      <td>1.610613e+09</td>\n",
       "      <td>0.306551</td>\n",
       "      <td>0.350485</td>\n",
       "      <td>0.216498</td>\n",
       "      <td>0.635536</td>\n",
       "      <td>0.321348</td>\n",
       "      <td>0.606692</td>\n",
       "      <td>0.694152</td>\n",
       "      <td>0.604486</td>\n",
       "      <td>0.741150</td>\n",
       "      <td>0.190559</td>\n",
       "      <td>0.011195</td>\n",
       "      <td>0.588272</td>\n",
       "      <td>0.683258</td>\n",
       "      <td>0.660156</td>\n",
       "      <td>0.336953</td>\n",
       "      <td>0.263594</td>\n",
       "      <td>0.633249</td>\n",
       "      <td>0.466919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.630846e+06</td>\n",
       "      <td>1.610613e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933032</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PLAYER_ID       TEAM_ID       AST_PCT     AST_RATIO       AST_TOV  \\\n",
       "count  7.970800e+04  7.970800e+04  79708.000000  79708.000000  79708.000000   \n",
       "mean   1.019341e+06  1.610613e+09      0.227147      0.270407      0.151329   \n",
       "std    7.107855e+05  8.500417e+00      0.148640      0.135253      0.131200   \n",
       "min    1.713000e+03  1.610613e+09      0.000000      0.000000      0.000000   \n",
       "25%    2.030980e+05  1.610613e+09      0.116328      0.171626      0.052376   \n",
       "50%    1.627732e+06  1.610613e+09      0.186189      0.249877      0.114172   \n",
       "75%    1.629006e+06  1.610613e+09      0.306551      0.350485      0.216498   \n",
       "max    1.630846e+06  1.610613e+09      1.000000      1.000000      1.000000   \n",
       "\n",
       "         DEF_RATING      DREB_PCT       EFG_PCT           MIN    NET_RATING  \\\n",
       "count  79708.000000  79708.000000  79708.000000  79708.000000  79708.000000   \n",
       "mean       0.587824      0.256084      0.530130      0.522678      0.558884   \n",
       "std        0.076285      0.113695      0.120786      0.204998      0.070835   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.022787   \n",
       "25%        0.546020      0.173408      0.465362      0.364406      0.519617   \n",
       "50%        0.592085      0.234831      0.540411      0.536372      0.563397   \n",
       "75%        0.635536      0.321348      0.606692      0.694152      0.604486   \n",
       "max        1.000000      1.000000      1.000000      1.000000      0.910407   \n",
       "\n",
       "         OFF_RATING      OREB_PCT          PACE           PIE          POSS  \\\n",
       "count  79708.000000  79708.000000  79708.000000  79708.000000  79708.000000   \n",
       "mean       0.683243      0.140036      0.010450      0.572093      0.516615   \n",
       "std        0.088492      0.121796      0.015361      0.033038      0.199153   \n",
       "min        0.000000      0.000000      0.000302      0.000000      0.000000   \n",
       "25%        0.638831      0.053801      0.007786      0.556350      0.363122   \n",
       "50%        0.692748      0.099965      0.009396      0.571739      0.530543   \n",
       "75%        0.741150      0.190559      0.011195      0.588272      0.683258   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                PTS       REB_PCT    TM_TOV_PCT        TS_PCT       USG_PCT  \n",
       "count  79708.000000  79708.000000  79708.000000  79708.000000  79708.000000  \n",
       "mean       0.580998      0.267454      0.213144      0.558146      0.390102  \n",
       "std        0.115368      0.129764      0.096079      0.115660      0.127350  \n",
       "min        0.011719      0.000000      0.000000      0.000000      0.000000  \n",
       "25%        0.503906      0.171342      0.150170      0.497039      0.298953  \n",
       "50%        0.583984      0.236501      0.203696      0.570960      0.374471  \n",
       "75%        0.660156      0.336953      0.263594      0.633249      0.466919  \n",
       "max        1.000000      0.933032      1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f7c9377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=3.3075 val_loss=3.1890 scale=2.0000 norm=10.6727                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL0 (val_loss=3.1890)                                      \n",
      "CrossValMSE:                                                                    \n",
      "53.66968558260976                                                               \n",
      "[iter 0] loss=3.3029 val_loss=3.1887 scale=1.0000 norm=5.3234                   \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL1 (val_loss=3.0908)                                      \n",
      "CrossValMSE:                                                                    \n",
      "37.843363441819896                                                              \n",
      "[iter 0] loss=3.3084 val_loss=3.2001 scale=2.0000 norm=10.6653                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL18 (val_loss=2.7355)                                     \n",
      "CrossValMSE:                                                                    \n",
      "19.36893684019131                                                               \n",
      "[iter 0] loss=3.3118 val_loss=3.7721 scale=2.0000 norm=10.7531                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL0 (val_loss=3.7721)                                      \n",
      "CrossValMSE:                                                                    \n",
      "84.215864460164                                                                 \n",
      "  8%|▉          | 4/50 [00:53<10:12, 13.32s/trial, best loss: 19.36893684019131]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:71: RuntimeWarning: overflow encountered in square\n",
      "  self.var = self.scale**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=3.3070 val_loss=3.3044 scale=2.0000 norm=10.6830                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL0 (val_loss=3.3044)                                      \n",
      "CrossValMSE:                                                                    \n",
      "40.50987366039145                                                               \n",
      "[iter 0] loss=3.3090 val_loss=3.2289 scale=2.0000 norm=10.6708                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL25 (val_loss=2.7560)                                     \n",
      "CrossValMSE:                                                                    \n",
      "20.310413561124044                                                              \n",
      "[iter 0] loss=3.3087 val_loss=3.1056 scale=2.0000 norm=10.7019                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL3 (val_loss=2.9022)                                      \n",
      "CrossValMSE:                                                                    \n",
      "24.370587788552193                                                              \n",
      "[iter 0] loss=3.3064 val_loss=3.6625 scale=2.0000 norm=10.6904                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL0 (val_loss=3.6625)                                      \n",
      "CrossValMSE:                                                                    \n",
      "122.74808798239296                                                              \n",
      " 16%|█▊         | 8/50 [01:59<09:56, 14.20s/trial, best loss: 19.36893684019131]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:71: RuntimeWarning: overflow encountered in square\n",
      "  self.var = self.scale**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=3.3081 val_loss=3.1146 scale=2.0000 norm=10.6853                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL5 (val_loss=2.8722)                                      \n",
      "CrossValMSE:                                                                    \n",
      "23.264980837583284                                                              \n",
      "[iter 0] loss=3.3065 val_loss=3.1527 scale=1.0000 norm=5.3395                   \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL3 (val_loss=2.9591)                                      \n",
      "CrossValMSE:                                                                    \n",
      "22.626757774963753                                                              \n",
      "[iter 0] loss=3.3073 val_loss=3.2870 scale=1.0000 norm=5.3385                   \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL34 (val_loss=2.9151)                                     \n",
      " 20%|██        | 10/50 [02:48<10:10, 15.27s/trial, best loss: 19.36893684019131][iter 0] loss=3.3173 val_loss=3.3246 scale=2.0000 norm=10.7679\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3246)\n",
      "[iter 0] loss=3.3219 val_loss=3.1949 scale=2.0000 norm=10.8255\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1949)\n",
      "[iter 0] loss=3.3171 val_loss=3.6316 scale=2.0000 norm=10.7842\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.6316)\n",
      "[iter 0] loss=3.2659 val_loss=3.2125 scale=2.0000 norm=10.2792\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL26 (val_loss=2.6992)\n",
      "[iter 0] loss=3.3128 val_loss=4.0641 scale=2.0000 norm=10.7374\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=4.0641)\n",
      "[iter 0] loss=3.3093 val_loss=3.2942 scale=1.0000 norm=5.3518\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL31 (val_loss=2.9357)\n",
      "CrossValMSE:                                                                    \n",
      "26.63939859144578                                                               \n",
      "[iter 0] loss=3.3068 val_loss=3.1927 scale=2.0000 norm=10.6770                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL2 (val_loss=3.0766)                                      \n",
      "CrossValMSE:                                                                    \n",
      "23.98709875640011                                                               \n",
      "[iter 0] loss=3.3071 val_loss=3.2437 scale=2.0000 norm=10.6744                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL2 (val_loss=3.1926)                                      \n",
      "CrossValMSE:                                                                    \n",
      "39.147330445245835                                                              \n",
      "[iter 0] loss=3.3067 val_loss=3.2119 scale=2.0000 norm=10.6756                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL0 (val_loss=3.2119)                                      \n",
      " 26%|██▌       | 13/50 [03:41<10:16, 16.67s/trial, best loss: 19.36893684019131]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:71: RuntimeWarning: overflow encountered in square\n",
      "  self.var = self.scale**2\n",
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:71: RuntimeWarning: overflow encountered in square\n",
      "  self.var = self.scale**2\n",
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:71: RuntimeWarning: overflow encountered in square\n",
      "  self.var = self.scale**2\n",
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:70: RuntimeWarning: overflow encountered in exp\n",
      "  self.scale = np.exp(params[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValMSE:                                                                    \n",
      "41.03032850926154                                                               \n",
      " 28%|██▊       | 14/50 [03:50<10:43, 17.86s/trial, best loss: 19.36893684019131]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:71: RuntimeWarning: overflow encountered in square\n",
      "  self.var = self.scale**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=3.3147 val_loss=3.3122 scale=1.0000 norm=5.3880                   \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL1 (val_loss=3.2731)                                      \n",
      "CrossValMSE:                                                                    \n",
      "56.96572139505258                                                               \n",
      "[iter 0] loss=3.3066 val_loss=3.0974 scale=2.0000 norm=10.6592                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL3 (val_loss=2.8765)                                      \n",
      "CrossValMSE:                                                                    \n",
      "22.711816782008995                                                              \n",
      "[iter 0] loss=3.3056 val_loss=3.1133 scale=2.0000 norm=10.6595                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL1 (val_loss=3.0406)                                      \n",
      " 32%|███▏      | 16/50 [04:30<08:42, 15.36s/trial, best loss: 19.36893684019131]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:71: RuntimeWarning: overflow encountered in square\n",
      "  self.var = self.scale**2\n",
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:70: RuntimeWarning: overflow encountered in exp\n",
      "  self.scale = np.exp(params[1])\n",
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:71: RuntimeWarning: overflow encountered in square\n",
      "  self.var = self.scale**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValMSE:                                                                    \n",
      "26.803313586717444                                                              \n",
      "[iter 0] loss=3.3104 val_loss=3.2521 scale=1.0000 norm=5.3601                   \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL11 (val_loss=2.7665)                                     \n",
      "CrossValMSE:                                                                    \n",
      "25.47589097531737                                                               \n",
      "[iter 0] loss=3.3090 val_loss=3.3457 scale=2.0000 norm=10.7094                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL0 (val_loss=3.3457)                                      \n",
      " 36%|███▌      | 18/50 [05:14<11:28, 21.51s/trial, best loss: 19.36893684019131]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:71: RuntimeWarning: overflow encountered in square\n",
      "  self.var = self.scale**2\n",
      "\n",
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:71: RuntimeWarning: overflow encountered in square\n",
      "  self.var = self.scale**2\n",
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:70: RuntimeWarning: overflow encountered in exp\n",
      "  self.scale = np.exp(params[1])\n",
      "/opt/homebrew/lib/python3.10/site-packages/ngboost/distns/normal.py:71: RuntimeWarning: overflow encountered in square\n",
      "  self.var = self.scale**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValMSE:                                                                    \n",
      "83.2870126922503                                                                \n",
      "[iter 0] loss=3.3073 val_loss=3.1983 scale=1.0000 norm=5.3391                   \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL1 (val_loss=3.1304)                                      \n",
      "CrossValMSE:                                                                    \n",
      "26.127528817919245                                                              \n",
      "[iter 0] loss=3.2988 val_loss=3.2737 scale=2.0000 norm=10.6122                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL78 (val_loss=2.6259)                                     \n",
      "CrossValMSE:                                                                    \n",
      "17.361750726208005                                                              \n",
      "[iter 0] loss=3.3194 val_loss=3.2896 scale=2.0000 norm=10.8077                  \n",
      "[iter 100] loss=2.5222 val_loss=2.6884 scale=2.0000 norm=4.7407                 \n",
      " 42%|███▊     | 21/50 [06:40<11:25, 23.64s/trial, best loss: 17.361750726208005][iter 0] loss=3.2663 val_loss=3.0758 scale=2.0000 norm=10.2968\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0711)\n",
      "[iter 0] loss=3.3068 val_loss=3.2004 scale=1.0000 norm=5.3367\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2004)\n",
      "[iter 0] loss=3.3264 val_loss=3.3408 scale=2.0000 norm=10.8655\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3408)\n",
      "[iter 0] loss=3.3093 val_loss=3.1698 scale=2.0000 norm=10.6708\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1698)\n",
      "[iter 0] loss=3.3125 val_loss=3.2182 scale=2.0000 norm=10.7247\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL18 (val_loss=2.7212)\n",
      "[iter 0] loss=3.3294 val_loss=3.6291 scale=2.0000 norm=10.8957\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.6291)\n",
      "[iter 0] loss=3.2717 val_loss=3.0947 scale=2.0000 norm=10.3565\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.0947)\n",
      "[iter 0] loss=3.3250 val_loss=3.2682 scale=2.0000 norm=10.8512\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL27 (val_loss=2.7506)\n",
      "[iter 0] loss=3.3190 val_loss=3.1738 scale=1.0000 norm=5.3917\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8961)\n",
      "[iter 0] loss=3.3218 val_loss=3.3085 scale=1.0000 norm=5.4151\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL31 (val_loss=2.9497)\n",
      "[iter 0] loss=3.3178 val_loss=3.3051 scale=2.0000 norm=10.7874\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=3.2074)\n",
      "[iter 0] loss=3.3178 val_loss=3.3611 scale=1.0000 norm=5.3884\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3611)\n",
      "[iter 0] loss=3.3110 val_loss=3.1137 scale=2.0000 norm=10.7154\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0328)\n",
      "[iter 0] loss=3.3229 val_loss=3.5459 scale=2.0000 norm=10.8251\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.5459)\n",
      "[iter 0] loss=3.3054 val_loss=3.1620 scale=1.0000 norm=5.3257\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL2 (val_loss=3.0467)\n",
      "[iter 0] loss=3.3155 val_loss=3.2975 scale=2.0000 norm=10.7697\n",
      "[iter 100] loss=2.5459 val_loss=2.7109 scale=2.0000 norm=4.8581\n",
      "[iter 0] loss=3.3259 val_loss=3.1499 scale=2.0000 norm=10.8556\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1499)\n",
      "[iter 0] loss=3.3145 val_loss=3.2027 scale=2.0000 norm=10.7382\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=3.0401)\n",
      "[iter 0] loss=3.3082 val_loss=3.3128 scale=2.0000 norm=10.6877\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3128)\n",
      "[iter 0] loss=3.2709 val_loss=3.1699 scale=2.0000 norm=10.3515\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL18 (val_loss=2.6873)\n",
      "[iter 0] loss=3.3187 val_loss=3.2587 scale=2.0000 norm=10.7800\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL27 (val_loss=2.7453)\n",
      "[iter 0] loss=3.3133 val_loss=3.0991 scale=2.0000 norm=10.7144\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.9192)\n",
      "[iter 0] loss=3.3034 val_loss=3.7455 scale=2.0000 norm=10.6617\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.7455)\n",
      "[iter 0] loss=3.3131 val_loss=3.1350 scale=2.0000 norm=10.7299\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL5 (val_loss=2.8902)\n",
      "[iter 0] loss=3.3051 val_loss=3.2971 scale=1.0000 norm=5.3254\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL31 (val_loss=2.9407)\n",
      "[iter 0] loss=3.3113 val_loss=3.2893 scale=2.0000 norm=10.7166\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=3.1774)\n",
      "[iter 0] loss=3.3250 val_loss=3.3568 scale=1.0000 norm=5.4143\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3568)\n",
      "[iter 0] loss=3.3234 val_loss=3.0982 scale=2.0000 norm=10.8440\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8847)\n",
      "[iter 0] loss=3.3242 val_loss=3.2649 scale=1.0000 norm=5.4360\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL10 (val_loss=2.8974)\n",
      "[iter 0] loss=3.3067 val_loss=3.2764 scale=2.0000 norm=10.6533\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL80 (val_loss=2.6194)\n",
      "[iter 0] loss=3.3050 val_loss=3.2850 scale=2.0000 norm=10.6965\n",
      "[iter 100] loss=2.5262 val_loss=2.6879 scale=2.0000 norm=4.7427\n",
      "CrossValMSE:                                                                    \n",
      "16.46182351650563                                                               \n",
      "[iter 0] loss=3.2941 val_loss=3.2753 scale=2.0000 norm=10.5514                  \n",
      "[iter 100] loss=2.5782 val_loss=2.6914 scale=2.0000 norm=5.1128                 \n",
      " 44%|████▍     | 22/50 [07:52<18:45, 40.21s/trial, best loss: 16.46182351650563][iter 0] loss=3.3154 val_loss=3.1984 scale=2.0000 norm=10.7613\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1984)\n",
      "[iter 0] loss=3.3088 val_loss=3.2460 scale=1.0000 norm=5.3406\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2460)\n",
      "[iter 0] loss=3.3067 val_loss=3.0820 scale=2.0000 norm=10.6681\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8510)\n",
      "[iter 0] loss=3.3079 val_loss=3.1075 scale=2.0000 norm=10.6685\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0242)\n",
      "[iter 0] loss=3.3039 val_loss=3.5019 scale=2.0000 norm=10.6179\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.5019)\n",
      "[iter 0] loss=3.3100 val_loss=3.1729 scale=1.0000 norm=5.3572\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.9755)\n",
      "[iter 0] loss=3.2614 val_loss=3.2338 scale=2.0000 norm=10.2717\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL84 (val_loss=2.5717)\n",
      "[iter 0] loss=3.2657 val_loss=3.2474 scale=2.0000 norm=10.2622\n",
      "[iter 100] loss=2.4718 val_loss=2.6315 scale=2.0000 norm=4.4844\n",
      "[iter 0] loss=3.3119 val_loss=3.3002 scale=2.0000 norm=10.6811\n",
      "[iter 100] loss=2.5716 val_loss=2.7111 scale=2.0000 norm=5.0320\n",
      "[iter 0] loss=3.3303 val_loss=3.1506 scale=2.0000 norm=10.9103\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1506)\n",
      "[iter 0] loss=3.3282 val_loss=3.2179 scale=2.0000 norm=10.8882\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=3.0666)\n",
      "[iter 0] loss=3.3211 val_loss=3.2342 scale=1.0000 norm=5.4172\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2342)\n",
      "[iter 0] loss=3.3076 val_loss=3.1497 scale=2.0000 norm=10.6729\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1497)\n",
      "[iter 0] loss=3.3285 val_loss=3.1200 scale=2.0000 norm=10.8805\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.9288)\n",
      "[iter 0] loss=3.3181 val_loss=3.3008 scale=1.0000 norm=5.3997\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL31 (val_loss=2.9614)\n",
      "[iter 0] loss=3.3117 val_loss=3.1379 scale=2.0000 norm=10.7180\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.9739)\n",
      "[iter 0] loss=3.2702 val_loss=3.2411 scale=2.0000 norm=10.3394\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=3.1298)\n",
      "[iter 0] loss=3.2728 val_loss=3.1976 scale=1.0000 norm=5.1847\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1976)\n",
      "[iter 0] loss=3.3182 val_loss=3.2763 scale=2.0000 norm=10.7680\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL80 (val_loss=2.6390)\n",
      "[iter 0] loss=3.2987 val_loss=3.2819 scale=2.0000 norm=10.5970\n",
      "[iter 100] loss=2.5571 val_loss=2.6883 scale=2.0000 norm=4.9355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=3.3136 val_loss=3.1314 scale=2.0000 norm=10.7450\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1314)\n",
      "[iter 0] loss=3.3132 val_loss=3.2253 scale=1.0000 norm=5.3780\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2253)\n",
      "[iter 0] loss=3.3118 val_loss=3.3003 scale=2.0000 norm=10.7128\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3003)\n",
      "[iter 0] loss=3.3109 val_loss=3.1892 scale=2.0000 norm=10.7125\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1892)\n",
      "[iter 0] loss=3.3192 val_loss=3.1774 scale=2.0000 norm=10.7938\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1774)\n",
      "[iter 0] loss=3.3148 val_loss=3.2505 scale=2.0000 norm=10.7503\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL27 (val_loss=2.7322)\n",
      "[iter 0] loss=3.3187 val_loss=3.1484 scale=2.0000 norm=10.7950\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL5 (val_loss=2.9413)\n",
      "[iter 0] loss=3.3240 val_loss=3.1789 scale=1.0000 norm=5.4181\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8917)\n",
      "[iter 0] loss=3.3189 val_loss=3.1475 scale=2.0000 norm=10.7949\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL2 (val_loss=3.0067)\n",
      "[iter 0] loss=3.3063 val_loss=3.2900 scale=2.0000 norm=10.6624\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=3.1649)\n",
      "[iter 0] loss=3.3102 val_loss=3.2563 scale=1.0000 norm=5.3591\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL9 (val_loss=2.8797)\n",
      "[iter 0] loss=3.3221 val_loss=3.1876 scale=1.0000 norm=5.4181\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL2 (val_loss=3.0508)\n",
      "[iter 0] loss=3.3261 val_loss=3.3103 scale=2.0000 norm=10.8619\n",
      "[iter 100] loss=2.5862 val_loss=2.7325 scale=2.0000 norm=5.1294\n",
      "CrossValMSE:                                                                    \n",
      "16.50296641688854                                                               \n",
      " 46%|████▌     | 23/50 [08:30<20:51, 46.35s/trial, best loss: 16.46182351650563][iter 0] loss=3.3084 val_loss=3.1264 scale=2.0000 norm=10.6792\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1264)\n",
      "[iter 0] loss=3.3075 val_loss=3.2114 scale=1.0000 norm=5.3401\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2114)\n",
      "[iter 0] loss=3.3214 val_loss=3.2223 scale=2.0000 norm=10.8039\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL18 (val_loss=2.7400)\n",
      "[iter 0] loss=3.3161 val_loss=3.1758 scale=2.0000 norm=10.7589\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1758)\n",
      "[iter 0] loss=3.3194 val_loss=3.1063 scale=2.0000 norm=10.7903\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.9190)\n",
      "[iter 0] loss=3.3238 val_loss=3.1521 scale=2.0000 norm=10.8314\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL5 (val_loss=2.9215)\n",
      "[iter 0] loss=3.3085 val_loss=3.1474 scale=1.0000 norm=5.3431\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8368)\n",
      "[iter 0] loss=3.3083 val_loss=3.1349 scale=2.0000 norm=10.6697\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.9794)\n",
      "[iter 0] loss=3.3231 val_loss=3.3069 scale=2.0000 norm=10.8392\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=3.1993)\n",
      "[iter 0] loss=3.2717 val_loss=3.1544 scale=2.0000 norm=10.3394\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1544)\n",
      "[iter 0] loss=3.3287 val_loss=3.3709 scale=1.0000 norm=5.4363\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3709)\n",
      "[iter 0] loss=3.2713 val_loss=3.0496 scale=2.0000 norm=10.3524\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8250)\n",
      "[iter 0] loss=3.2708 val_loss=3.0609 scale=2.0000 norm=10.3372\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=2.9730)\n",
      "[iter 0] loss=3.3152 val_loss=3.5451 scale=2.0000 norm=10.7474\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.5451)\n",
      "[iter 0] loss=3.2657 val_loss=3.2442 scale=2.0000 norm=10.2651\n",
      "[iter 100] loss=2.5117 val_loss=2.6480 scale=2.0000 norm=4.6928\n",
      "[iter 0] loss=3.3189 val_loss=3.1810 scale=2.0000 norm=10.8567                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL10 (val_loss=2.8803)                                     \n",
      " 46%|████▌     | 23/50 [08:34<20:51, 46.35s/trial, best loss: 16.46182351650563][iter 0] loss=3.2685 val_loss=3.1620 scale=1.0000 norm=5.1591\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1620)\n",
      "[iter 0] loss=3.3196 val_loss=3.6602 scale=2.0000 norm=10.7867\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.6602)\n",
      "[iter 0] loss=3.3082 val_loss=3.1652 scale=2.0000 norm=10.6976\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1652)\n",
      "[iter 0] loss=3.3196 val_loss=4.0765 scale=2.0000 norm=10.8284\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=4.0765)\n",
      "[iter 0] loss=3.3105 val_loss=3.1259 scale=2.0000 norm=10.6831\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL5 (val_loss=2.8811)\n",
      "[iter 0] loss=3.2701 val_loss=3.1103 scale=1.0000 norm=5.1679\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8128)\n",
      "[iter 0] loss=3.3290 val_loss=3.2208 scale=2.0000 norm=10.9027\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2208)\n",
      "[iter 0] loss=3.3108 val_loss=3.0996 scale=2.0000 norm=10.7118\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8815)\n",
      "[iter 0] loss=3.3234 val_loss=3.1184 scale=2.0000 norm=10.8466\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0300)\n",
      "[iter 0] loss=3.3071 val_loss=3.2540 scale=1.0000 norm=5.3271\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL10 (val_loss=2.8579)\n",
      "[iter 0] loss=3.3122 val_loss=3.5435 scale=2.0000 norm=10.7141\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.5435)\n",
      "[iter 0] loss=3.2690 val_loss=3.1146 scale=1.0000 norm=5.1610\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL2 (val_loss=3.0152)\n",
      "[iter 0] loss=3.3069 val_loss=3.2987 scale=2.0000 norm=10.6636\n",
      "[iter 100] loss=2.5421 val_loss=2.7110 scale=2.0000 norm=4.8650\n",
      "[iter 0] loss=3.3224 val_loss=3.1836 scale=2.0000 norm=10.8680\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL9 (val_loss=2.8885)\n",
      "CrossValMSE:                                                                    \n",
      "22.472568607809993                                                              \n",
      "[iter 0] loss=3.3062 val_loss=3.2976 scale=2.0000 norm=10.6296                  \n",
      "[iter 100] loss=2.4244 val_loss=2.6842 scale=2.0000 norm=4.1213                 \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL136 (val_loss=2.6260)                                    \n",
      " 48%|████▊     | 24/50 [09:35<15:01, 34.66s/trial, best loss: 16.46182351650563][iter 0] loss=3.2974 val_loss=3.1776 scale=2.0000 norm=10.6477\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL11 (val_loss=2.8766)\n",
      "[iter 0] loss=3.2743 val_loss=3.2498 scale=2.0000 norm=10.3546\n",
      "[iter 100] loss=2.3867 val_loss=2.6297 scale=2.0000 norm=3.9714\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL145 (val_loss=2.5537)\n",
      "CrossValMSE:                                                                    \n",
      "17.239945466826505                                                              \n",
      " 50%|█████     | 25/50 [10:23<23:17, 55.90s/trial, best loss: 16.46182351650563][iter 0] loss=3.2658 val_loss=3.1480 scale=2.0000 norm=10.2845\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.9623)\n",
      "[iter 0] loss=3.3055 val_loss=3.0986 scale=2.0000 norm=10.6535\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8924)\n",
      "[iter 0] loss=3.3060 val_loss=4.0233 scale=2.0000 norm=10.6854\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=4.0233)\n",
      "[iter 0] loss=3.3096 val_loss=3.1882 scale=2.0000 norm=10.6891\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1882)\n",
      "[iter 0] loss=3.3169 val_loss=3.2827 scale=2.0000 norm=10.7382\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL78 (val_loss=2.6595)\n",
      "[iter 0] loss=3.2946 val_loss=3.2766 scale=2.0000 norm=10.5137\n",
      "[iter 100] loss=2.5120 val_loss=2.6730 scale=2.0000 norm=4.6465\n",
      "[iter 0] loss=3.3165 val_loss=3.1967 scale=2.0000 norm=10.7940\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL11 (val_loss=2.9031)\n",
      "[iter 0] loss=3.3123 val_loss=3.2872 scale=2.0000 norm=10.7222\n",
      "[iter 100] loss=2.4289 val_loss=2.6590 scale=2.0000 norm=4.1656\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL146 (val_loss=2.5821)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=3.2985 val_loss=3.2064 scale=2.0000 norm=10.5999                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL15 (val_loss=2.8479)                                     \n",
      "CrossValMSE:                                                                    \n",
      "21.92862309027281                                                               \n",
      "[iter 0] loss=3.2978 val_loss=3.2216 scale=2.0000 norm=10.5690                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL4 (val_loss=3.0293)                                      \n",
      "CrossValMSE:                                                                    \n",
      "25.812626203815192                                                              \n",
      "[iter 0] loss=3.3068 val_loss=3.1161 scale=2.0000 norm=10.6689                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL1 (val_loss=3.0764)                                      \n",
      "CrossValMSE:                                                                    \n",
      "34.64928957609211                                                               \n",
      "[iter 0] loss=3.3172 val_loss=3.1599 scale=2.0000 norm=10.7498                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL6 (val_loss=2.9621)                                      \n",
      "CrossValMSE:                                                                    \n",
      "24.231299628073906                                                              \n",
      "[iter 0] loss=3.3084 val_loss=3.2510 scale=2.0000 norm=10.6856                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL24 (val_loss=2.7635)                                     \n",
      "CrossValMSE:                                                                    \n",
      "21.09442976012776                                                               \n",
      "[iter 0] loss=3.3086 val_loss=3.2069 scale=2.0000 norm=10.7076                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL0 (val_loss=3.2069)                                      \n",
      "CrossValMSE:                                                                    \n",
      "61.94455289583466                                                               \n",
      "[iter 0] loss=3.3046 val_loss=3.1219 scale=2.0000 norm=10.6598                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL1 (val_loss=3.0754)                                      \n",
      "CrossValMSE:                                                                    \n",
      "36.166115293363596                                                              \n",
      "[iter 0] loss=3.3078 val_loss=3.2991 scale=2.0000 norm=10.6599                  \n",
      " 64%|██████▍   | 32/50 [11:30<03:51, 12.87s/trial, best loss: 16.46182351650563][iter 0] loss=3.3062 val_loss=3.1917 scale=2.0000 norm=10.6448\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=3.0040)\n",
      "[iter 0] loss=3.2741 val_loss=3.2579 scale=2.0000 norm=10.3824\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2579)\n",
      "[iter 0] loss=3.2719 val_loss=3.1112 scale=2.0000 norm=10.3477\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1112)\n",
      "[iter 0] loss=3.3114 val_loss=3.2085 scale=2.0000 norm=10.7104\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL18 (val_loss=2.7084)\n",
      "[iter 0] loss=3.3069 val_loss=3.6434 scale=2.0000 norm=10.6759\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.6434)\n",
      "[iter 0] loss=3.3025 val_loss=3.2473 scale=2.0000 norm=10.6160\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL26 (val_loss=2.7301)\n",
      "[iter 0] loss=3.2658 val_loss=3.0543 scale=2.0000 norm=10.2934\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8494)\n",
      "[iter 0] loss=3.2700 val_loss=3.6857 scale=2.0000 norm=10.3493\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.6857)\n",
      "[iter 0] loss=3.2708 val_loss=3.0813 scale=2.0000 norm=10.3381\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL5 (val_loss=2.8224)\n",
      "[iter 0] loss=3.2703 val_loss=3.2585 scale=1.0000 norm=5.1669\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL31 (val_loss=2.9014)\n",
      "[iter 0] loss=3.3254 val_loss=3.1437 scale=2.0000 norm=10.8638\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.9867)\n",
      "[iter 0] loss=3.3214 val_loss=3.2102 scale=2.0000 norm=10.8204\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2102)\n",
      "[iter 0] loss=3.3170 val_loss=3.1222 scale=2.0000 norm=10.7802\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0474)\n",
      "[iter 0] loss=3.2685 val_loss=3.2134 scale=1.0000 norm=5.1560\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL11 (val_loss=2.7913)\n",
      "[iter 0] loss=3.2675 val_loss=3.4661 scale=2.0000 norm=10.3112\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.4661)\n",
      "[iter 0] loss=3.3269 val_loss=3.2912 scale=2.0000 norm=10.8456\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL79 (val_loss=2.6548)\n",
      "[iter 0] loss=3.3229 val_loss=3.2953 scale=2.0000 norm=10.8473\n",
      "[iter 100] loss=2.5866 val_loss=2.7264 scale=2.0000 norm=5.1532\n",
      "[iter 0] loss=3.3278 val_loss=3.1994 scale=2.0000 norm=10.9164\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL10 (val_loss=2.9208)\n",
      "[iter 0] loss=3.3264 val_loss=3.2958 scale=2.0000 norm=10.8889\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL15 (val_loss=2.8799)\n",
      "[iter 0] loss=3.3184 val_loss=3.2408 scale=2.0000 norm=10.7918\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2408)\n",
      "[iter 0] loss=3.3112 val_loss=3.2984 scale=2.0000 norm=10.7076\n",
      "CrossValMSE:                                                                    \n",
      "25.62640663800847                                                               \n",
      "[iter 0] loss=3.3092 val_loss=3.2004 scale=2.0000 norm=10.6983                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL11 (val_loss=2.9594)                                     \n",
      "CrossValMSE:                                                                    \n",
      "24.9134368494209                                                                \n",
      "[iter 0] loss=3.3167 val_loss=3.1151 scale=2.0000 norm=10.7767                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL3 (val_loss=2.9625)                                      \n",
      "CrossValMSE:                                                                    \n",
      "27.084910454850313                                                              \n",
      "[iter 0] loss=3.3045 val_loss=3.2903 scale=2.0000 norm=10.6484                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL0 (val_loss=3.2903)                                      \n",
      "CrossValMSE:                                                                    \n",
      "47.99726066986251                                                               \n",
      "[iter 0] loss=3.3054 val_loss=3.2277 scale=2.0000 norm=10.6579                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL19 (val_loss=2.8148)                                     \n",
      "CrossValMSE:                                                                    \n",
      "20.445456513993776                                                              \n",
      "[iter 0] loss=3.3049 val_loss=3.3090 scale=2.0000 norm=10.6629                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL2 (val_loss=3.2995)                                      \n",
      "CrossValMSE:                                                                    \n",
      "46.45450870549664                                                               \n",
      "[iter 0] loss=3.3058 val_loss=3.1949 scale=2.0000 norm=10.6492                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL0 (val_loss=3.1949)                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValMSE:                                                                    \n",
      "52.09122801135529                                                               \n",
      "[iter 0] loss=3.3043 val_loss=3.1139 scale=2.0000 norm=10.6390                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL1 (val_loss=3.0662)                                      \n",
      "CrossValMSE:                                                                    \n",
      "37.42851879992906                                                               \n",
      "[iter 0] loss=3.3012 val_loss=3.1371 scale=2.0000 norm=10.6230                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL3 (val_loss=3.0512)                                      \n",
      "CrossValMSE:                                                                    \n",
      "25.50081726057204                                                               \n",
      "[iter 0] loss=3.3078 val_loss=3.1064 scale=2.0000 norm=10.6923                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL0 (val_loss=3.1064)                                      \n",
      "CrossValMSE:                                                                    \n",
      "39.815427464540946                                                              \n",
      "[iter 0] loss=3.3012 val_loss=3.2653 scale=2.0000 norm=10.6210                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL31 (val_loss=2.8763)                                     \n",
      "CrossValMSE:                                                                    \n",
      "22.069159783114884                                                              \n",
      "[iter 0] loss=3.3085 val_loss=3.1644 scale=2.0000 norm=10.6972                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL11 (val_loss=2.7287)                                     \n",
      "CrossValMSE:                                                                    \n",
      "20.224571840418115                                                              \n",
      "[iter 0] loss=3.3055 val_loss=3.1350 scale=2.0000 norm=10.6609                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL7 (val_loss=2.8465)                                      \n",
      "CrossValMSE:                                                                    \n",
      "21.47756382580981                                                               \n",
      "[iter 0] loss=3.3098 val_loss=3.2323 scale=2.0000 norm=10.7039                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL25 (val_loss=2.7303)                                     \n",
      " 90%|█████████ | 45/50 [15:09<01:40, 20.15s/trial, best loss: 16.46182351650563][iter 0] loss=3.2643 val_loss=3.2405 scale=2.0000 norm=10.2917\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL15 (val_loss=2.7753)\n",
      "[iter 0] loss=3.2695 val_loss=3.1111 scale=2.0000 norm=10.3498\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL5 (val_loss=2.9102)\n",
      "[iter 0] loss=3.3015 val_loss=3.1016 scale=2.0000 norm=10.6447\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0592)\n",
      "[iter 0] loss=3.2705 val_loss=3.1139 scale=2.0000 norm=10.3612\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL6 (val_loss=2.8793)\n",
      "[iter 0] loss=3.3236 val_loss=3.2754 scale=2.0000 norm=10.8508\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2754)\n",
      "[iter 0] loss=3.2653 val_loss=3.0783 scale=2.0000 norm=10.2851\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0218)\n",
      "[iter 0] loss=3.2774 val_loss=3.2440 scale=2.0000 norm=10.4064\n",
      "[iter 0] loss=3.2685 val_loss=3.1788 scale=2.0000 norm=10.3283\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL10 (val_loss=2.8966)\n",
      "[iter 0] loss=3.3136 val_loss=3.3454 scale=2.0000 norm=10.7370\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3454)\n",
      "[iter 0] loss=3.2690 val_loss=3.2192 scale=2.0000 norm=10.3102\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL33 (val_loss=2.7839)\n",
      "[iter 0] loss=3.3243 val_loss=3.2696 scale=1.0000 norm=5.4343\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL12 (val_loss=2.8063)\n",
      "[iter 0] loss=3.3175 val_loss=3.2300 scale=2.0000 norm=10.7715\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL25 (val_loss=2.7377)\n",
      "CrossValMSE:                                                                    \n",
      "19.845055794469438                                                              \n",
      "[iter 0] loss=3.3020 val_loss=3.2719 scale=2.0000 norm=10.6503                  \n",
      "[iter 100] loss=2.4300 val_loss=2.6549 scale=2.0000 norm=4.1426                 \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL145 (val_loss=2.5930)                                    \n",
      " 92%|█████████▏| 46/50 [16:44<01:50, 27.58s/trial, best loss: 16.46182351650563][iter 0] loss=3.3232 val_loss=3.2137 scale=2.0000 norm=10.8392\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=3.0673)\n",
      "[iter 0] loss=3.3184 val_loss=3.2160 scale=2.0000 norm=10.7998\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2160)\n",
      "[iter 0] loss=3.3260 val_loss=3.2291 scale=2.0000 norm=10.8473\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL17 (val_loss=2.7599)\n",
      "[iter 0] loss=3.2680 val_loss=3.5638 scale=2.0000 norm=10.3270\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.5638)\n",
      "[iter 0] loss=3.3116 val_loss=3.1669 scale=1.0000 norm=5.3590\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8666)\n",
      "[iter 0] loss=3.2703 val_loss=3.0765 scale=2.0000 norm=10.3302\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8858)\n",
      "[iter 0] loss=3.3184 val_loss=3.1101 scale=2.0000 norm=10.7914\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=2.8949)\n",
      "[iter 0] loss=3.3193 val_loss=3.2543 scale=1.0000 norm=5.4126\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL9 (val_loss=2.8869)\n",
      "[iter 0] loss=3.3150 val_loss=3.1807 scale=1.0000 norm=5.3814\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=3.0007)\n",
      "[iter 0] loss=3.2584 val_loss=3.1340 scale=2.0000 norm=10.2530\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL10 (val_loss=2.8183)\n",
      "[iter 0] loss=3.3139 val_loss=3.2826 scale=2.0000 norm=10.7805\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL15 (val_loss=2.8649)\n",
      "[iter 0] loss=3.3105 val_loss=3.1717 scale=2.0000 norm=10.6671\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL6 (val_loss=2.9784)\n",
      "[iter 0] loss=3.3344 val_loss=3.2632 scale=2.0000 norm=10.9645\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL23 (val_loss=2.8244)\n",
      "[iter 0] loss=3.3108 val_loss=3.2365 scale=2.0000 norm=10.7202\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2365)\n",
      "[iter 0] loss=3.3169 val_loss=3.2304 scale=2.0000 norm=10.7848\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL10 (val_loss=2.9321)\n",
      "[iter 0] loss=3.3185 val_loss=3.1550 scale=2.0000 norm=10.8039\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=3.0373)\n",
      "[iter 0] loss=3.2671 val_loss=3.2547 scale=2.0000 norm=10.3043\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2547)\n",
      "[iter 0] loss=3.2529 val_loss=3.1737 scale=2.0000 norm=10.1258\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL21 (val_loss=2.7328)\n",
      "[iter 0] loss=3.2696 val_loss=3.2945 scale=2.0000 norm=10.3266\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2945)\n",
      "[iter 0] loss=3.2680 val_loss=3.1607 scale=2.0000 norm=10.3003\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.1139)\n",
      "[iter 0] loss=3.2676 val_loss=3.1072 scale=2.0000 norm=10.3123\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.9477)\n",
      "[iter 0] loss=3.3126 val_loss=3.1040 scale=2.0000 norm=10.7141\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.1037)\n",
      "[iter 0] loss=3.3119 val_loss=3.2019 scale=2.0000 norm=10.7123\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL7 (val_loss=2.8162)\n",
      "[iter 0] loss=3.2691 val_loss=3.1915 scale=2.0000 norm=10.3228\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL25 (val_loss=2.6743)\n",
      "[iter 0] loss=3.3215 val_loss=3.2918 scale=2.0000 norm=10.8208\n",
      "[iter 100] loss=2.4342 val_loss=2.6558 scale=2.0000 norm=4.1687\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL145 (val_loss=2.5901)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=3.3080 val_loss=3.1350 scale=2.0000 norm=10.7092\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.9610)\n",
      "[iter 0] loss=3.3211 val_loss=3.1675 scale=2.0000 norm=10.7673\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL5 (val_loss=2.9697)\n",
      "[iter 0] loss=3.3043 val_loss=3.2554 scale=2.0000 norm=10.6662\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL24 (val_loss=2.7894)\n",
      "[iter 0] loss=3.3214 val_loss=3.2346 scale=2.0000 norm=10.8197\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL10 (val_loss=2.9507)\n",
      "[iter 0] loss=3.2679 val_loss=3.0889 scale=2.0000 norm=10.3135\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.9196)\n",
      "[iter 0] loss=3.3175 val_loss=3.2228 scale=2.0000 norm=10.7558\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL19 (val_loss=2.8124)\n",
      "[iter 0] loss=3.3044 val_loss=3.3454 scale=2.0000 norm=10.6360\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3454)\n",
      "[iter 0] loss=3.3174 val_loss=3.1861 scale=2.0000 norm=10.7861\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1861)\n",
      "[iter 0] loss=3.2978 val_loss=3.2086 scale=2.0000 norm=10.5349\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.1496)\n",
      "[iter 0] loss=3.3165 val_loss=3.2666 scale=2.0000 norm=10.7682\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL33 (val_loss=2.8701)\n",
      "[iter 0] loss=3.3115 val_loss=3.2565 scale=1.0000 norm=5.3681\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL12 (val_loss=2.7849)\n",
      "[iter 0] loss=3.3290 val_loss=3.2116 scale=2.0000 norm=10.9044\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL7 (val_loss=2.8487)\n",
      "[iter 0] loss=3.2742 val_loss=3.2412 scale=2.0000 norm=10.3851\n",
      "[iter 100] loss=2.3858 val_loss=2.5888 scale=2.0000 norm=3.9570\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL149 (val_loss=2.5091)\n",
      "CrossValMSE:                                                                    \n",
      "16.396616715752316                                                              \n",
      " 94%|████████▍| 47/50 [17:47<03:02, 60.77s/trial, best loss: 16.396616715752316][iter 0] loss=3.3092 val_loss=3.1577 scale=2.0000 norm=10.6465\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL5 (val_loss=2.9634)\n",
      "[iter 0] loss=3.3107 val_loss=3.2543 scale=2.0000 norm=10.7221\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.2543)\n",
      "[iter 0] loss=3.3269 val_loss=3.2385 scale=2.0000 norm=10.8658\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL10 (val_loss=2.9627)\n",
      "[iter 0] loss=3.3053 val_loss=3.1255 scale=2.0000 norm=10.6552\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.9628)\n",
      "[iter 0] loss=3.3276 val_loss=3.2348 scale=2.0000 norm=10.8855\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL19 (val_loss=2.8288)\n",
      "[iter 0] loss=3.3264 val_loss=3.3599 scale=2.0000 norm=10.8820\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3599)\n",
      "[iter 0] loss=3.3086 val_loss=3.1280 scale=2.0000 norm=10.6708\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1280)\n",
      "[iter 0] loss=3.3252 val_loss=3.1646 scale=2.0000 norm=10.8726\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL3 (val_loss=3.0066)\n",
      "[iter 0] loss=3.3166 val_loss=3.1278 scale=2.0000 norm=10.7820\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1278)\n",
      "[iter 0] loss=3.3206 val_loss=3.2003 scale=2.0000 norm=10.8145\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL7 (val_loss=2.8459)\n",
      "[iter 0] loss=3.3050 val_loss=3.2830 scale=2.0000 norm=10.6683\n",
      "[iter 100] loss=2.4253 val_loss=2.6331 scale=2.0000 norm=4.1294\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL150 (val_loss=2.5566)\n",
      "[iter 0] loss=3.3010 val_loss=3.1239 scale=2.0000 norm=10.6177                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL4 (val_loss=2.9012)                                      \n",
      "CrossValMSE:                                                                    \n",
      "23.79694460571867                                                               \n",
      "[iter 0] loss=3.3084 val_loss=3.3234 scale=2.0000 norm=10.6796                  \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL0 (val_loss=3.3234)                                      \n",
      "CrossValMSE:                                                                    \n",
      "44.505057814268795                                                              \n",
      "[iter 0] loss=3.3091 val_loss=3.1992 scale=1.0000 norm=5.3487                   \n",
      "== Early stopping achieved.                                                     \n",
      "== Best iteration / VAL1 (val_loss=3.1098)                                      \n",
      "CrossValMSE:                                                                    \n",
      "37.87425008531859                                                               \n",
      "100%|█████████| 50/50 [18:10<00:00, 21.81s/trial, best loss: 16.396616715752316]\n",
      "Best:  {'Base': 0, 'Dist': 0, 'Score': 0, 'col_sample': 1.0, 'learning_rate': 0.01, 'minibatch_frac': 0.21, 'n_estimators': 31}\n"
     ]
    }
   ],
   "source": [
    "def objective(space):\n",
    "\n",
    "    #warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "    hyperopt_regressor = ngb.NGBRegressor(n_estimators = space['n_estimators'],\n",
    "                            Dist = space['Dist'],\n",
    "                            #max_depth = int(space['max_depth']),\n",
    "                            learning_rate = space['learning_rate'],\n",
    "                            #gamma = space['gamma'],\n",
    "                            #min_child_weight = space['min_child_weight'],\n",
    "                            minibatch_frac = space['minibatch_frac'],\n",
    "                            col_sample = space['col_sample'],\n",
    "                            Score = space['Score'],\n",
    "                            Base = space['Base'],\n",
    "                            early_stopping_rounds = 3\n",
    "                            )\n",
    "    \n",
    "    hyperopt_regressor.fit(X_train.loc[:, ~X_train.columns.isin(['GAME_DATE', 'PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID', 'TEAM_ABBREVIATION'])], \n",
    "                           np.ravel(y_train))\n",
    "\n",
    "    # Applying k-Fold Cross Validation\n",
    "    mses = cross_val_score(estimator = hyperopt_regressor, \n",
    "                           X = X_train.loc[:, ~X_train.columns.isin(['GAME_DATE', 'PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID', 'TEAM_ABBREVIATION'])],\n",
    "                           y = np.ravel(y_train), cv = 5, n_jobs=-1, scoring=make_scorer(mean_squared_error)\n",
    "                          )\n",
    "    \n",
    "    CrossValMSE = mses.mean()\n",
    "\n",
    "    print(\"CrossValMSE: \", CrossValMSE)\n",
    "\n",
    "    return{'loss': CrossValMSE, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Best: \", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "70b030c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Base': 0,\n",
       " 'Dist': 0,\n",
       " 'Score': 0,\n",
       " 'col_sample': 1.0,\n",
       " 'learning_rate': 0.01,\n",
       " 'minibatch_frac': 0.21,\n",
       " 'n_estimators': 31}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=3.3203 val_loss=3.3463 scale=2.0000 norm=10.8171\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3463)\n",
      "[iter 0] loss=3.3017 val_loss=3.2555 scale=2.0000 norm=10.6058\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL31 (val_loss=2.8408)\n",
      "[iter 0] loss=3.2732 val_loss=3.1744 scale=2.0000 norm=10.3723\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL7 (val_loss=2.7911)\n",
      "[iter 0] loss=3.3082 val_loss=3.2331 scale=2.0000 norm=10.6809\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL24 (val_loss=2.7350)\n",
      "[iter 0] loss=3.3034 val_loss=3.1074 scale=2.0000 norm=10.6398\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.8704)\n",
      "[iter 0] loss=3.3234 val_loss=3.3070 scale=2.0000 norm=10.8451\n",
      "[iter 100] loss=2.4581 val_loss=2.7079 scale=2.0000 norm=4.3208\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL143 (val_loss=2.6410)\n",
      "[iter 0] loss=3.3183 val_loss=3.1696 scale=2.0000 norm=10.7922\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=3.0042)\n",
      "[iter 0] loss=3.3322 val_loss=3.2596 scale=2.0000 norm=10.9352\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL22 (val_loss=2.8050)\n",
      "[iter 0] loss=3.3169 val_loss=3.2271 scale=2.0000 norm=10.7777\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL21 (val_loss=2.8242)\n",
      "[iter 0] loss=3.3215 val_loss=3.2370 scale=2.0000 norm=10.8249\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.1879)\n",
      "[iter 0] loss=3.3048 val_loss=3.1610 scale=2.0000 norm=10.6515\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.9836)\n",
      "[iter 0] loss=3.3134 val_loss=3.2500 scale=2.0000 norm=10.7084\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL32 (val_loss=2.8244)\n",
      "[iter 0] loss=3.3119 val_loss=3.2444 scale=1.0000 norm=5.3621\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL12 (val_loss=2.7641)\n",
      "[iter 0] loss=3.3218 val_loss=3.2415 scale=2.0000 norm=10.8167\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL24 (val_loss=2.7430)\n",
      "[iter 0] loss=3.3342 val_loss=3.3051 scale=2.0000 norm=10.9534\n",
      "[iter 100] loss=2.4468 val_loss=2.6709 scale=2.0000 norm=4.2050\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL145 (val_loss=2.6060)\n",
      "[iter 0] loss=3.3112 val_loss=3.1186 scale=2.0000 norm=10.7064\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.9053)\n",
      "[iter 0] loss=3.3147 val_loss=3.2893 scale=2.0000 norm=10.7606\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL15 (val_loss=2.8941)\n",
      "[iter 0] loss=3.3211 val_loss=3.1287 scale=2.0000 norm=10.8255\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0801)\n",
      "[iter 0] loss=3.3043 val_loss=3.1049 scale=2.0000 norm=10.6268\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0702)\n",
      "[iter 0] loss=3.3055 val_loss=3.2316 scale=2.0000 norm=10.6964\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL10 (val_loss=2.9433)\n",
      "[iter 0] loss=3.3072 val_loss=3.1314 scale=2.0000 norm=10.6887\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.9755)\n",
      "[iter 0] loss=3.3035 val_loss=3.3183 scale=2.0000 norm=10.6342\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3183)\n",
      "[iter 0] loss=3.3155 val_loss=3.2253 scale=2.0000 norm=10.7685\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.1553)\n",
      "[iter 0] loss=3.3199 val_loss=3.1362 scale=2.0000 norm=10.7946\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1362)\n",
      "[iter 0] loss=3.3305 val_loss=3.2422 scale=2.0000 norm=10.9086\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL24 (val_loss=2.7492)\n",
      "[iter 0] loss=3.3278 val_loss=3.1247 scale=2.0000 norm=10.8750\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.8984)\n",
      "[iter 0] loss=3.3067 val_loss=3.3051 scale=1.0000 norm=5.3397\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.2417)\n",
      "[iter 0] loss=3.3112 val_loss=3.3519 scale=2.0000 norm=10.6849\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3519)\n",
      "[iter 0] loss=3.3173 val_loss=3.3040 scale=2.0000 norm=10.7875\n",
      "[iter 100] loss=2.4551 val_loss=2.7096 scale=2.0000 norm=4.3242\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL141 (val_loss=2.6490)\n",
      "[iter 0] loss=3.2977 val_loss=3.2766 scale=2.0000 norm=10.6042\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL15 (val_loss=2.8316)\n",
      "[iter 0] loss=3.2630 val_loss=3.0569 scale=2.0000 norm=10.3027\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0108)\n",
      "[iter 0] loss=3.2642 val_loss=3.2176 scale=2.0000 norm=10.2956\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL27 (val_loss=2.7368)\n",
      "[iter 0] loss=3.3183 val_loss=3.1538 scale=2.0000 norm=10.7850\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.1240)\n",
      "[iter 0] loss=3.3086 val_loss=3.2933 scale=2.0000 norm=10.6935\n",
      "[iter 0] loss=3.3221 val_loss=3.3873 scale=2.0000 norm=10.8260\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3873)\n",
      "[iter 0] loss=3.2970 val_loss=3.2167 scale=2.0000 norm=10.4919\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL20 (val_loss=2.7762)\n",
      "[iter 0] loss=3.3238 val_loss=3.1921 scale=2.0000 norm=10.8549\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1921)\n",
      "[iter 0] loss=3.2719 val_loss=3.0728 scale=2.0000 norm=10.3603\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0591)\n",
      "[iter 0] loss=3.2718 val_loss=3.2128 scale=1.0000 norm=5.1803\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL12 (val_loss=2.7173)\n",
      "[iter 0] loss=3.3236 val_loss=3.3092 scale=2.0000 norm=10.8285\n",
      "[iter 100] loss=2.4415 val_loss=2.6714 scale=2.0000 norm=4.1903\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL146 (val_loss=2.6070)\n",
      "[iter 0] loss=3.3188 val_loss=3.3340 scale=1.0000 norm=5.3812\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3340)\n",
      "[iter 0] loss=3.2715 val_loss=3.0921 scale=2.0000 norm=10.3431\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.0921)\n",
      "[iter 0] loss=3.2685 val_loss=3.2754 scale=1.0000 norm=5.1730\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.2023)\n",
      "[iter 0] loss=3.3104 val_loss=3.1457 scale=2.0000 norm=10.6743\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1457)\n",
      "[iter 0] loss=3.3075 val_loss=3.1558 scale=2.0000 norm=10.6651\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL5 (val_loss=2.9701)\n",
      "[iter 0] loss=3.3249 val_loss=3.1330 scale=2.0000 norm=10.8691\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0996)\n",
      "[iter 0] loss=3.3109 val_loss=3.1590 scale=2.0000 norm=10.6995\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL5 (val_loss=2.9432)\n",
      "[iter 0] loss=3.2721 val_loss=3.1690 scale=2.0000 norm=10.3721\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1690)\n",
      "[iter 0] loss=3.3089 val_loss=3.1551 scale=2.0000 norm=10.6839\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.1322)\n",
      "[iter 0] loss=3.3165 val_loss=3.2835 scale=2.0000 norm=10.7587\n",
      "[iter 0] loss=3.3106 val_loss=3.1420 scale=2.0000 norm=10.7164\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=3.0323)\n",
      "[iter 0] loss=3.3250 val_loss=3.3650 scale=2.0000 norm=10.8530\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3650)\n",
      "[iter 0] loss=3.3176 val_loss=3.1972 scale=2.0000 norm=10.7907\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1972)\n",
      "[iter 0] loss=3.3289 val_loss=3.2480 scale=2.0000 norm=10.9139\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.1894)\n",
      "[iter 0] loss=3.3134 val_loss=3.1582 scale=2.0000 norm=10.7546\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.9852)\n",
      "[iter 0] loss=3.3210 val_loss=3.2514 scale=2.0000 norm=10.8535\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL32 (val_loss=2.8562)\n",
      "[iter 0] loss=3.2665 val_loss=3.0684 scale=2.0000 norm=10.3046\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.8402)\n",
      "[iter 0] loss=3.3243 val_loss=3.3406 scale=1.0000 norm=5.4034\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3406)\n",
      "[iter 0] loss=3.3281 val_loss=3.1799 scale=2.0000 norm=10.8981\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1799)\n",
      "[iter 0] loss=3.3118 val_loss=3.2880 scale=2.0000 norm=10.7198\n",
      "[iter 100] loss=2.4413 val_loss=2.6911 scale=2.0000 norm=4.2364\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL142 (val_loss=2.6294)\n",
      "[iter 0] loss=3.3244 val_loss=3.1740 scale=2.0000 norm=10.8462\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=3.0075)\n",
      "[iter 0] loss=3.3152 val_loss=3.1190 scale=2.0000 norm=10.7700\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.0724)\n",
      "[iter 0] loss=3.3167 val_loss=3.2539 scale=2.0000 norm=10.7670\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL23 (val_loss=2.7992)\n",
      "[iter 0] loss=3.3206 val_loss=3.1624 scale=2.0000 norm=10.8106\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.1343)\n",
      "[iter 0] loss=3.3197 val_loss=3.3034 scale=2.0000 norm=10.8038\n",
      "[iter 0] loss=3.3144 val_loss=3.3411 scale=2.0000 norm=10.7338\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.3411)\n",
      "[iter 0] loss=3.2707 val_loss=3.0898 scale=2.0000 norm=10.3158\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.0898)\n",
      "[iter 0] loss=3.3205 val_loss=3.1668 scale=2.0000 norm=10.8232\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=3.0120)\n",
      "[iter 0] loss=3.3284 val_loss=3.1373 scale=2.0000 norm=10.9025\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL1 (val_loss=3.1335)\n",
      "[iter 0] loss=3.3176 val_loss=3.2641 scale=1.0000 norm=5.3979\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL12 (val_loss=2.8140)\n",
      "[iter 0] loss=3.3172 val_loss=3.2034 scale=2.0000 norm=10.7856\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL7 (val_loss=2.8257)\n",
      "[iter 0] loss=3.3231 val_loss=3.1201 scale=2.0000 norm=10.8303\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL4 (val_loss=2.9058)\n",
      "[iter 0] loss=3.3193 val_loss=3.1649 scale=2.0000 norm=10.7874\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1649)\n",
      "[iter 0] loss=3.3210 val_loss=3.1609 scale=2.0000 norm=10.8109\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL0 (val_loss=3.1609)\n"
     ]
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8877274",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ddea521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=3.3029 val_loss=3.2884 scale=2.0000 norm=10.6409\n"
     ]
    }
   ],
   "source": [
    "ngb_regressor = ngb.NGBRegressor(Base = b1,\n",
    "                  Dist = ngb.distns.Normal,\n",
    "                  Score = ngb.scores.LogScore,\n",
    "                  n_estimators = best['n_estimators'],\n",
    "                  learning_rate = best['learning_rate'],\n",
    "                  minibatch_frac = best['minibatch_frac'],\n",
    "                  early_stopping_rounds=5).fit(X_train.loc[:, ~X_train.columns.isin(['GAME_DATE', 'PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID', 'TEAM_ABBREVIATION'])], np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a4c2627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_preds = ngb_regressor.predict(X_test.loc[:, ~X_test.columns.isin(['GAME_DATE', 'PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID', 'TEAM_ABBREVIATION'])])\n",
    "Y_dists = ngb_regressor.pred_dist(X_test.loc[:, ~X_test.columns.isin(['GAME_DATE', 'PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID', 'TEAM_ABBREVIATION'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1d7bfc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE 19.662727468819938\n"
     ]
    }
   ],
   "source": [
    "# test Mean Squared Error\n",
    "test_MSE = mean_squared_error(Y_preds, y_test)\n",
    "print('Test MSE', test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "375b8f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.434267410612484"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bdabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Negative Log Likelihood\n",
    "test_NLL = -Y_dists.logpdf(Y_test).mean()\n",
    "print('Test NLL', test_NLL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
